\documentclass[12pt]{article}
\usepackage[margin=1in]{geometry}
%\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{graphicx}% http://ctan.org/pkg/graphicx
\usepackage{systeme}
%% tables
\usepackage{booktabs}
\usepackage[table,xcdraw]{xcolor}
\usepackage{float}

%% Make red text
\newcommand{\com}[1]{\textcolor{red}{ #1}}

%\linespread{2}

\usepackage[authoryear]{natbib}


\usepackage{tikz}
\tikzset{
  int/.style={circle, draw, fill=blue!20, minimum size=3em},
  init/.style={pin distance=1.2cm,pin edge={loop,thin,black}}
}
\usetikzlibrary{arrows,automata}

\pagenumbering{arabic}

\newcommand{\XX}{\ensuremath{25}} % number of methods

\newcommand{\xxsir}{\ensuremath{11}} % number of SIR methods

\newcommand{\rr}{\ensuremath{\mathcal{R}_0}}

\setlength{\itemsep}{0pt}






\begin{document}

%%% draw a tree
\tikzset{
  treenode/.style = {shape=rectangle, rounded corners,
                     draw, align=center,
                     top color=white, bottom color=blue!20},
  root/.style     = {treenode, font=\Large, bottom color=red!30},
  env/.style      = {treenode, font=\ttfamily\normalsize},
  dummy/.style    = {circle,draw}
}




\title{$\XX$ dubious ways of estimating $\rr$}
\author{ Department of Statistics, Carnegie Mellon University}
\date{\today}
\maketitle

% \tableofcontents


\section{Introduction}\label{sec:intro}
What has been called ``arguably the most important quantity in the study of epidemics'', $\mathcal{R}_0$, the reproduction number (by convention pronounced ``R-naught''), for nearly a century, has been an elusive quantity for epidemic modelers to estimate \citep{Heesterbeek2002}.  Defined by \citet{anderson1992}, $\rr$ is the ``the average number of secondary infections produced when one infected individual is introduced into a host population where everyone is susceptible.''  $\rr$, in some ways, summarizes an entire outbreak of a disease; it  is used to assess whether a disease will break out and what percentage of the population needs to be vaccinated to avoid such an outbreak.  Despite a clear definition of $\rr$, epidemiologists have struggled to create a standard  estimator for $\rr$  \citep{hethcote2000}.  The primary issue in estimating $\rr$ is that the quantity is a \textit{property of the model}, meaning that $\rr$ is dependent not only on the usual noise that comes with statistical modeling but also on a variety of assumptions on how researchers assume a disease is transmitted through a population.

Often, disease models are a part of the ``SI-framework'' pioneered by Kermack and McKendrick in the 1920s \citep{getz2006}.  Here, S and I are known as compartments where S stands for ``susceptible'' and I for ``infected.''  The issue of estimating $\rr$ stems from how individuals are assumed, either implicitly or explicitly, to move from the S compartment to the I compartment and any other compartments that have been introduced such as immune, recovered, dead, or exposed, for example.  One of the simplest models in the SI framework is the SIR model where R stands for a ``recovered'' state where individuals are no longer susceptible to the disease \citep{Kermack700}.  Even in this simplified framework, there is much dispute on how $\rr$ is to be estimated arising from assumptions on how populations intermix, if any intervention has been put in place, or whether disease parameters are constant over time.

Other issues about estimating $\rr$ include mathematical versus statistical approaches (i.e. solving for $\rr$ versus estimating $\rr$), how to estimate the quantity in the presence of types of available data, and when to estimate $\rr$ in a disease's lifetime.  This depends on the ``time constant'' of the disease.  Some diseases like measles progress rapdily, HIV more slowly, prion diseases glacially so.  For instance, if only weekly incidence counts of a disease are available, then we have different methods of estimating $\rr$ available than when we have more data available such as contact networks or the serial interval of a disease.  Another major issue is that since $\rr$ is difficult to estimate in the first place, the estimated variance of the estimated $\rr$ has largely been glossed over or  ignored.  This is especially troubling because we are concerned whether $\rr > 1$, which indicates an outbreak of a disease.  A point estimate is simply not enough to estimate the truth of this inequality.

In short, $\rr$ is an important parameter in epidemiology that is very difficult to estimate.  This, in turn, makes it difficult to compare $\rr$ across different diseases or even different instances of the same disease.  Many of these challenges with estimating $\rr$ and more are summarized and expanded upon by \cite{li2011}.

We review $\XX$ ways to estimate $\rr$ across a number of model assumptions and in the presence of different sources of data.  The title of this manuscript is a reference to a seminal paper by Moler and Van Loan (1978) (and its subsequent update in 2003) in which they discuss ``Nineteen dubious ways to compute the exponential of a matrix'' \citep{moler2003}.  We not only review the methods of estimating  $\rr$ but also include recommendations on when to use the methods as well as introducing techniques which allow estimation the variance.  We compare these estimates, when applicable, in the presence of different data, both simulated and real.

The rest of this manuscript is organized as follows.  In Section \ref{sec:overview}, we introduce the methods in four broad groups: the survival function, compartment models, exponential growth models, and agent-based models.    In Section \ref{sec:details}, we give an overview of each of the methods, what assumptions are used, what data is required, and some of the places the method has been used in the past along with ways in which to estimate the variance of the various methods, when possible.  In Section \ref{sec:results}, for a number of different data sets, we give estimates of $\rr$ for the different methods and their variances over a variety of data sets.  Finally, in Section \ref{sec:dis} we discuss our conclusions and recommendations.  


\section{Overview of $\XX$ Methods to estimate $\rr$}
\label{sec:overview}

In this manuscript, $\XX$ methods were chosen to give a survey of methods for estimating $\rr$.  This is not exhaustive, but these methods were chosen due to historical importance and impact in the field of epidemiology. 

In order to organize the methods for estimating $\rr$, we have chosen four broad classes of estimation methods.  These partitions are meant to be guidelines for distinguishing methods from one another rather than immutable groups.  Many of these methods  fit in many of the groups.  We have chosen the four groups as application of the survival function and its close relatives, classic compartment models, exponential growth assumptions, and estimation from networks.  We list these point estimate methods below in Table \ref{tab:methods} and explain each in detail in Section \ref{sec:details}.  In Table \ref{tab:terms}, we summarize many of the common terms used throughout the document.


% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage[table,xcdraw]{xcolor}
% If you use beamer only pass "xcolor=table" option, i.e. \documentclass[xcolor=table]{beamer}
\begin{table}[h]
  \centering
  \resizebox{\textwidth}{!}{
\begin{tabular}{@{}llll@{}}
\toprule
\textbf{Survival Function}        & \textbf{Compartment Models}            & \textbf{Exponential Growth}                       & \textbf{Networks}        \\ \midrule
Survival Function (SF)            & \textbf{SIR}                           & Exponential Growth (EG)                           & Contact Tracing (CT)     \\
Direct Parameter Estimation (DPE) & - Least Squares (SIR LS)               & Maximum Likelihood of Secondary Infections (MLSI) & Branching Process (BP)   \\
Von Foerster Equations (VFE)      & - Reparameterized LS (SIR ReLS)        & Time Dependent Reproduction Number (TDR)          & Agent-Based Models (ABM) \\
                                  & - Linear Model Approximation (SIR LMA) & Initial Growth and  Final Size (IGFS)             &                          \\
                                  & - LMA, All Time Points (SIR LMAT)      &                                                   &                          \\
                                  & - Max of Data (SIR Max)                &                                                   &                          \\
                                  & - Smooth Max (SIR SMax)                &                                                   &                          \\
                                  & - Incidence-Prevalence Ratio (SIR IPR) &                                                   &                          \\
                                  & - Smooth IPR (SIR SIPR)                &                                                   &                          \\
                                  & - Log Linear (SIR LL)                  &                                                   &                          \\
                                  & - Markov Chain Estimation (SIR MC)     &                                                   &                          \\
                                  & - Sequential Bayes (SIR SB)            &                                                   &                          \\
                                  & SEIR Model (SEIR)                      &                                                   &                          \\
                                  & MSEIR Model (MSEIR)                    &                                                   &                          \\
                                  & Amplifier Model (AMP)                  &                                                   &                          \\
                                  & Next Generation Model (NGM)            &                                                   &                          \\ \bottomrule
\end{tabular}
}
\caption{Table of methods used to estimate $\rr$, divided into four categories.}
\label{tab:methods}
\end{table}

Additionally, we describe  interval estimates for the above methods for estimating $\rr$ in order to better answer the question of whether $\rr > 1$, which implies an outbreak of the disease.  These methods are the delta method and posterior distribution.

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
\begin{table}[]
\centering
\begin{tabular}{@{}ll@{}}
\toprule
Term        & Meaning                                                                                    \\ \midrule
$\rr$     & reproduction number                                                                        \\
$\mathcal{R}_t$     & effective reproduction number at time $t$                                                  \\
$\beta$     & average rate of infection                                                                  \\
$\gamma$    & average rate of recovery                                                                   \\
$\alpha$    & rate of transfer from exposed to infectious                                                \\
$\delta$    & rate of transfer from immune infant to susceptible                                         \\
$D$         & average duration of an infection                                                           \\
$N$         & population size                                                                            \\
$X(t)$      & number of susceptible individuals at time $t$                                              \\
$Y(t)$      & number of infected individuals at time $t$                                                 \\
$Z(t)$      & number of recovered individuals at time $t$                                                \\
$M(t)$      & number of immune infants at time $t$                                                       \\
$E(t)$      & number of exposed individuals at time $t$                                                  \\
$J(t)$      & incidence at time $t$                                                                      \\
$\omega(t)$ & the time from onset of symptoms in an index case to onset of symptoms in a subsequent case \\
$\mu_\infty$       & percentage of population eventually infected                                               \\
IPR         & incidence to prevalence ratio                                                             \\ \bottomrule
\end{tabular}
\caption{Table of common terms used throughout the document and their meanings}\label{tab:terms}
\end{table}




\section{Details for the $\XX$ methods}
\label{sec:details}
\subsection{Survival function}
\label{sec:direct}

The first set of methods are those of the survival function, which is closely  tied to the history of $\rr$.  The survival function, which originates from the field of demography in the late 1800s, describes how many female offspring a woman would produce \citep{dietz1993estimation}.  In demography, we have
\begin{align*}
\rr = \int_0^\infty p(a) \beta(a) da
\end{align*}
where $p(a)$ denotes the probability of a woman surviving to age $a$ and $\beta(a)$ the rate of an individual of age $a$ giving birth to a  girl.   This interpretation also explains the origins of the name of $\rr$, which was imported to the field of epidemiology by MacDonald and Smith \citep{dietz1993estimation}.  Analogously in epidemiology, $p(a)$ is the age of a disease, and $\beta$ is the transmission rate.

This is the chronologically first, and in many ways, the most direct method to estimate $\rr$.  However, this model also seems to be the most difficult to estimate as both $p(a)$ and $\beta (a)$ are random variables.   In general, it is also difficult to estimate these parameters before or even during an ongoing disease outbreak.  As such, $\rr$ can only be estimated after an outbreak, which means it cannot be used to determine ongoing prevention strategies.  However, having a post-disease estimation of $\rr$ may be useful for preventing future outbreaks of the disease and aids in comparing the severity of many diseases.  Another advantage of this method is that it is intuitive and independent of assumptions of how the disease is transmitted from person to person.    We describe four methods which estimate $\rr$ from the survival function:  the survival function, direct estimation, and direct parameter estimation.


\subsubsection{Survival Function (SF)}
\label{sec:survival_fxn}
The first method of estimating $\rr$, is not surprisingly, the survival function itself as presented by \cite{Heffernan2005}.  We use standard notation from infectious disease epidemiology.

Let $F(a)$ be the probability that a newly infected individual remains infectious for at least time $a$, and let $b(a)$ be the average number of newly infected individuals that an infectious individual will produce per unit time when infected for total time $a$.  Then $\rr$ is given in Equation (\ref{eq:r0_survivalfxn}),

\begin{align}\label{eq:r0_survivalfxn}
  \rr = \int_0^\infty b(a)F(a)da.
\end{align}

This method presents one of the unique difficulties of estimating $\rr$: that of reconciling generations and time.  $\rr$ tells us about the generations of the disease, meaning an infector and its infected, but not necessarily when the infections occur.  This reconciliation between generations and time is made difficult as the data we have about a disease is often of a temporal nature.  On the other hand, this method is flexible in that this derivation of $\rr$  does not place assumptions on how the disease transmits through a population.  A disadvantage is that data about $F(a)$ and especially $b(a)$ is difficult to collect directly.

Estimating this quantity in Equation \ref{eq:r0_survivalfxn} is hard and, in fact, rarely attempted. It is due to this difficulty that many of the other methods for estimating $\rr$ even exist.  



\subsubsection{Direct Parameter Estimation (DPE)}
\label{sec:dpe}

Direct parameter estimation, an approach for parameters that are readily available to be estimated, is a `plug-in' estimator approach as described by \cite{lipsitch2003} and \cite{dietz1993estimation}.  This method, like the survival function, is highly interpretable.  We assume that we can estimate $\rr$ if we know how many others an individual comes into contact $(k)$, the probability of transmission given contact $(b)$, and how long the disease lasts within an individual $(D)$.  Then $\rr$ is the product of these parameters, 
\begin{align}\label{eq:dpe}
\rr = kbD.
\end{align}
\cite{lipsitch2003} note that directly estimating $\rr$ in such a way may be difficult due to the available data, as in the previous  method.  They instead estimate $\rr$ using the mean serial interval, $\omega$, the time from onset of symptoms in an index case to the onset of symptoms in a subsequent case infected by the index patient in addition to the assumption of exponential growth of infected cases, as a proxy for the duration $D$.  An advantage of this method is that it reduces the problem to three parameters that we can conceivably collect data on.  A disadvantage is that to estimate $\rr$ alone,  data is only used from the initial outbreak of the disease, so often we have a small sample size, resulting in large confidence sets or credible intervals.


%%%%%%%

\subsubsection{Von Foerster Equations (VFE)}
\label{sec:direct-estim-surv}

A derivation of $\rr$ similar to the survival function comes from the idea of the von Foerster equations, which were used to calculate the number of cells in a ``deme,'' a constrained area.  The function $Y(t, \tau)$ calculates the number of cells of age $\tau$ at time $t$ \citep{trucco1965}.  In epidemiology, $Y$ represents the number of infections instead of cells. Letting $Y(t, \tau)$ be the number of infections at time $t$ of age $\tau$, the von Foerster equation is
\begin{align*}
  \frac{\partial Y(t, \tau)}{\partial t} +   \frac{\partial Y(t, \tau)}{\partial \tau} = -  \lambda \cdot n(t, a) ,
\end{align*}
where $Y(t,\tau)$ is the number of people at time $t$ who were infected time $\tau$ ago and $\lambda$ is the loss function, which we assume to be 0, that is no deaths.  The right hand side represents the net loss of cells.  This derivation is based on the ``reasoning that the number of cells which are in the age category $(a, a + \Delta a)$ at time $t$, is equal to $n(t,a) \Delta a$'' \citep{trucco1965}.     \cite{fraser2004factors} derived an expression for $\rr$ from the von Foerster equation and the additional boundary equation
\begin{align*}
  Y(t,0) &= \int_0^t \beta( \tau) Y(t, \tau) d \tau,
\end{align*}
with initial conditions $Y(0,0) = Y_0$ and $Y(t, \tau) = 0$ when $t < \tau$.  Then the reproduction number is given by Equation \ref{eq:vfr0},
\begin{align}\label{eq:vfr0}
\rr &= \int_0^\infty \beta(\tau) d\tau,
\end{align}
 where $\beta (\tau)$ represents the infectiousness at time $\tau$ since the infection.    Again, this model is more mathematical in nature than practical as $\beta (\tau)$ is difficult to estimate.  To estimate $\rr$, \cite{fraser2004factors} make an approximation that $Y(t, \tau) = K(\tau) \exp (rt)$ and further adapt the method to include prevention strategies.  The main function to be estimated is $\beta (\tau)$.  Estimating $\beta (\tau)$ has many of the same issues as trying to estimate $b(a)$ in the survival function in that it is difficult to estimate while the disease is ongoing but is also advantageous in that it does not place assumptions on how the disease is transmitted throughout the population.


%%%%%%%%%%%%%%%



\subsection{Compartment Models}
\label{sec:cms}

First introduced by \cite{Kermack700}, a large and common set of methods used estimate $\rr$ is that of fitting compartment models (CMs) to disease data.  As described in \cite{daley2001epidemic},  CMs model how objects move from one discrete state to another over time.  These models provide an aggregate model of how diseases may be spread throughout a population.  Within the CM framework, we make four essential assumptions,
\begin{enumerate}
\item The compartments are discrete and have no overlap.
\item The transition of objects into and out of compartments is described by a set of known equations, possibly dependent on unknown parameters.
\item The populations mix homogeneously
  \item The number of objects in each compartment at time $t=0$ is known
  \end{enumerate}  


Another common assumption for compartment models, is the law of mass action, a property borrowed from chemistry which says that the mass of the product is proportional to the mass of the reactants.  In epidemiology, this means that the proportion of new infections is proportional to the current  number of susceptible and infected \citep{anderson1992}.  Therefore, the flow of objects from one compartment to another may be dependent on the percentage of objects within one or many of the compartments. 

For disease modelling, all CMs fall into the SI-framework, meaning that there is a susceptible population that may become infected.  Thus, we require there be susceptible and infected compartments but also allow additional compartments such as the recovered state, an exposed latent state, and more.  CMs may either work with continuous or discrete time and \cite{getz2006} remark that there is no preference between the two.  Of course, data is always recorded in discrete time!

In this section, we discuss $15$ different CM-based methods used to estimate $\rr$.  Different model specifications of CMs yield different mathematical derivations of $\rr$.  That is, for example, an SIR model with non-constant population will yield a different mathematical formula for $\rr$ than the SEIR model (here ``E'' stands for exposed) with non-constant population.  The difference in these two models is that the SEIR model has an extra compartment which in turn effects the estimates of transitions from one compartment to another.  That said, even within the same compartment model framework, we can estimate $\rr$ in a variety of ways using statistical techniques.  In fact, we describe $\xxsir$ methods to estimate $\rr$, within the SIR framework alone!  Most of these methods are recipes to estimate parameters within the SIR model and may be extended to other CM frameworks such as the SEIR model.  The final method described in this section, the next generation operator (NGM) is a general method to derive $\rr$ from any valid CM.  A particular advantage to CMs compared to survival function methods is that they are easier to estimate as the data required is typically collected about diseases: namely, incidence counts.  




\subsubsection{SIR Model}
\label{sec:sir-model}

We begin with the most well known CM, the SIR model.   First introduced to epidemiology by  \cite{Kermack700}, the SIR model governs how individuals transition from susceptible, infected, and recovered compartments.  The model is represented graphically in Figure \ref{fig::sir}. 

\begin{figure}[h]
\centering
\begin{tikzpicture}[node distance=3cm,auto,>=latex',every node/.append style={align=center}]
    \node [int,  fill = white!70!blue] (a)              {$S$};
    \node [int,  fill = white!70!blue]           (c) [right of=a] {$I$};
    \node [int,  fill = white!70!blue] (e) [right of=c] {$R$};
    \path[->, auto=false] (a) edge node {} (c)
                          (c) edge node {} (e) ;

\end{tikzpicture}
\caption{Depiction of a SIR model.  One can only get the disease once in this model.}\label{fig::sir}
\end{figure}
In this particular  SIR model, $N$, the number of individuals is constant, as we see no arrows pointing away from the three nodes in Figure \ref{fig::sir}. Simple adaptations of the SIR model can include birth and death rates, which may correspondingly change the derivation of $\rr$ (for further discussion, see Section \ref{sec:ngm}). The parameters  $\beta$ and $\gamma$ have epidemiological meaning.  Here, $\beta$ is the average infection rate and $\gamma$ is the average recovery rate.  The transition of individuals from one compartment to another is represented through the ODEs below.  Throughout, we use $X$, $Y$, and $Z$ for stand-ins for the ODEs for $S$, $I$, and $R$, respectively, to avoid any confusion with $\rr$.
\begin{align}
\systeme{\frac{dX}{dt} = -\frac{\beta XY}{N}, \frac{dY}{dt} = \frac{\beta XY}{N} - \gamma Y, \frac{dZ}{dt} = \gamma Z}. \label{eq::sir}
\end{align}

In words, susceptible individuals become infected at a rate that is proportional to the percentage of infected individuals multiplied by $\beta$, the infection rate, and the number of susceptible individuals.  Infected individuals recover at a rate of $\gamma$ multiplied by the number of infected individuals.

An outbreak occurs if the rate of change of infected individuals is positive,
\begin{align*}
  \frac{dY}{dt} &> 0 \\
  \frac{\beta X Y}{N}  - \gamma Y &> 0 ,\\
  Y \left ( \beta \frac{X}{N} - \gamma \right ) & > 0
\end{align*}
that is,  the rate of new infections is greater than the rate of recovery.  So as long as the number of susceptibles is large, $\frac{X}{N} \approx 1$, then an outbreak will occur if $\rr >1$,
\begin{align}\label{eq:deriv_sir}
  \rr = \frac{\beta}{\gamma} > 1.
  \end{align}
Ideally, we have access data of the number of susceptibles, infecteds, and recovered individuals at different points over time.  From this we would like to estimate the $\beta$ and $\gamma$ parameters and ultimately $\rr$.

There are numerous ways to estimate the parameters in this model.  We detail $\xxsir$ of them here.  In every one of these methods, it is assumed that the SIR model is the correct specification.

\subsubsection{Least Squares ($\beta$, $\gamma$) (SIR LS)}\label{least-squares-beta-gamma}
The first approach to estimate $\rr$ in the SIR model is simply to minimize the joint mean square error for the data collected at each time point and use the plug-in estimator found in Equation \ref{eq:sirls}.  In particular, we find

\begin{align*}
(\hat{\beta}, \hat{\gamma} )&=\text{argmin}_{\beta, \gamma} \sum_{t} \left [ \left (Y_{obs}(t) - Y(t)\right )^2 + \left ( X_{obs}(t) - X(t) \right )^2 \right ]
\end{align*}
Then  the estimate for $\rr$ is given by Equation \ref{eq:sirls},
\begin{align}\label{eq:sirls}
  \hat{R}_0= \frac{\hat{\beta}}{\hat{\gamma}}.
  \end{align}
We can use gradient descent or another optimizer to find estimates for these parameters.  If we assume $Y= f(t) + \epsilon_t$ with $\epsilon_t \overset{iid}{\sim} N(0, \sigma^2)$ where $f(t) = \int_0^t \frac{dY}{ds} ds$, then this this method is equivalent to finding the maximum likelihood estimator (MLE) of $\beta$ and $\gamma$.  However, we note that the the $\epsilon_t$ are certainly not independent from one another and so valid inference is still an issue in this framework.  Even if it were correct, ratio estimatiors typically result in large, uninformative confidence intervals.   The advantage is that this is a very straightforward way to estimate $\rr$ from the SIR model.

\subsubsection{Reparametrized Least Squares ($\rr$, $\gamma$) ( SIR ReLS)}\label{reparametrized-least-squares-rux5f0-gamma}

In the previous method, we estimated $\beta$ and $\gamma$ and then estimated $\rr$.  However, it is possible to directly estimate $\rr$ if we reparametrize the ODEs in Equation \eqref{eq::sir} directly with \(\rr\) and \(\gamma\), using the relation $\rr = \frac{\beta}{\gamma}$.

\begin{align*}
  \left \{
  \begin{array}{cl}
    \frac{dX}{dt} &= - \rr \gamma Y \frac{X}{N}\\
    \frac{dY}{dt} &=  \rr \gamma Y \frac{X}{N}  - \gamma Y \\
    \frac{dZ}{dt} &=  - \gamma Y 
  \end{array}
  \right .
  \end{align*}
We find
\begin{align*}
(\hat{R}_0, \hat{\gamma} ) &= \text{argmin}_{\beta, \gamma} \sum_{t} \left [ \left (Y_{obs}(t) - Y(t)\right )^2 + \left ( X_{obs}(t) - X(t) \right )^2 \right ]
\end{align*}
We use the \(\rr\) directly from the above estimation process, which again can be done with gradient descent or another optimization process.  Again, we cannot assume $\rr$ has a normal distribution as the errors are correlated.  This method is better than SIR LS for estimating $\rr$ as there is no ratio.



\subsubsection{Linear Model Approximation (SIR LMA)}\label{linear-model-approximation-degree-10}

The SIR ODEs have no known closed form solution, and so we are already using approximations using numerical integration, albeit small approximations.  In addition to this, data collected from real diseases are typically very noisy in the first place.  \cite{chang2017} discovered that an SIR model may be well approximated by a linear model.  We use this approach here to estimate $\rr$.

Specifically, we fit two linear polynomials in \(t\) with degree \(K= 10\) to \(X_{obs}\)
and \(Y_{obs}\) using least squares to find the coefficients $\{(\hat{x}_k,
\hat{y}_k)\}_{k=1, \dots, K}$,
\begin{align*}
\hat{X}(t) &= \sum_{k=0}^K \hat{x}_k t^k\\
{\hat{Y}}(t) &= \sum_{k=0}^K \hat{y}_k t^k
\end{align*}
Then, we estimate the derivatives as
\begin{align*}
\hat{X}^\prime(t) &= \sum_{k=1}^K k \hat{x}_k t^{k-1}\\
\hat{Y}^\prime(t) &= \sum_{k=0}^K k \hat{y}_k t^{k-1}
\end{align*}
Following,  \(\rr\) is derived from the ODEs in Equation \eqref{eq::sir},
\begin{align}
  - \frac{X^\prime}{Z^\prime}&= \rr \frac{X}{N} \nonumber\\
\rr &=       \frac{X}{ X + Y} \cdot \frac{N}{X} \nonumber\\
  \hat{\rr} &= \frac{\hat{X}^\prime(0)}{ \hat{X}^\prime(0) + \hat{Y}^\prime(0)} \cdot \frac{N}{\hat{X}(0)}. \nonumber
  \end{align}
Like in the previous cases, we are assuming the true models are
\begin{align*}
X(t) &= \sum_{k=0}^K x_k t^k + \epsilon_t\\
  {Y}(t) &= \sum_{k=0}^K y_k t^k + \epsilon_t,
\end{align*}
with $\epsilon_t \overset{iid}{\sim} N(0, \sigma^2)$.  Here, $K=10$ is rather arbitrary and should be selected using some criterion such as AIC in addition with the knowledge of how many data points are available to the user.  Besides optionally deciding on the degree of polynomials to fit, this model is simple to implement and gives very comparable results to using least squares with the SIR model.  The time $t=0$ to best capture the initial outbreak.  Again, we cannot use this model for inference, as our error assumption is violated.  Another disadvantage is that we are using the first time point $t=0$ to estimate $\rr$.  

\subsubsection{Linear Model Approximation, All Time Points (SIR LMAT)}\label{linear-model-approximation-all-time-points-degree-10}

The above formulation of a linear model approximation only uses the estimate at time $t=0$ to estimate $\rr$.  We can instead, use all time points available to estimate $\rr$.  We fit a linear polynomial of \(t\) with degree \(K= 10\) model to \(X\)
and \(Y\) as above, with a slight modification in how we calculate
\(\rr\),
\[\rr = \frac{1}{\# \text{ Obs }t}\sum_t \frac{\hat{X}^\prime(t)}{ \hat{X}^\prime(t) + \hat{Y}^\prime(t)} \cdot \frac{N}{X(0)} \]
The intuition is that $\frac{X^\prime}{X^\prime + Y^\prime}$ is constant in $t$, but due to our approximations with the linear model, this is no longer the case.  Here, we average over the different possible values of $\rr$, estimated at different times.  An advantage to this approach is that we have a more robust estimate of $\rr$ than just using one time point.  The disadvantages are the same as before.

\subsubsection{Max of Data (SIR Max)}\label{max-of-data}
The I ($Y$) compartment of the SIR model has one peak.    We note that \(Y^\prime(t) = \frac{\beta XY}{N} - \gamma Y = 0\) when \(\rr = \frac{\beta}{\gamma} = \frac{N(t)}{X(t)}\). The max
of \(Y\) occurs when \(Y^\prime = 0\) and hence
\[\rr = \frac{N}{X_{obs}(t^*)},\] where
\(t^* = \text{arg max}_{t} Y_{obs}(t)\).   A major disadvantage to this method is that it only uses one sample, making it a highly variable statistic.  An advantage of this method is that it places no distributional assumptions on any parameter.

\subsubsection{Smooth Maximum of Data (SIR SMax)}\label{smooth-maximum-of-data}

We use a spline with 4 degrees of freedom to fit both \(X\), and \(Y\).
We then apply the principle as above, except now,
\[\rr = \frac{N}{\hat{X}(t^*)},\] where
\(t^* = \text{arg max}_{t} \hat{Y}(t)\).   This approach is a statistical approximation of the previous method of using the smoothed max to estimate $\rr$ instead of a single point.  In this way, we can utilize all our data.  Another advantage is the flexibility in model choice in that we can use a spline, linear regression, or any other model which gives us predictions for the number infected and number of susceptibles.  However, despite this flexibility in model choice, one must remember that we assumed an underlying SIR model for the derivation of $\rr$.  Also, it may be unclear how to estimate the variance from this method.

\subsubsection{Incidence to Prevalence
  Ratio (SIR IPR)}\label{incidence-to-prevalence-ratio}
The incidence to prevalence ratio (IPR), described by \cite{Nishiura2009} is another intuitive method to calculate $\rr$ as it incorporates some of the most basic epidemiological quantities, incidence and prevalence.

In terms of data from the SIR model, incidence $J(t) \approx -(X(t+1) - X(t))$, and the IPR$(t) = \frac{J(t)}{Y(t)}$. The \textit{actual} reproduction number, $R_a(t) = \text{IPR}(t)\cdot D$ where $D = 1 /\gamma$ the duration of the infection.  This method assumes that we have some prior knowledge about $\gamma$, the transmission rate.  Thus we use as our estimate,
\begin{align*}
\rr &= \textnormal{IPR}(t) \cdot D
\end{align*}

Here we assume that the time step is small enough to approximate the incidence.  The advantage of this method is that incidence data is generally readily available as is prevalence data for certain diseases such as HIV.  However, as one is required to have prior knowledge about $\gamma$, it may be easier to directly estimate $\rr$ with one of the many other methods described that does not require a prior knowledge about $\gamma$.  Again, we are using only one time point to estimate $\rr$.

\subsubsection{Smoothed Incidence to Prevalence Ratio (SIR SIPR)}
We use the same method as above, IPR, but first fit splines with 4 degrees of freedom, $\hat{X}(t)$ and $\hat{Y}(t)$, , to fit to $X_{obs}$ and $Y_{obs}$, respectively.  Then  $J(t) \approx -(\hat{X}(t+1) - \hat{X}(t))$, and the $\hat{\textnormal{IPR}}(t) = \frac{J(t)}{\hat{Y}(t)}$.  Then
\begin{align*}
\rr &= \hat{\textnormal{IPR}}(t) \cdot D
\end{align*}
The advantage of this method is that it creates a less noisy estimate than estimating IPR using only one point.  It has the same disadvantages as the regular IPR ratio in that it requires knowledge about $\gamma$.

%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Log-Linear (SIR LL)}
Recently, \cite{harko2014exact} were able to reduce the SIR model to one ODE.  From this, we can derive the following,
\begin{align}
  X(t) &=  X(0) e^{\frac{\beta}{N \gamma}Z(t)} \nonumber\\
  \log \frac{X(t)}{X(0)} &=  \frac{\beta }{\gamma N }Z(t) \nonumber\\
  N\log \frac{X(t)}{X(0)} &=  \rr Z(t) \label{eq:harko_lin}
\end{align}
If we add error into Equation (\ref{eq:harko_lin}), then we have
\begin{align}
N  \log \frac{X(t)}{X(0)} &=  \rr Z  + \epsilon_t\label{eq:r0_harko}
\end{align}
If we assume $\epsilon_t \overset{iid}{\sim}N(0, \sigma^2)$, then we can directly estimate $\rr$ via least squares linear regression.  This method greatly simplifies the computational effort needed to estimate $\rr$ as it does not require any numerical integration.  Another advantage is that unlike using the linear approximation method, the coefficient from this model is directly interpretable.  In words, $\rr$ means that for one more recovery, we expect $N \log \frac{X(t)}{X(0)}$ to increase by $\rr$.  A disadvantage is that again, $\epsilon_t$ are not likely iid.  Another disadvantage is that this does not generalize to more complicated CMs.

\subsubsection{Markov chain estimation (SIR MC)}
A natural approach to epidemic modeling is that of Markov chains (MC), since it is assumed an individual's next state is only dependent on its current state.  Much work has been done over the years in this specific field including asymptotic behavior, continuous time MC, confidence intervals, and more \citep{jacquez1991,gani1995,daley2001epidemic}.  We present one simple instantiation of the model, the discrete time case, which traces its origin back to the Reed-Frost model \citep{abbey1952}.

In this framework, the number of susceptibles at the next step, $X_{t}$, has a Binomial distribution based on the contacts with the current number of infected, $Y_{t}$ and the current number of susceptibles.  That is $X_{t+1} \sim \text{Binom}(X_t, \alpha^{Y_t}$), where $\alpha$ is the probability of avoiding infection from an infective.  \cite{barbour2004} report that the reproduction number is thus,
\begin{align}\label{eq:r0-mc}
\rr &= \log \left ( \frac{1}{1-\alpha}\right ).
\end{align}

Thus, using regression on susceptible/infection counts will lead to an estimate of $\alpha$ and hence $\rr$.  However, this framework typically allows more than just the reproduction number to be estimated.  Through recursion, one can calculate the probability of having a given number of susceptibles and infected at each time step, and hence the entire distribution may be known.





\subsubsection{Sequential Bayes (SIR SB)}\label{sec:seqbayes}

Described by \cite{bettencourt2008} and summarized in \cite{obadia2012r0}, the sequential Bayes method is a Bayesian approach to an approximation of the classic SIR model.  The approximated SIR model assumes that the incidence at $t+1$, $J(t+1)$ has a Poisson distribution, with $\gamma$ as the  average inverse of the infectious period and $\mathcal{R}$ as the effective reproduction number, which we take here to be $\rr$. In order to estimate $\rr$, we must have some idea about $\gamma$,
\begin{align*}
J(t+1)  \sim Poisson( J(t) \exp \left \{  \gamma (\rr-1)\right \})
\end{align*}
Then, the posterior distribution of $\rr$ given the previous days' incidences is
\begin{align*}
  P(\rr | J_0, \dots, J_{t+1}) = \frac{P(J_{t+1} | \rr, J_0, \dots, J_t)P(\rr| J_0, \dots, J_t)}{P(J_0, \dots, J_{t+1})}.
\end{align*}
This method is sequential in that the prior distribution for $\rr$ comes from the previous day.  The initial prior for $\rr$ is assumed to be flat.  This method results in a posterior distribution from which credible intervals may be obtained.  This method assumes, initial growth in incidence to be exponential, and homogeneous mixing of populations as with any compartment model.  The advantages of this method are that of the ability to obtain an entire posterior distribution, whereas many other methods are difficult to even find an estimate of the variance.  Disadvantages include strict assumptions about the distributions and computational time required to estimate the relevant parameters. 




\subsubsection{SEIR Model (SEIR)}
\label{sec:seir-model}

The SEIR model is a common adaptation, especially in recent years, of the SIR compartment model used to model infectious diseases such as influenza and Ebola \citep{mills2004,althaus2014}.  The ``E'' compartment stands for ``exposed'' and represents the stage where individuals are infected but not yet infectious.  The model is described by the following set of equations, as shown by \cite{cintronarias2009}.  This particular model  does not include changse in population size but can be adapted to do so,
\begin{align*}
  \frac{dX}{dt} &= - \frac{\beta XY}{N} \\
  \frac{dE}{dt} &= \frac{\beta XY}{N}  - \alpha E\\
  \frac{dY}{dt} &= \alpha E - \gamma Y \\
  \frac{dZ}{dt} &= \gamma Y.
\end{align*}

Then $\rr = \beta / \gamma$, just as in the SIR model.  As in the SIR model, $\beta$ and $\gamma$ have the same interpretation and additionally, $\alpha$ is the rate at which exposed individuals become infectious.  The methods used to estimate the parameters $\beta$ and $\gamma$ are of the same types used to estimate those parameters in the SIR model such as least squares and maximum likelihood estimation.  Here, we see a very strong example of how $\rr$ is a property of the model and why it is important for this model to be correct.  An advantage of this model is that the addition of an exposed class is thought to better mimic reality of the transmission of many diseases.  A major disadvantage is that exposure counts are typically back-estimated from incidence counts, and noise is rarely considered in this estimation of the exposure counts.   Typically, some form of maximum likelihood estimation is used to estimate parameters in this model, which places strict assumptions on each of the compartments \citep{althaus2014}.

\subsubsection{MSEIR model (MSEIR)}
\label{sec:mseir-model-mseir}

\begin{figure}[h]
\centering
\begin{tikzpicture}[node distance=3cm,auto,>=latex',every node/.append style={align=center}]
  \node [int,  fill = white!70!blue] (s)              {$S$};
  \node[int, fill = white!70!blue] (m) [left of=s] {$M$};
  \node [int,  fill = white!70!blue] (e) [right of=s]             {$E$};
  \node [int,  fill = white!70!blue]           (i) [right of=e] {$I$};
  \node [int,  fill = white!70!blue] (r) [right of=i] {$R$};
  \path[->, auto=false] (s) edge node {} (e)
  (e) edge node {} (i)
  (i) edge node {} (r)
  (m) edge node {} (s)
  ++ (0,2) edge node {} (m)
  (s) ++ (0,2) edge node {} (s);
  \path[->]   (m) edge node {}  ++ (0,-2);
  \path[->]   (s) edge node {}  ++ (0,-2);
  \path[->]   (e) edge node {}  ++ (0,-2);
  \path[->]   (i) edge node {}  ++ (0,-2);
  \path[->]   (r) edge node {}  ++ (0,-2);
\end{tikzpicture}
\caption{Depiction of a MSEIR model with birth and death rates.  $M$ - passive immunity from mother; $S$ - susceptible; $E$- exposed, latent; $I$- infected; $R$ - recovered.  An arrow out of the system indicates death and an arrow into the system indicates birth.}\label{fig::mseir}
\end{figure}


This compartment model approach includes not only the exposed, latent compartment $E$ but additionally the compartment $M$ which stands for passive immunity inherited from the mother.  \cite{hethcote2000} describes the model with the following set of ODES that include birth rate $b$ and death rate $d$, respectively (reparametrized to match our notation),
\begin{align*}
  \frac{dM}{dt} &= b(N-X) - (\delta + d) M \\
  \frac{dX}{dt} &= bX + \delta M - \frac{\beta XY}{N} - dX\\
  \frac{dE}{dt} &= \frac{\beta XY}{N} - (\alpha + d) E\\ 
  \frac{dY}{dt} &= \alpha E - (\gamma + d) Y \\
  \frac{dZ}{dt} &= \gamma Y - dZ \\
  \frac{dN}{dt} &= (b-d)N,
\end{align*}
where $\delta$ is the rate of passive, immune infants transitioning to susceptible indnividuals, $\beta$ is the rate of infection, $\alpha$ is the rate of of infectious individuals becoming infected, and $\gamma$ is the rate of recovery.  The reproduction number is then,
\begin{align}\label{eq:r0-mseir}
\rr &= \frac{ \beta \alpha}{ (\gamma + b)(\alpha + b)}.
\end{align}
\cite{hethcote2000} note that this calculation of $\rr$ may be strictly derived through interpretation of the definition, as ``$\rr$ is the product of the contact rate $\beta$ per unit time, the average infectious period adjusted for population growth of $\frac{1}{\gamma + b}$ and the fraction $\frac{\alpha}{\alpha + b}$ of exposed people surviving the latent class $E$.''  They note that this model is well suited for diseases such as measles, rubella, or mumps where the mother may pass some antibodies to her newborn.  This expression may also be derived using the next generation operator described in Section \ref{sec:ngm}.

Again, this is simply a model for transmission of a disease and does not specify how $\rr$ is to be estimated once the model is written down.  Method such as least squares and MLE are common ways to estimate the parameters in this model and hence $\rr$.  When the birth rate $b=0$, note that $\rr$ reduces to the derived expression for both the SIR and SEIR models without birth and death rates, $\frac{\beta}{\gamma}$.




\subsubsection{Amplifier Model (AMP)}
As mentioned previously, compartment models may be arbitrarily complex.  We demonstrate this idea with a set of compartments for each strain of a disease like influenza or HIV.  \cite{blower2004} discuss the case where a disease may have different strains and consequently a different reproduction number for each strain.  They introduce a multistrain mathematical model called the amplifier model to deal with this case.

This model is based on the the SEIR compartment model, accounting for a number of strains simultaneously.  There is a set of ODEs for each strain of the disease with different parameters for recovery, transmission rate, and drug resistance of the disease.  Additionally, there is mixing of the different strains as ``the amplifier model also allows for immigrating and emigration of individuals of all types, as well as reinfection of latently infected individuals'' \citep{blower2004}.  Ultimately, their formula for $\rr^{(i)}$ where $i$ stands for the $i$th strain is
\begin{align*}
\rr^{(i)} = X^* \frac{ ( \beta_i^Y + \beta_i^E)\alpha_i + \beta_i^Y d_i^E}{(\alpha_i + d_i^E)(c_i + k_{i,i+1} + d_i^Y)}.
\end{align*}
Here, $X^*$ is the number of susceptibles individuals in the disease-free equilibrium in the absence of immigration or emigration; $\beta_i^E$ and $\beta_i^I$ are strain-specific transmission rates for the latent, and infected classes, respectively; $\alpha_i$ denotes the progression rate from latently infected to disease for strain $i$; $d_i^E$ and $d_i^{Y}$ are death rates for the latent and infected classes, respectively; $c_i$ is the cure rate; and $k_{i, i+1}$ is a parameter associated with the degree of amplification of drug resistance.  They note that ``each reproduction number is the product of average number of secondary infections caused per unit time, the average time a case remains infectious and the probability that an infected individual develops disease.''

In the amplifier model, $\rr$ was derived from using the stability point (disease free equilibrium state in the absence of immigration or emigration) of the system by solving the characteristic polynomial for the eigenvalues.  This derivation is explained in more depth in Section \ref{sec:ngm}.

We describe this model as opposed to other available CMs because it shows not only how complex CMs can become, but also to demonstrate how researchers approach the often controversial assumption of homogeneity.  In this case, we have strain-specific transmission rates, so we are essentially stratifying compartments into different groups, allowing for some heterogeneity.  A disadvantage is the amount of strain-specific data required to estimate all parameters, which may be estimated through the usual methods such as least squares and maximum likelihood estimation.


\subsubsection{Next Generation Model (NGM)}
\label{sec:ngm}

Very complicated models have arisen from stratification of compartments such as one with 26 compartments \citep{pandey2014}.  Thus, there is a need to be able to derive an expression for $\rr$ regardless of the structure of the compartment model.   The Next Generation Model (NGM) is a generalization of any compartment model at the infection-free steady state. This model solves the problem of having an expression for $\rr$ in terms of the epidemiological parameters for a general compartment model.  Originally, introduced by \citep{diekmann1990}, \cite{diekmann2009} posit a recipe to find $\rr$ for a wide class of compartment models, including models we discussed, SIR, SEIR, and MSEIR. We summarize that recipe here. First, define the infected subsystem as ``the equations of the ODE system that describe the production of new infections and changes in state among infected individuals.''  Let $x = (C_1, C_2, \dots, C_m)$ where $C_i$ are the different compartments of the infected subsystem.  The steps to find $\rr$ are as follows:


\begin{enumerate}
\item ``Linearize the infected subsystem of nonlinear ODEs about the infection-free steady state''
\item Decompose the linearized infected subsystem into $(T + \Sigma )x$ where $T$ is a $m\times m$ matrix of transmissions and $\Sigma$ is a $m \times m$ matrix of transitions.
\item $\rr$ is the spectral radius (i.e. the dominant eigenvalue) of $K_L=-T \Sigma^{-1}$.  
\end{enumerate}

Here, $T_{ij}$ is the rate of transmission of \textit{newly} infected individuals in state $i$ created by individuals in state $j$.  $\Sigma_{ij}$ is the transition rate of individuals into compartment $i$ from compartment $j$.

This method has the assumptions of a basic compartment model: homogeneous mixing among populations and the law of mass action, i.e., that the number of infected individuals are proportional to the number of infected individuals at the previous step.   Additionally, this method is advantageous in that the matrix $-\Sigma^{-1}$ has intuitive meaning.  The $ij$th entry of $- \Sigma^{-1}$ is the ``expected time that an individual who presently has state $j$ will spend in infected state $i$.''

Although, this method is useful for deriving an expression for $\rr$, it is unclear how to actually estimate it from these expressions.  One could use maximum likelihood if one specifies distributions for how compartments change over time or least squares may be applied. In general, adding numerous compartments is often tedious and difficult and needlessly complex.  Instead, researchers have started to lean towards ABMs when introducing heterogeneity in their models, which we describe in Section \ref{sec:network}.  

%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Exponential Growth}\label{sec:exp-growth}
Typically, CMs expressed through ordinary non-linear differential equations have no known analytical solutions, which makes deriving a closed-form expression for $\rr$ difficult or impossible.  One simplification that is often introduced to the SI-framework is that of exponential growth, or that the number of infections in the beginning of an epidemic mimics that of an exponential curve.  This simplified expression has allowed for many derivations of $\rr$.


\subsubsection{Exponential Growth (EG)}
\label{sec:expgrowth}
\cite{wallinga2007generation} report that the effective reproduction number $\mathcal{R}_t$ and hence the initial reproduction number $\rr$ may derived using the fact that infection ``counts increase exponentially in the initial phase of an epidemic.''  We then have to estimate $r$, the \textit{per capita} change in the number of new cases per unit of time and $\omega$ the serial interval, the distribution of time between a primary and secondary infection. Then, we have
\begin{align}\label{eq:lotka}
\rr = \exp{(r \omega)}
\end{align}
or its first order approximation
\begin{align}\label{eq:anderson}
\rr = 1 + r \omega.
\end{align}
Equation \eqref{eq:lotka} is derived from a demographic view using the Lotka-Euler survival equations which come from the fields of demography, ecology, and evolutionary biology, whereas Equation \eqref{eq:anderson} is derived through an epidemiologist's perspective. 

\cite{wallinga2007generation}, instead, use a moment generating function expression for $\rr$.  With $\omega(t)$ as the serial interval, then
\begin{align*}
\rr^{-1} &= \int_{a=0}^\infty e^{-rt}\omega(t)dt.
\end{align*}
For all these above estimates of $\rr$, we observe the duration of serial intervals in a period of exponential growth.  Deciding when exponential growth occurs and which data points to use to estimate $\rr$ is difficult.  \citeauthor{wallinga2007generation} specify $\omega$ in multiple ways.  Parametrically, they use exponential, normal, and point-wise distributions.  Additionally, they discuss using the the empirical distribution of the serial interval.

\citeauthor{wallinga2007generation} note that specifying the epidemic model ``implicitly specifies a generation interval distribution.''  The above equations are under the SIR framework, but this method may be adapted to acommodate other frameworks.

\subsubsection{Maximum Likelihood Estimator of Secondary Infections (MLSI)}\label{sec:mle-si}
A common method used to estimate parameters in the SI-framework is maximum likelihood estimation.  This method requires assuming the likelihood of a parameter given the data observed, and hence many distributions may be specified.  The MLE of secondary infections, described by \cite{forsberg2008}, finds the estimate of $\rr$ by maximizing the likelihood function under a certain assumptions.  We assume that the ``the number of secondary cases produced by an infected individual follows a Poisson distribution, and that the serial interval is described by a multinomial distribution.''  Recall, the serial interval is the distribution of time between a primary case developing symptoms and a case infected by the primary case developing symptoms.  An approximate version of this can be simplified to a thinned Poisson  where
\begin{align}\label{eq:mlesi}
  L(\rr, \mathbf{p}) = \prod_{t=1}^T \frac{e^{- \rr \sum_{j=1}^{\min(k,t)}J_{t-j}p_j}\left (\rr \sum_{j=1}^{\min(k,t)}J_{t-j}p_j \right )^{J_t}}{J_t!}.
\end{align}
Here, $k$ is the maximal amount of the serial number (the maximum amount of time between infections), $T$ is the total time and $p_j$ is the probability of displaying symptoms on day $j$ after being infected.  The number of cases on day $t$, $J_t$ are observed incidence counts.

This method assumes that the number of new cases caused by an infected individual follows a Poisson distribution with mean $\rr$ and that hte serial interval follows a multinomial distribution. Additionally, this method also assumes a period of exponential growth.  An advantage of this specification is that we can write down a nice likelihood for $\rr$. Two disadvantages are that we only have an approximation for $\rr$ and have to decide which data points to use in a certain time frame.  Additionally, we have to validate our assumptions about the specified distributions.  This method was used to estimate $\rr$ for Ebola, Avian Influenza, and Swine Flu.


%%%%%%%%%%%%% 5

\subsubsection{Time Dependent Reproduction Number (TDR)}\label{sec:timedep}
Another likelihood based approach to estimate $\rr$ is that of the time dependent reproduction number, as shown by \cite{wallinga2007generation} and summarized by \cite{obadia2012r0}.  They maximize the likelihood that case $i$ has been infected by case $j$ at certain time using the serial interval, $w(t)$ where $t_i$ is when case $i$ was infected and $N$ is the total number of cases,
\begin{align*}
  p_{ij} &= w(t_i- t_j) / \sum_{i \neq k} w(t_i - t_k),\\
  \rr &= \frac{1}{Y(0)}\sum_{i=1}^N p_{i0}
  \end{align*}
  A disadvantage of this method is that it assumed we have knowledge about the serial interval distribution, $w(t)$, but an advantage is that the method works with any specification of $\omega$.  This method also requires that the initial number of susceptibles $X(0)$ is roughly the size of the total population.  Another advantage is the explicit use of distributions to indicate that $\rr$ is indeed a random variable.  This method was used to estimate $\rr$ for Influenza A.

\subsubsection{Initial Growth Rate and Final Size (IGRFS)}
\label{sec:igr-fs}

Initial growth rate, as described by \cite{dietz1993estimation} is derived by \cite{anderson1986}, initially to study the spread of HIV.  Initial growth rate estimates $\rr$ through
\begin{align*}
\rr = \frac{D \ln 2} {t_d} + 1,
  \end{align*}
  with $D$ as the average incubation period and $t_d$ as the doubling time during its early stages.  This method assumes one knows when the ``early stage'' ends along with knowledge of the average incubation period.  

  Final size, on the other hand, only looks at $\rr$ once a disease has run its course.  Here $\mu_0$ is the proportion of initially infected individuals and $\mu_\infty$ is the final, cumulative proportion of infected cases at the end of an epidemic,
\begin{align}\label{r0_attackrate}
\rr =  \frac{\log \left ( \frac{X(0)}{\mu_{\infty} N}\right )}{\mu_\infty - \frac{X(0)}{N}}
\end{align}
This method can only be used to assess $\rr$ after a disease has passed through a population.  Additionally, if either $\beta$ or $\gamma$ were affected during the disease's lifetime, such as through interventions and prevention strategies, then $\rr$ cannot be properly assessed.

  These two methods allow one to look at $\rr$ at both the beginning and the end of an epidemic.   For the initial growth rate, while one can see $\rr$ at early stages, estimating the doubling time $t_d$ is difficult.  For final size, one has to wait until the end of an epidemic to calculate $\rr$, and prevention strategies may skew the final result.  These methods were used to estimate $\rr$ for AIDS and smallpox.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  \subsection{Estimation from Networks}
As estimating $\rr$ is concerned with the number of new infections based on the contacts of old infections, a network is a seemingly intuitive way to model such contacts and hence infections.  Networks that have been used to estimate $\rr$ include contact tracing of actual individuals, branching processes to model the spread of disease, and agent-based models (also known as individual level models), which all leverage networks along with different sources of data to generate estimates of $\rr$.  Additionally, networks provide an intuitive framework in which individuals can interact with varying levels of heterogeneity.  However, when using a network approach, we must be careful of our interpretation of $\rr$ as we often are conditioning on certain individuals.    \cite{breban2007} note that calculating $\rr$ from a network approach may give misleading results due to conditioning on the network structure.
  
\label{sec:network}

%%%%%%%% 5
\subsubsection{Contact Tracing (CT)}
\label{sec:contact_tracing}
A reasonable idea to estimate $\rr$ is to directly observe the number of secondary infections produced by an initial infector.  However, this approach is generally impractical as it is difficult to estimate every single contact an individual may have.   With seriological information, we could conceivably recreate the transmission of the disease throughout a population.  Even in this case it is unlikely, for one, to have a completely susceptible population, which is essential to an unbiased estimate of $\rr$.  Moreover, this would implicitly be conditioning on the initial infector's contacts which may not reflect those of the rest of the population.  We write the calculation for $\rr$ under contact tracing in Equation \ref{eq:r0_contacttracing}, where $X_0$ is a random variable denoting the number of individuals in the population individual 0 infects.  Here $X(0)=N$ indicates that the initial number of susceptibles is $N$, the size of the population,
\begin{align}\label{eq:r0_contacttracing}
\rr = E[ X_0 | X(0) = N].
\end{align}
\cite{eames2003} describe contact tracing as an ``extreme form of targeted control, where the potential next-generation cases are the primary focus.'' That is, one focuses on treating the contacts of an infected individual rather than applying uniform treatment across a population.

They describe an estimate for $\rr$ for a SIR model on a network, with $r$ as the rate of transmission across a contact multiplied by the infectious period and $n$ as the number of contacts as
\begin{align*}
\rr = r(n-2)
\end{align*}
Again, the difficulties of this method lies in one, estimating the contacts of an initial infected individual and two, extrapolating the contacts of the initial individual to the population as a whole.  As such, although important in treating an outbreak of a disease in its early stages, is not the best choice to estimate $\rr$ due to the high level of uncertainty inherit in the method and due to conditional assumptions of the initially infected individual.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Branching Process (BP)}
\label{sec:branching-process}

Branching processes are models for population growth which track generations of new offspring.  We describe branching processes as adapted for epidemic modelling from \cite{grimmett1992}.  An infector $i$ may produce new infections, called offspring, over the duration of the  infector $i$'s disease.  The offspring of the original infector directly produces are called the first generation.  The offspring of the first generation are called the second generation and so on.  This process is illustrated in Figure \ref{fig:bp}.  An important assumption in branching processes is that the offspring produced from different infectors are indepedent and identically distributed, and so for a branching process to be a valid model in epidemic modeling, we must assume a large population with few infectors.

\begin{figure}
  \centering
\begin{tikzpicture}
  [
    grow                    = right,
    sibling distance        = 4em,
    level distance          = 10em,
    edge from parent/.style = {draw, -latex},
    every node/.style       = {font=\footnotesize},
    sloped
  ]
  \node {Case 0}
  child { node {Gen. 1}
    child {node{ $\dots$}}
  }
  child {node {Gen. 1}
    child{ node  {$\dots$}}
  }
  child{ node  {Gen. 1 }
    child{ node  {Gen. 2}
    }
    child{ node {Gen 2}
      child{ node  {$\vdots$}}
      child{ node  {$\vdots$}}
      child{ node  {$\vdots$}}}
  };
\end{tikzpicture}
\caption{Illustration of a branching process.}\label{fig:bp}
\end{figure}

\cite{getz2006} give a way to calculate $\rr$ from a branching process.  ``We define the offspring distribution $\{q_i \}_{i=0}^\infty$, where $q_i$  is the probability that an infectious individual infects $i$ other individuals.  Thus we require $\sum_{i=0}^\infty q_i =1$ and note that $\rr$, the mean number of cases contracting disease from each infective'', is simply given by'
  \begin{align*}
    \rr = \sum_{i=0}^\infty iq_i.
  \end{align*}
  It is assumed we know how to estimate the $q_i$, and so in this case, it is not too different of an approach than contact tracing.  The disadvantages of this method are similar to that of contact tracing as data about which individuals  infected another is very difficult to obtain.  An advantage is that this method directly gets to the ``heart'' of the definition of $\rr$ as it is concerned with generations of infected individuals rather than incidence at a certain time.


    

\subsubsection{Agent-Based Models (ABM)}
\label{sec:agent-based-models}
Agent-based models (ABMs) or individual level models (ILMs) are ``bottom-up'' models, meaning micro patterns are simulated and macro patterns are then inferred from these micro patterns.  ABMs consist of agents which represent individuals (e.g., people, mosquitoes, poultry) and a series of activities in which the agents can affect one another and evolve over time.

In epidemiology, ABMs may be used to simulate the spread of disease.  ABMs allow for every detail of an infection to be known, in the context of estimating $\rr$, meaning we know exactly who is infected by whom and when.  Let $L$ be the number of simulations run and $X_{n \ell}$ be the number of infections caused by agent $n = 1, 2, \dots, N$ who was infected at time $t=0$ in simulation $\ell$.   Thus $\rr$ can be estimated simply through
\begin{align*}
  \rr =& \frac{1}{LN}\sum_{\ell =1}^L\sum_{n=1}^NX_{n \ell}
\end{align*}
Examples of estimations of $\rr$ using ABMs/ILMs are in \cite{breban2007,ahmed2013variance}.

There are many advantages to using ABMs, a main one being able to run the model multiple times under the same initial conditions.  In this way, one can easily obtain variance estimates of $\rr$ in addition to empirical distributions of it.  Moreover, this estimation can accommodate any model specification of how the disease is transmitted through the population.  However, the major disadvantage of the ABM lies in calibrating the evolving of agents over time to accurately reflect reality, a problem that cannot be understated.  Thus, it is typically necessary to have knowledge of parameters such as the serial interval, transmission rate, recovery rate, etc.  Additionally, we need to have accurate agents in characteristics and their implied network structure.  ABMs are flexible in that they may be used with any model in the SI framework.  The only requirement is that we track who infects whom and can adapt for any additional states such as exposure or prevention strategies such as closing schools \citep{fred}.







\subsection{Variance Methods}
\label{sec:methods}

Estimating $\rr$ is difficult and estimating $V[\rr]$, the variance, is even more so.  We describe general methods which may be applicable to estimate the variance.  We describe the assumptions that are used along with each method and note that they may not be applicable to all of our methods.  Still, reporting confidence intervals is very important, and we hope to encourage this practice by describing the below methods used to estimate models in the SIR framework in a simple fashion.  Regardless of the method used to estimate variance, confidence intervals, or credible intervals, we encourage researchers to carefully examine all underlying assumptions of 1) specifying a model for disease transmission, 2) estimating $\rr$ from the model, and 3) the method used to estimate the variance or CIs.


\subsubsection{Delta Method}\label{delta-method}

When the method estimates \(\beta\) and \(\gamma\) instead of \(\rr\) directly, we use the delta method approximation to calculate the
variance of \(\rr\). Here, we know \(\rr = h(\beta, \gamma) = \frac{\beta}{\gamma}\). Then \(\bigtriangleup h = (\frac{1}{\gamma},  -\frac{\beta}{\gamma^2})^T\) and \(V[\rr] = \bigtriangleup h^T \Sigma_{\beta, \gamma} \bigtriangleup h\), where \(\Sigma_{\beta, \gamma}\) is covariance matrix of \(\beta\) and \(\gamma\).

This estimate assumes that the distribution of $\beta$ and $\gamma$ are asymptotically normal.  Here, we use the relationship of $\rr = \frac{\beta}{\gamma}$ in order to estimate the variance, which is specific to the SIR framework.  This method may be extended to other frameworks.  The advantages of the delta method are that as the NGM posits a recipe to derive $\rr$, then it is theoretically possible to use the delta method to derive an expression for the variance of $\rr$ for any compartment model.


\subsubsection{Posterior Distribution}
When one uses a Bayesian approach to estimate $\rr$, the result should be a distribution rather than an estimate.  One can simply look at the quantities of this distribution to form a credible interval for this estimate.  Advantages of this method include having an entire distribution compared to a point estimate.  One disadvantage is that the Bayesian approach will not guarantee frequentist properties that epidemiologists may be looking for such as being able to compare diseases to one another.



%%%%%%%%%%%%%%%




\section{Analysis and Results}
\label{sec:results}

\subsection{Comparing Models}

\textit{With the many different ways to estimate $\rr$, we want to compare them all to see which ones give more accurate and reliable estimates.}  As there is no way to know the ``true'' $\rr$ of an actual disease we compare our methods by fitting them to simulated data sets and look at the resulting $\rr$ estimates and standard errors. Our main comparisons are done with data generated from the SIR model with random error added so that we can compare model estimates to a known true $\rr$ value. We vary the conditions under which the SIR data is generated to see how the results from the estimators change. We also look at data from different functional models to see how the estimators perform under different circumstances.

%% What do you mean by describing the standard standard deviations?

\textit{To see the effects of different conditions for the data, we change one of the starting conditions while holding all of the other ones constant. We try changing two conditions: the starting number of susceptible and infected people in the population (holding the total population constant) and the variance of the errors added on to the The starting population sizes for the $X$ and $Y$ are set to ($X_0$, $Y_0$) =  (99999, 1), (99990, 10), (99900, 100), and (99000, 1000), holding all other conditions constant. The variance of the added errors by scaling them to: ($\sigma_X$, $\sigma_Y$) = (10, 1), (100, 10), (500, 50), (1000, 100), (2000, 200), (5000, 500), (10000, 1000), (15000, 1500); these are done with ($X_0$, $Y_0$) = (99000, 1000). The errors start very small but become very large relative to the actual compartment sizes; this allows us to more clearly see the effects of the changes.}

%% Describe the initial S, I, R values
%% Describe the different error methods
%% Describe the standard standard deviations involved
%% High level fitting comments

%% Throw fitting details into the appendix

%% Talk about top 3-5 methods
%%%%%% Honestly, all the methods seem to have flaws in one way or another - maybe point out why?
%% Talk about methods that really miss
%% Talk about the standard error reports
%% Talk about 4 categories.  Does anyone do worse/better?
%% What about the one where we cannot estimate std
%% What happens when SD is large?


%% Throw some graphs in


\subsubsection{SIR Data}

\textit{The datasets from the SIR model are simulated from the ODEs: \begin{align}
	\systeme{\frac{dX}{dt} = -\frac{\beta XY}{N}, \frac{dY}{dt} = \frac{\beta XY}{N} - \gamma Y, \frac{dZ}{dt} = \gamma Z}.
	\end{align}
} 

We use arbitrarily selected the parameters to generate the time series data set from the SIR model. We first generate "baseline" data from which we can make comparisons by changing the  conditions under which the data were generated. The baseline SIR data are simulated with the following conditions:

\begin{center}
	
	$\beta$ = 0.06, $\gamma$ = 0.05; $\rr$ = 1.2
	
	Number of time steps: 365
	
	Starting X size ($X_0$): 99950
	
	Starting Y size ($Y_0$): 50
	
	Starting Z size: 0
	
	Total population: 100000.
	
\end{center}

%% Is this enough detail on describing the initial S, I, and R, or are we looking for something more specific?

\textbf{Our baseline initial size for $Y_0$ is 0.05\% of the total population in our simulated environment, which is fairly large compared to the starting point for the data in most epidemics. However, I/we felt that this was necessary in order to see the differences between the models.}

We then generate random errors that are added to the X and Y compartments at each time step in order to simulate some form of randomness. \textbf{The sum of these was then subtracted from the Z compartment in order to keep the total population in the system constant.} We generated data sets with two different types of errors: Gaussian and autoregressive. \textbf{The Gaussian errors are independently distributed with mean 0 and constant variance for the entire length of the time series. The autoregressive errors are generated from a first-order process of the form: $$ \epsilon_{t} = \rho * \epsilon_{t-1} + \delta_{t}$$, where $\rho$ is fixed throughout and $\delta_{t} \sim N(0, \sigma)$ with constant $\sigma$.} 

We also add a monotonicity adjustment to the X and Z compartments with both of the error models. \textbf{This is done using the Pool Adjacent Violators Algorithm (PAVA); the algorithm checks if the observation at each time step is greater than or equal to the previous one, and changes the value of both observations to be equal to the mean of the two if the condition is not fulfilled.} This gives us a total of four error models: autoregressive (Auto), autoregressive monotonic (AutoM), Gaussian (Norm), and Gaussian monotonic (NormM). For our baseline data, we have ($\sigma_X, \sigma_Y$) = (10, 1). We simulate 5 different baseline sets to see how consistent each of the models are.

\subsubsection{Other Models}
We simulated data from 6 other models besides the classic SIR model. 5 of these were polynomial linear regression models, from order 0 to 4 with respect to time/day. The coefficients used to generate data from the linear regression models were originally chosen by taking the approximate coefficient estimates from fitting the linear regression models to classic SIR data. This does not necessarily guarantee that the true $\rr$ will be 1.2, especially for the 0th and 1st order linear regression models. What we are interested in for these cases is to see if the models still give reasonable estimates for $\rr$. The other model is the linear SIR model, which is a three compartment model with the following ODEs:

	\begin{eqnarray*}
		\frac{dS}{dt} &=& -\beta I \\
		\frac{dI}{dt} &=& \beta I - \gamma I \\
		\frac{dR}{dt} &=& \gamma I.
	\end{eqnarray*}

 The data from the linear SIR model were simulated with parameters $\beta$ = 0.06 and $\gamma$ = 0.05, such that $\rr$ = 1.2. 
 
 \textbf{Random error is added to the data generated here as well, using the same 4 error models as above. The other specifications for these datasets are the same as for the baseline classic SIR data, namely the variance of the added errors and starting compartment sizes.}

\subsubsection{Variance Estimation}

\textbf{The method for calculating the standard errors for $\hat{\rr}$ depend on the model used. The following models take the standard error directly from the regression methods used to estimate $\rr$:}

	\begin{itemize}
		\item SIR LMA
		\item SIR LMAT
		\item SIR Log Linear
		\item SIR MC
		\item Exponential Growth
		\item Initial Growth
	\end{itemize}

\textbf{The ones that are calculated using the delta method:}
	
	\begin{itemize}
		\item-SIR LS
		\item SIR ReLS
		\item SEIR
		\item MSEIR
		%% \item AMP
		\item NGM
		\item Final Size
	\end{itemize}
	

\textbf{Standard errors for the maximum likelihood and branching process models are calculated using the negative reciprocal of the Fisher information. The sequential Bayes model uses the standard deviation from the posterior distribution. We use the jackknife method to estimate the standard error of the SIR smoothed max and the SIR smoothed IPR methods. The jackknife calculates the standard error using the standard deviation of estimates of $\rr$ after leaving out a single data point.}
	
\textbf{Some of the models use only one single point in time to estimate $\rr$ and therefore do not seem to have an innate source of variance. We currently do not have standard errors for the SIR Max and SIR IPR models.}

\subsection{Results}

\subsubsection{Survival Function}

%% How do we want to estimate this?????????

\subsubsection{Compartment Models}

\paragraph{SIR Models}
In general, the models derived from the SIR ordinary differential equations are relatively accurate. Most estimates are within the range of $\pm$ 0.2 of the true $\rr$ for the baseline SIR datasets. The magnitude of the standard errors on the baseline datasets for the different SIR models generally depend on the method used. The models that estimate $\rr$ using optimization or gradient descent, namely SIR LS and SIR ReLS, have standard errors on the order of $10^{-4}$ to $10^{-7}$. The standard errors are somewhat larger for the methods that use regression, on the order of $10^{-1}$ to $10^{-3}$. 

The SIR models as a whole also do fairly well with changing the starting number of infected people. However, they tend not to do as well when the variance of the added errors is very large; in these situations, many of the models tend to have very large standard errors, sometimes on the order of $10^{1}$ or $10^{2}$.

The exceptions to the above are the linear, incidence-prevalence ratio, and Markov chain models. The estimates tend to be far from the true $\rr$, and they fluctuate greatly between the baseline data sets. The standard errors for these models (if they exist) also tend to be very large under some of the changing conditions.

Many of the SIR models do not work as well with the data that come from the polynomial linear models. Specifically, for the 0th and 1st order model datasets, the estimates can be unreasonably large or negative. The Log linear, Sequential Bayes, Smoothed max, and Max models do not have this problem. 

\paragraph{Other Compartment Models}

All of these models use optimization to estimate the parameters that are used to calculate $\rr$. They tend to be fairly accurate and consistent on the baseline data, often as good as or better than those from the various SIR models. The SEIR models (least squares and next generation) do not handle the large added errors very well, as it tends to give ridiculously large results. The standard errors tend to be on the order of $10^{-2}$ to $10^{-3}$. However, as with the estimates above, the standard errors from the SEIR models can become unreasonably large when the variance of the added errors becomes large. Also, as with the SIR models, the $\rr$ estimates and standard errors from the 0th and 1st order polynomial linear model datasets are not very good, as the former is sometimes negative and the latter can be very large. The MSEIR model tends to do fairly well in all situations.

\subsubsection{Exponential Growth}

We have assumed for these models that the number of infections follows an exponential curve throughout the entire time frame of the data sets. This assumption certainly does not hold for the data from the 0th and 1st order linear models, and may not hold for some of the other data sets either. 

These models are generally not as accurate as the compartment models above. Most of them, except for the initial size model, tend to underestimate $\rr$, with estimates between 0.8 and 1.0. The standard errors are all fairly small, on the order of magnitude from $10^{-1}$ to $10^{-3}$. This is larger than many of the compartment model standard errors. The main advantage that all of these models, except for the maximum likelihood model, seem to be more robust to changing conditions and functional forms of the data compared to the compartment models. The estimates and standard errors for $\rr$ appear to be reasonable in all cases, including for very large added errors and for the 0th and 1st order linear model data.

\subsubsection{Network Models}

The branching process model (the only one in this category so far) is not very accurate. In particular, for the SIR datasets, the $\rr$ estimates are less than 1, while the true $\rr$ is greater than 1. The standard errors, though, are relatively small, around the same order of magnitude as the SIR least squares models. Model estimates and standard errors are fairly consistent across the baseline datasets and changing starting compartment sizes. However, increasing the standard deviation of the added errors tends to have a somewhat noticeable impact on model results. The model also does not handle the 0th order polynomial linear data well.


\section{Discussion}
\label{sec:dis}

%% What kind of graphs should we throw in?




\bibliographystyle{apa}%Choose a bibliograhpic style
\bibliography{Master}


\section{Appendix}

Included here are the technical details for fitting the models in R, as well as tables of the results.

\subsection{Compartment Models}

\subsubsection{SIR Least Squares}

\paragraph{Estimation}

For the least-squares SIR model, we estimate the parameters $\beta$ and $\gamma$ of the ODEs using optim() in R to minimize the sum of squared errors using Nelder-Mead. The standard errors for $\beta$ and $\gamma$ were estimated using the Hessian matrix given to us by optim(), and then the standard error for $\rr$ was calculated using the delta method.

\paragraph{Comments}

The $\rr$ estimates for the baseline data are both close to the actual value, and the standard errors were relatively small. We would probably expect this given that this is the same model from which the data were generated. 

For the changing compartment sizes, we see that the standard errors are much larger for smaller values of $Y_0$. This might be because the variance of the I compartment is much larger relative to the size of the compartment, since we hold $\sigma_Y$ at 1 no matter if $Y_0$ is 1 or 1000. 

For the changing error variances, we see that the estimates for $\rr$ become less accurate for increasing $\sigma_X$ and $\sigma_Y$. However, it is interesting to note that the standard errors are increasing for the monotonic Gaussian error data, but decreasing for the autoregressive and monotonic autoregressive error data and relatively stable for the Gaussian non-monotonic data. It is not clear why this would be the case.


\begin{table}[H]
	\caption{$\rr$ Estimates and Std. Errs, SIR Model, 
		Baseline Data, $X_0 = 99950, Y_0 = 50$, 
		$\sigma_X = 10, \sigma_Y = 1$}
	\begin{footnotesize}
		\hskip -1cm
		\begin{tabular}{l|r|r|r|r|r|r|r|r}
			\hline
			Data Set & Auto Est & Auto SE & AutoM Est & AutoM SE & Norm Est & Norm SE & NormM Est & NormM SE\\
			\hline
			Baseline1 & 1.199737 & 0.000476 & 1.199557 & 0.000476 & 1.199737 & 0.000475 & 1.199737 & 0.000475\\
			\hline
			Baseline2 & 1.199737 & 0.000475 & 1.200380 & 0.000481 & 1.200376 & 0.000482 & 1.200376 & 0.000482\\
			\hline
			Baseline3 & 1.200615 & 0.000485 & 1.199557 & 0.000476 & 1.200380 & 0.000481 & 1.200298 & 0.000479\\
			\hline
			Baseline4 & 1.200376 & 0.000481 & 1.200061 & 0.000479 & 1.200380 & 0.000481 & 1.200602 & 0.000484\\
			\hline
			Baseline5 & 1.200376 & 0.000481 & 1.200376 & 0.000482 & 1.200298 & 0.000479 & 1.200602 & 0.000484\\
			\hline
		\end{tabular}
	\end{footnotesize}
\end{table}


\begin{table}[H]
	
	\caption{$\rr$ Estimates and Std. Errs, SIR Model,
		Different Starting Populations, 
		$\sigma_X = 10, \sigma_Y = 1$}
	\begin{footnotesize}
		\hskip -1.7cm
		\begin{tabular}{l|r|r|r|r|r|r|r|r}
			\hline
			Data Set & Auto Est & Auto SE & AutoM Est & AutoM SE & Norm Est & Norm SE & NormM Est & NormM SE\\
			\hline
			$X_0 = 99999, Y_0 = 1$ & 1.214968 & 0.095795 & 1.236988 & 0.096579 & 1.218704 & 0.099065 & 1.218704 & 0.096508\\
			\hline
			$X_0 = 99990, Y_0 = 10$ & 1.219334 & 0.006658 & 1.219334 & 0.006854 & 1.219596 & 0.017843 & 1.219596 & 0.017847\\
			\hline
			$X_0 = 99900, Y_0 = 100$ & 1.199311 & 0.000166 & 1.200216 & 0.000170 & 1.200336 & 0.000170 & 1.200336 & 0.000170\\
			\hline
			$X_0 = 99000, Y_0 = 1000$ & 1.199807 & 0.000029 & 1.199807 & 0.000029 & 1.200160 & 0.000029 & 1.200160 & 0.000029\\
			\hline
		\end{tabular}
	\end{footnotesize}
\end{table}

\begin{table}[H]
	
	\caption{$\rr$ Estimates and Std. Errs, SIR Model, 
		Different Variances, $X_0 = 99000$, $Y_0 = 1000$}
	\begin{footnotesize}
		\hskip -1.7cm
		\begin{tabular}{l|r|r|r|r|r|r|r|r}
			\hline
			Error & Auto Est & Auto SE & AutoM Est & AutoM SE & Norm Est & Norm SE & NormM Est & NormM SE\\
			\hline
			$\sigma_X = 10, \sigma_Y = 1$ & 1.200061 & 2.9e-05 & 1.200005 & 2.9e-05 & 1.199951 & 2.9e-05 & 1.199916 & 2.9e-05\\
			\hline
			$\sigma_X = 100, \sigma_Y = 10$ & 1.200317 & 2.9e-05 & 1.200427 & 2.9e-05 & 1.199963 & 2.9e-05 & 1.199734 & 2.9e-05\\
			\hline
			$\sigma_X = 500, \sigma_Y = 50$ & 1.201642 & 2.8e-05 & 1.201871 & 2.8e-05 & 1.199996 & 2.9e-05 & 1.198872 & 2.9e-05\\
			\hline
			$\sigma_X = 1000, \sigma_Y = 100$ & 1.203259 & 2.8e-05 & 1.203811 & 2.8e-05 & 1.199985 & 2.9e-05 & 1.197928 & 2.9e-05\\
			\hline
			$\sigma_X = 2000, \sigma_Y = 200$ & 1.206447 & 2.7e-05 & 1.207566 & 2.7e-05 & 1.199849 & 2.9e-05 & 1.195612 & 3.0e-05\\
			\hline
			$\sigma_X = 5000, \sigma_Y = 500$ & 1.215906 & 2.5e-05 & 1.218773 & 2.5e-05 & 1.199656 & 2.9e-05 & 1.189382 & 3.2e-05\\
			\hline
			$\sigma_X = 7500, \sigma_Y = 750$ & 1.223554 & 2.3e-05 & 1.228037 & 2.4e-05 & 1.199579 & 2.9e-05 & 1.183914 & 3.4e-05\\
			\hline
			$\sigma_X = 10000, \sigma_Y = 1000$ & 1.230992 & 2.2e-05 & 1.237142 & 2.2e-05 & 1.199381 & 2.9e-05 & 1.178701 & 3.6e-05\\
			\hline
			$\sigma_X = 15000, \sigma_Y = 1500$ & 1.245535 & 1.9e-05 & 1.255188 & 2.0e-05 & 1.199137 & 2.9e-05 & 1.168520 & 4.1e-05\\
			\hline
		\end{tabular}
	\end{footnotesize}
\end{table}


\begin{table}[H]
	
	\caption{$\rr$ Estimates and Std. Errs, SIR Model,
		Other Models, $X_0 = 99950, Y_0 = 50$, 
		$\sigma_X = 10, \sigma_Y = 1$}
	\begin{footnotesize}
		\hskip -1cm
		\begin{tabular}{l|r|r|r|r|r|r|r|r}
			\hline
			Data Set & Auto Est & Auto SE & AutoM Est & AutoM SE & Norm Est & Norm SE & NormM Est & NormM SE\\
			\hline
			0th & 0.005468 & 0.000745 & -0.034869 & 0.000764 & -0.034869 & 7.4e-04 & 0.034869 & 0.000673\\
			\hline
			1st & 1.038004 & 0.000029 & 1.037900 & 0.000029 & 1.037826 & 2.9e-05 & 1.037826 & 0.000029\\
			\hline
			2nd & 1.199663 & 0.000045 & 1.198627 & 0.000045 & 1.199663 & 4.5e-05 & 1.198949 & 0.000045\\
			\hline
			3rd & 1.202909 & 0.000043 & 1.202282 & 0.000043 & 1.202414 & 4.3e-05 & 1.202414 & 0.000043\\
			\hline
			4th & 1.211092 & 0.000047 & 1.211092 & 0.000047 & 1.209479 & 4.7e-05 & 1.211092 & 0.000047\\
			\hline
			Linear SIR & 1.274529 & 0.000034 & 1.273768 & 0.000034 & 1.275057 & 3.4e-05 & 1.274745 & 0.000034\\
			\hline
		\end{tabular}
	\end{footnotesize}
\end{table}

\subsubsection{SIR Reparameterized Least Squares}

\paragraph{Estimation}

For the reparameterized SIR model, we estimate the parameters $\rr$ (as well as $\gamma$) of the ODEs using optim() in R to minimize the sum of squared errors using Nelder-Mead. The standard errors were taken directly from the term in the Hessian matrix corresponding to $\rr$. (I also have the sensitivity analysis code here.)

\paragraph{Comments}

The $\rr$ estimates for the baseline data are both close to the actual value, and the estimated standard errors are extremely small. We would probably expect this given that this is the same model from which the data were generated. In general, the estimates for the standard errors are also much smaller than normal least squares. (Some of the errors are 0 because I rounded to 6 digits - all of them are around the magnitude of $10 ^ {-7}$)

The $\rr$ estimates for the data with non-monotonic errors become more accurate as ($\sigma_X$, $\sigma_Y$) increase, while they become less accurate for the monotonic values. The standard errors are also positively correlated with the estimate of $\rr$ as long as all other parts, i.e., $X_0$ and $Y_0$ are held constant.

\begin{table}[H]
	
	\caption{$\rr$ Estimates and Std. Errs, SIRREPAR Model,
		Baseline Data, $X_0 = 99950, Y_0 = 50$, 
		$\sigma_X = 10, \sigma_Y = 1$}
	\begin{footnotesize}
		\hskip -1cm
		\begin{tabular}{l|r|r|r|r|r|r|r|r}
			\hline
			Data Set & Auto Est & Auto SE & AutoM Est & AutoM SE & Norm Est & Norm SE & NormM Est & NormM SE\\
			\hline
			Baseline 1 & 1.200196 & 1e-06 & 1.199942 & 1e-06 & 1.200108 & 1e-06 & 1.199965 & 1e-06\\
			\hline
			Baseline 2 & 1.199574 & 1e-06 & 1.199733 & 1e-06 & 1.199922 & 1e-06 & 1.200219 & 1e-06\\
			\hline
			Baseline 3 & 1.199909 & 1e-06 & 1.200416 & 1e-06 & 1.199851 & 1e-06 & 1.200009 & 1e-06\\
			\hline
			Baseline 4 & 1.199735 & 1e-06 & 1.199829 & 1e-06 & 1.199925 & 1e-06 & 1.199856 & 1e-06\\
			\hline
			Baseline 5 & 1.199849 & 1e-06 & 1.199683 & 1e-06 & 1.199943 & 1e-06 & 1.199900 & 1e-06\\
			\hline
		\end{tabular}
	\end{footnotesize}
\end{table}

\begin{table}[H]
	
	\caption{$\rr$ Estimates and Std. Errs, SIRREPAR Model,
		Different Starting Populations, 
		$\sigma_X = 10, \sigma_Y = 1$}
	\begin{footnotesize}
		\hskip -1.7cm
		\begin{tabular}{l|r|r|r|r|r|r|r|r}
			\hline
			Data Set & Auto Est & Auto SE & AutoM Est & AutoM SE & Norm Est & Norm SE & NormM Est & NormM SE\\
			\hline
			$X_0 = 99999, Y_0 = 1$ & 1.209265 & 3.7e-05 & 1.198242 & 3.5e-05 & 1.193162 & 3.4e-05 & 1.200995 & 3.5e-05\\
			\hline
			$X_0 = 99990, Y_0 = 10$ & 1.202026 & 4.0e-06 & 1.198515 & 4.0e-06 & 1.199652 & 4.0e-06 & 1.199716 & 4.0e-06\\
			\hline
			$X_0 = 99900, Y_0 = 100$ & 1.199895 & 1.0e-06 & 1.199895 & 1.0e-06 & 1.199976 & 1.0e-06 & 1.200103 & 1.0e-06\\
			\hline
			$X_0 = 99000, Y_0 = 1000$ & 1.200089 & 0.0e+00 & 1.200008 & 0.0e+00 & 1.199992 & 0.0e+00 & 1.199985 & 0.0e+00\\
			\hline
		\end{tabular}
	\end{footnotesize}
\end{table}

\begin{table} [H]
	
	\caption{$\rr$ Estimates and Std. Errs, SIRREPAR Model,
		Different Variances, 
		$X_0 = 99000, Y_0 = 1000$}
	\begin{footnotesize}
		\hskip -1.7cm
		\begin{tabular}{l|r|r|r|r|r|r|r|r}
			\hline
			Data Set & Auto Est & Auto SE & AutoM Est & AutoM SE & Norm Est & Norm SE & NormM Est & NormM SE\\
			\hline
			$\sigma_X = 10, \sigma_Y = 1$ & 1.199998 & 0 & 1.199990 & 0 & 1.200027 & 0 & 1.199972 & 0\\
			\hline
			$\sigma_X = 100, \sigma_Y = 10$ & 1.199920 & 0 & 1.199837 & 0 & 1.200179 & 0 & 1.199611 & 0\\
			\hline
			$\sigma_X = 500, \sigma_Y = 50$ & 1.199579 & 0 & 1.199154 & 0 & 1.200844 & 0 & 1.198063 & 0\\
			\hline
			$\sigma_X = 1000, \sigma_Y = 100$ & 1.199168 & 0 & 1.198307 & 0 & 1.201705 & 0 & 1.196130 & 0\\
			\hline
			$\sigma_X = 2000, \sigma_Y = 200$ & 1.198425 & 0 & 1.196618 & 0 & 1.203466 & 0 & 1.192321 & 0\\
			\hline
			$\sigma_X = 5000, \sigma_Y = 500$ & 1.196655 & 0 & 1.191769 & 0 & 1.208871 & 0 & 1.181229 & 0\\
			\hline
			$\sigma_X = 7500, \sigma_Y = 750$ & 1.195734 & 0 & 1.187972 & 0 & 1.213591 & 0 & 1.172358 & 0\\
			\hline
			$\sigma_X = 10000, \sigma_Y = 1000$ & 1.195439 & 0 & 1.184353 & 0 & 1.218455 & 0 & 1.163930 & 0\\
			\hline
			$\sigma_X = 15000, \sigma_Y = 1500$ & 1.196493 & 0 & 1.177762 & 0 & 1.228683 & 0 & 1.148505 & 0\\
			\hline
		\end{tabular}
	\end{footnotesize}
\end{table}
\begin{table}[H]
	
	\caption{$\rr$ Estimates and Std. Errs, SIRREPAR Model,
		Other Models, $X_0 = 99950, Y_0 = 50$, 
		$\sigma_X = 10, \sigma_Y = 1$}
	\begin{footnotesize}
		\hskip -1cm
		\begin{tabular}{l|r|r|r|r|r|r|r|r}
			\hline
			Data Set & Auto Est & Auto SE & AutoM Est & AutoM SE & Norm Est & Norm SE & NormM Est & NormM SE\\
			\hline
			0th & 2.803084 & 0e+00 & 2.663158 & 0e+00 & 2.688581 & 0e+00 & 2.756037 & 0e+00\\
			\hline
			1st & 1.038122 & 0e+00 & 1.037880 & 0e+00 & 1.037900 & 0e+00 & 1.037887 & 0e+00\\
			\hline
			2nd & 1.198739 & 1e-06 & 1.198544 & 1e-06 & 1.198847 & 1e-06 & 1.198585 & 1e-06\\
			\hline
			3rd & 1.202419 & 1e-06 & 1.202136 & 1e-06 & 1.202294 & 1e-06 & 1.202288 & 1e-06\\
			\hline
			4th & 1.210605 & 1e-06 & 1.210605 & 1e-06 & 1.210274 & 1e-06 & 1.210259 & 1e-06\\
			\hline
			Linear SIR & 1.274256 & 1e-06 & 1.273828 & 1e-06 & 1.274260 & 1e-06 & 1.274030 & 1e-06\\
			\hline
		\end{tabular}
	\end{footnotesize}
\end{table}

\subsubsection{SIR Linear Model}

\paragraph{Estimation}

To calculate the estimates for $\rr$, we use lm() in R to fit the linear polynomial regression functions for X(t) and Y(t) with respect to t. We then calculated the derivatives for the linear regression functions at time 0 using our own function. For the standard error, the jackknife was used on the data set. This calculates the standard error of the $\rr$ by using the standard deviation of the $R-0$ estimates after leaving out a single observation.

\paragraph{Comments}

The estimates for $\rr$ are very inaccurate compared to the other models. The inaccuracy is most glaring for the data with large error variances and the simulated from models besides the SIR one. The estimates seem to become less accurate for the data generated from the classic SIR model as $\sigma_X$ and $\sigma_Y$ increase and as $Y_0$ decreases relative to the total population. There are also many estimates of $\rr$ that are negative, which implies that there is a negative derivative estimate for either the susceptible or infected compartment (is this supposed to be possible?). 

The standard errors are also extremely large in some cases. This seems to hold consistently when $\sigma_X$ and $\sigma_Y$ are large for the classic SIR data as well as for the 4th order linear data. However, there are also other places where large standard error estimates appear for no apparent reason. The large standard errors and the above issue with the $\rr$ estimates being negative could be because we are only using a point estimate. In general, the only time we see fairly consistent and reasonable estimates from the linear model is for the SIR data with relatively large $Y_0$ as well as small $\sigma_X$ and $\sigma_Y$ (i.e., for the data sets where ($X_0$, $Y_0$) = (99000, 1000)).

\begin{table}[H]
	
	\caption{$\rr$ Estimates and Std. Errs, SIRLMA Model,
		Baseline Data, $X_0 = 99950, Y_0 = 50$, 
		$\sigma_X = 10, \sigma_Y = 1$}
	\begin{footnotesize}
		\hskip -1cm
		\begin{tabular}{l|r|r|r|r|r|r|r|r}
			\hline
			Data Set & Auto Est & Auto SE & AutoM Est & AutoM SE & Norm Est & Norm SE & NormM Est & NormM SE\\
			\hline
			Baseline 1 & 1.269690 & 0.120767 & 0.130935 & 10.750359 & 1.154610 & 0.065991 & 1.397649 & 0.233918\\
			\hline
			Baseline 2 & 1.187856 & 0.168327 & 1.121309 & 0.040461 & 1.193012 & 0.081712 & 1.357958 & 0.199538\\
			\hline
			Baseline 3 & 0.812981 & 0.084419 & 1.092512 & 0.034560 & 1.179720 & 0.054902 & 1.233221 & 0.060551\\
			\hline
			Baseline 4 & 1.180116 & 0.078889 & 1.168589 & 0.134484 & 1.293454 & 0.271027 & 1.165880 & 0.036177\\
			\hline
			Baseline 5 & 0.853203 & 0.064872 & 1.116949 & 0.041290 & 1.198555 & 0.106425 & 1.161485 & 0.032157\\
			\hline
		\end{tabular}
	\end{footnotesize}
\end{table}

\begin{table}[H]
	
	\caption{$\rr$ Estimates and Std. Errs, SIRLMA Model,
		Different Starting Populations, 
		$\sigma_X = 10, \sigma_Y = 1$}
	\begin{footnotesize}
		\hskip -1.7cm
		\begin{tabular}{l|r|r|r|r|r|r|r|r}
			\hline
			Data Set & Auto Est & Auto SE & AutoM Est & AutoM SE & Norm Est & Norm SE & NormM Est & NormM SE\\
			\hline
			$X_0 = 99999, Y_0 = 1$ & 1.185172 & 1.285564 & 1.158778 & 0.152094 & 0.859907 & 83.111543 & 1.116840 & 0.064265\\
			\hline
			$X_0 = 99990, Y_0 = 10$ & 0.934239 & 3.360057 & 0.944893 & 0.034982 & 1.101154 & 0.287314 & 6.153800 & 1525.693988\\
			\hline
			$X_0 = 99900, Y_0 = 100$ & 1.255903 & 0.086428 & 0.462015 & 0.325984 & 1.250009 & 0.051102 & 1.193192 & 0.035157\\
			\hline
			$X_0 = 99000, Y_0 = 1000$ & 1.205152 & 0.005332 & 1.193656 & 0.005244 & 1.199742 & 0.004094 & 1.208671 & 0.004804\\
			\hline
		\end{tabular}
	\end{footnotesize}
\end{table}

\begin{table}[H]
	
	\caption{$\rr$ Estimates and Std. Errs, SIRLMA Model,
		Different Variances, 
		$X_0 = 99000, Y_0 = 1000$}
	\begin{footnotesize}
		\hskip -2cm
		\begin{tabular}{l|r|r|r|r|r|r|r|r}
			\hline
			Data Set & Auto Est & Auto SE & AutoM Est & AutoM SE & Norm Est & Norm SE & NormM Est & NormM SE\\
			\hline
			$\sigma_X = 10, \sigma_Y = 1$ & 1.185958 & 0.008294 & 1.202635 & 0.006364 & 1.200712 & 0.004175 & 1.194196 & 0.003282\\
			\hline
			$\sigma_X = 100, \sigma_Y = 10$ & 1.080974 & 0.051432 & 1.242575 & 0.081110 & 1.221708 & 0.047705 & 1.160408 & 0.025033\\
			\hline
			$\sigma_X = 500, \sigma_Y = 50$ & 0.738334 & 0.237194 & 1.506353 & 7.578001 & 1.343488 & 0.552568 & 1.071472 & 0.060095\\
			\hline
			$\sigma_X = 1000, \sigma_Y = 100$ & 0.474710 & 0.832353 & 2.427907 & 19.349048 & 1.619427 & 3.245662 & 1.018452 & 0.087734\\
			\hline
			$\sigma_X = 2000, \sigma_Y = 200$ & 0.184986 & 2.306650 & -1.221597 & 233.262939 & 9.803881 & 71195.968128 & 0.969270 & 0.132646\\
			\hline
			$\sigma_X = 5000, \sigma_Y = 500$ & -0.130854 & 10.533729 & 0.351591 & 7.021734 & 0.389501 & 23.012536 & 0.916922 & 0.188977\\
			\hline
			$\sigma_X = 7500, \sigma_Y = 750$ & -0.225912 & 37.943501 & 0.472891 & 14.985760 & 0.546636 & 21.663742 & 0.895990 & 0.203207\\
			\hline
			$\sigma_X = 10000, \sigma_Y = 1000$ & -0.276013 & 174.714931 & 0.530827 & 128.425370 & 0.603731 & 28.923054 & 0.880411 & 0.207996\\
			\hline
			$\sigma_X = 15000, \sigma_Y = 1500$ & -0.324210 & 28.295186 & 0.598720 & 14.309696 & 0.650612 & 8.469867 & 0.855473 & 0.206941\\
			\hline
		\end{tabular}
	\end{footnotesize}
\end{table}

\begin{table}[H]
	
	\caption{$\rr$ Estimates and Std. Errs, SIRLMA Model,
		Other Models, $X_0 = 99950, Y_0 = 50$, 
		$\sigma_X = 10, \sigma_Y = 1$}
	\begin{footnotesize}
		\hskip -1cm
		\begin{tabular}{l|r|r|r|r|r|r|r|r}
			\hline
			Data Set & Auto Est & Auto SE & AutoM Est & AutoM SE & Norm Est & Norm SE & NormM Est & NormM SE\\
			\hline
			0th & 1.049994 & 0.056806 & 1.286339 & 0.479624 & 0.980772 & 0.060085 & 0.474270 & 2.099030\\
			\hline
			1st & 1.000767 & 0.125396 & 1.009129 & 0.142918 & 1.018881 & 0.127098 & 1.019178 & 0.159078\\
			\hline
			2nd & 1.467835 & 0.558738 & 0.085260 & 1.223959 & 1.215482 & 0.162666 & -6.656798 & 265.908999\\
			\hline
			3rd & 4.877978 & 57.939148 & 1.111925 & 0.120143 & 1.238831 & 0.321316 & 1.227691 & 0.221355\\
			\hline
			4th & -0.892745 & 7.391321 & -0.892745 & 7.391321 & 0.445461 & 2.016867 & 1.709272 & 2.038759\\
			\hline
			Linear SIR & 1.214648 & 0.062718 & 0.950315 & 0.106644 & 1.219144 & 0.109327 & 1.206761 & 0.048099\\
			\hline
		\end{tabular}
	\end{footnotesize}
\end{table}

\subsubsection{SIR Linear Model Average}

\paragraph{Estimation}

To calculate the estimates for $\rr$, we use lm() in R to fit the linear polynomial regression functions for X(t) and Y(t) with respect to t. We then calculated the derivatives of the linear regression functions at each time using our own function in order to find the average linear model $\rr$ estimate across all time steps $t$. The standard error was calculated by taking the standard deviation of the $\rr$ estimates across all time steps $t$ in the data set.

\paragraph{Comments}

The estimates for $\rr$ here are somewhat more accurate compared to the linear model estimates that only looked at the derivative first time point. In particular, it does a lot better for the baseline data and the data from other models. The estimates are still really bad for large values of $\sigma_X$ and $\sigma_Y$ (including some negative estimates). Interestingly, the linear model average estimates here are less accurate for large values of $Y_0$, whereas they were more accurate for the linear model.

The standard errors are not as ridiculously large compared to the linear model with only one time point, but are still extremely big compared to what we might expect to see from other models. This is especially apparent when $\sigma_X$ and $\sigma_Y$ are large.

\begin{table}[H]
	
	\caption{$\rr$ Estimates and Std. Errs, SIRLMAT Model,
		Baseline Data, $X_0 = 99950, Y_0 = 50$, 
		$\sigma_X = 10, \sigma_Y = 1$}
	\begin{footnotesize}
		\hskip -1cm
		\begin{tabular}{l|r|r|r|r|r|r|r|r}
			\hline
			Data Set & Auto Est & Auto SE & AutoM Est & AutoM SE & Norm Est & Norm SE & NormM Est & NormM SE\\
			\hline
			Baseline 1 & 1.171311 & 0.029553 & 1.169409 & 0.077379 & 1.171164 & 0.028320 & 1.173413 & 0.034820\\
			\hline
			Baseline 2 & 1.170936 & 0.027339 & 1.172363 & 0.032062 & 1.171630 & 0.027987 & 1.173249 & 0.032884\\
			\hline
			Baseline 3 & 1.023937 & 2.701492 & 1.168652 & 0.029267 & 1.171384 & 0.028419 & 1.171662 & 0.028700\\
			\hline
			Baseline 4 & 1.171501 & 0.028128 & 1.174232 & 0.033153 & 1.173115 & 0.031369 & 1.171094 & 0.028392\\
			\hline
			Baseline 5 & 1.178276 & 0.303469 & 1.167810 & 0.027766 & 1.170475 & 0.027259 & 1.171001 & 0.027792\\
			\hline
		\end{tabular}
	\end{footnotesize}
\end{table}

\begin{table}[H]
	
	\caption{$\rr$ Estimates and Std. Errs, SIRLMAT Model,
		Different Starting Populations, 
		$\sigma_X = 10, \sigma_Y = 1$}
	\begin{footnotesize}
		\hskip -1.7cm
		\begin{tabular}{l|r|r|r|r|r|r|r|r}
			\hline
			Data Set & Auto Est & Auto SE & AutoM Est & AutoM SE & Norm Est & Norm SE & NormM Est & NormM SE\\
			\hline
			$X_0 = 99999, Y_0 = 1$ & 1.422018 & 8.968638 & 1.188044 & 0.101487 & 1.151807 & 0.275409 & 1.195600 & 0.033124\\
			\hline
			$X_0 = 99990, Y_0 = 10$ & 1.191840 & 0.033509 & 1.178169 & 0.160530 & 1.194324 & 0.017020 & 1.209180 & 0.263013\\
			\hline
			$X_0 = 99900, Y_0 = 100$ & 1.148868 & 0.047200 & 1.144860 & 0.232458 & 1.149893 & 0.047661 & 1.149490 & 0.046846\\
			\hline
			$X_0 = 99000, Y_0 = 1000$ & 0.995113 & 0.120333 & 0.994869 & 0.120456 & 0.994867 & 0.120621 & 0.995126 & 0.120503\\
			\hline
		\end{tabular}
	\end{footnotesize}
\end{table}

\begin{table}[H]
	
	\caption{$\rr$ Estimates and Std. Errs, SIRLMAT Model,
		Different Variances, 
		$X_0 = 99000, Y_0 = 1000$}
	\begin{footnotesize}
		\hskip -1.7cm
		\begin{tabular}{l|r|r|r|r|r|r|r|r}
			\hline
			Data Set & Auto Est & Auto SE & AutoM Est & AutoM SE & Norm Est & Norm SE & NormM Est & NormM SE\\
			\hline
			$\sigma_X = 10, \sigma_Y = 1$ & 0.994680 & 0.119919 & 0.995029 & 0.120570 & 0.994850 & 0.120592 & 0.994886 & 0.120268\\
			\hline
			$\sigma_X = 100, \sigma_Y = 10$ & 0.993618 & 0.116092 & 0.996562 & 0.121634 & 0.994779 & 0.121739 & 0.994844 & 0.119207\\
			\hline
			$\sigma_X = 500, \sigma_Y = 50$ & 1.018998 & 0.741279 & 1.003635 & 0.132436 & 0.994571 & 0.130529 & 0.992271 & 0.121888\\
			\hline
			$\sigma_X = 1000, \sigma_Y = 100$ & 0.976732 & 0.194268 & 1.015891 & 0.176727 & 0.994170 & 0.159640 & 0.986805 & 0.133885\\
			\hline
			$\sigma_X = 2000, \sigma_Y = 200$ & 0.584095 & 6.159187 & 1.131111 & 2.595232 & 1.118248 & 1.135767 & 0.954122 & 0.243263\\
			\hline
			$\sigma_X = 5000, \sigma_Y = 500$ & 2.054850 & 17.230026 & 0.731612 & 3.124131 & 1.018859 & 0.714739 & 1.087380 & 0.674374\\
			\hline
			$\sigma_X = 7500, \sigma_Y = 750$ & 1.326042 & 5.616124 & 0.741682 & 6.293590 & 1.061083 & 1.608632 & 0.914812 & 2.034648\\
			\hline
			$\sigma_X = 10000, \sigma_Y = 1000$ & -0.424508 & 24.406119 & 0.728716 & 7.478691 & 0.984529 & 0.261810 & 0.677480 & 4.972663\\
			\hline
			$\sigma_X = 15000, \sigma_Y = 1500$ & -0.352152 & 22.171481 & 0.774077 & 3.857982 & 0.984145 & 0.158089 & 0.980721 & 1.294957\\
			\hline
		\end{tabular}
	\end{footnotesize}
\end{table}
\begin{table}[H]
	
	\caption{$\rr$ Estimates and Std. Errs, SIRLMAT Model,
		Other Models, $X_0 = 99950, Y_0 = 50$, 
		$\sigma_X = 10, \sigma_Y = 1$}
	\begin{footnotesize}
		\hskip -1cm
		\begin{tabular}{l|r|r|r|r|r|r|r|r}
			\hline
			Data Set & Auto Est & Auto SE & AutoM Est & AutoM SE & Norm Est & Norm SE & NormM Est & NormM SE\\
			\hline
			0th & 0.848941 & 3.448691 & 0.925502 & 18.031398 & -0.885541 & 17.053815 & 0.898319 & 0.676024\\
			\hline
			1st & 1.152006 & 0.018441 & 1.152364 & 0.017047 & 1.151996 & 0.016274 & 1.152495 & 0.015856\\
			\hline
			2nd & 1.164984 & 0.747602 & 1.181705 & 1.419549 & 1.233825 & 1.421868 & 3.337101 & 40.581184\\
			\hline
			3rd & 1.184468 & 0.202637 & 1.168692 & 0.036327 & 1.170062 & 0.035413 & 1.170215 & 0.034933\\
			\hline
			4th & 1.188048 & 0.446227 & 1.188048 & 0.446227 & 1.172531 & 0.353343 & 1.189808 & 0.052711\\
			\hline
			Linear SIR & 1.203159 & 0.014809 & 1.195875 & 0.024693 & 1.200980 & 0.003110 & 1.200130 & 0.004076\\
			\hline
		\end{tabular}
	\end{footnotesize}
\end{table}

\subsubsection{SIR Maximum}

\paragraph{Estimation}

We use the which.max() function in R to find the point in time where the number people in the infected compartment is at a maximum. We need to come up with a way to calculate the variance (sensitivity analysis?). 

\paragraph{Comments}

The SIR max model gives decent results. Most of the estimates are fairly close to the true $\rr$ value. It appears to give more accurate $\rr$ estimates better for higher values of $Y_0$ relative to the total population as well as smaller values of $\sigma_X$ and $\sigma_Y$. This model also consistently gives reasonable results, unlike for other models, which sometimes fail miserably when dealing with larger errors or data generated from other models (especially for 0th and 1st order data).

\begin{table}[H]
	
	\caption{$\rr$ Estimates and Std. Errs, SIRMAX Model,
		Baseline Data, $X_0 = 99950, Y_0 = 50$, 
		$\sigma_X = 10, \sigma_Y = 1$}
	\begin{footnotesize}
		\hskip -1cm
		\begin{tabular}{l|r|r|r|r|r|r|r|r}
			\hline
			Data Set & Auto Est & Auto SE & AutoM Est & AutoM SE & Norm Est & Norm SE & NormM Est & NormM SE\\
			\hline
			Baseline 1 & 1.091592 & NA & 1.091985 & NA & 1.091542 & NA & 1.091488 & NA\\
			\hline
			Baseline 2 & 1.091390 & NA & 1.091563 & NA & 1.091537 & NA & 1.091717 & NA\\
			\hline
			Baseline 3 & 1.091540 & NA & 1.091211 & NA & 1.091819 & NA & 1.091724 & NA\\
			\hline
			Baseline 4 & 1.091405 & NA & 1.091566 & NA & 1.091414 & NA & 1.091801 & NA\\
			\hline
			Baseline 5 & 1.091637 & NA & 1.091549 & NA & 1.091627 & NA & 1.091556 & NA\\
			\hline
		\end{tabular}
	\end{footnotesize}
\end{table}

\begin{table}[H]
	
	\caption{$\rr$ Estimates and Std. Errs, SIRMAX Model,
		Different Starting Populations, 
		$\sigma_X = 10, \sigma_Y = 1$}
	\begin{footnotesize}
		\hskip -1.7cm
		\begin{tabular}{l|r|r|r|r|r|r|r|r}
			\hline
			Data Set & Auto Est & Auto SE & AutoM Est & AutoM SE & Norm Est & Norm SE & NormM Est & NormM SE\\
			\hline
			$X_0 = 99999, Y_0 = 1$ & 1.002195 & NA & 1.002221 & NA & 1.002225 & NA & 1.002435 & NA\\
			\hline
			$X_0 = 99990, Y_0 = 10$ & 1.021510 & NA & 1.021948 & NA & 1.021505 & NA & 1.021804 & NA\\
			\hline
			$X_0 = 99900, Y_0 = 100$ & 1.154799 & NA & 1.155005 & NA & 1.155144 & NA & 1.155213 & NA\\
			\hline
			$X_0 = 99000, Y_0 = 1000$ & 1.202948 & NA & 1.201580 & NA & 1.199981 & NA & 1.196597 & NA\\
			\hline
		\end{tabular}
	\end{footnotesize}
\end{table}

\begin{table}[H]
	
	\caption{$\rr$ Estimates and Std. Errs, SIRMAX Model,
		Different Variances, 
		$X_0 = 99000, Y_0 = 1000$}
	\begin{footnotesize}
		\hskip -1.7cm
		\begin{tabular}{l|r|r|r|r|r|r|r|r}
			\hline
			Data Set & Auto Est & Auto SE & AutoM Est & AutoM SE & Norm Est & Norm SE & NormM Est & NormM SE\\
			\hline
			$\sigma_X = 10, \sigma_Y = 1$ & 1.200091 & NA & 1.199638 & NA & 1.203322 & NA & 1.198214 & NA\\
			\hline
			$\sigma_X = 100, \sigma_Y = 10$ & 1.192908 & NA & 1.206314 & NA & 1.204461 & NA & 1.189191 & NA\\
			\hline
			$\sigma_X = 500, \sigma_Y = 50$ & 1.164184 & NA & 1.198797 & NA & 1.209550 & NA & 1.220847 & NA\\
			\hline
			$\sigma_X = 1000, \sigma_Y = 100$ & 1.177169 & NA & 1.226796 & NA & 1.215972 & NA & 1.223427 & NA\\
			\hline
			$\sigma_X = 2000, \sigma_Y = 200$ & 1.204027 & NA & 1.212075 & NA & 1.175607 & NA & 1.228619 & NA\\
			\hline
			$\sigma_X = 5000, \sigma_Y = 500$ & 1.292498 & NA & 1.169958 & NA & 1.208613 & NA & 1.348023 & NA\\
			\hline
			$\sigma_X = 7500, \sigma_Y = 750$ & 1.376802 & NA & 1.137033 & NA & 1.237567 & NA & 1.359960 & NA\\
			\hline
			$\sigma_X = 10000, \sigma_Y = 1000$ & 1.288801 & NA & 1.105911 & NA & 1.075749 & NA & 1.372110 & NA\\
			\hline
			$\sigma_X = 15000, \sigma_Y = 1500$ & 1.228367 & NA & 1.158778 & NA & 0.978885 & NA & 1.397074 & NA\\
			\hline
		\end{tabular}
	\end{footnotesize}
\end{table}

\begin{table}[H]
	
	\caption{$\rr$ Estimates and Std. Errs, SIRMAX Model,
		Other Models, $X_0 = 99950, Y_0 = 50$, 
		$\sigma_X = 10, \sigma_Y = 1$}
	\begin{footnotesize}
		\hskip -1cm
		\begin{tabular}{l|r|r|r|r|r|r|r|r}
			\hline
			Data Set & Auto Est & Auto SE & AutoM Est & AutoM SE & Norm Est & Norm SE & NormM Est & NormM SE\\
			\hline
			0th & 1.000604 & NA & 1.000563 & NA & 1.000366 & NA & 1.000543 & NA\\
			\hline
			1st & 1.083647 & NA & 1.083855 & NA & 1.083627 & NA & 1.083315 & NA\\
			\hline
			2nd & 1.086218 & NA & 1.085875 & NA & 1.085994 & NA & 1.085891 & NA\\
			\hline
			3rd & 1.096363 & NA & 1.095844 & NA & 1.096062 & NA & 1.095990 & NA\\
			\hline
			4th & 1.085509 & NA & 1.085509 & NA & 1.085289 & NA & 1.085296 & NA\\
			\hline
			Linear SIR & 1.127272 & NA & 1.127403 & NA & 1.127313 & NA & 1.127194 & NA\\
			\hline
		\end{tabular}
	\end{footnotesize}
\end{table}

\subsubsection{SIR Smoothed Max}

\paragraph{Estimation}

We use the smooth.spline() function in R to get estimates for X(t) and Y(t), then used the which.max() function to find the point in time where the number of people in the infected compartment is at a maximum. The standard error for the $\rr$ estimates was done using the jackknife method; we estimate a new spline function after removing one observation from the data set and calculate a new $\rr$ estimate, then use the standard deviation of those estimates as the estimate of the standard error.

\paragraph{Comments}

The results are fairly similar to the non-smoothed SIR maximum model.  While the model does not give perfectly accurate estimates for $\rr$, the results are decent. It does not give us any crazy results. The model is more accurate for larger values of $Y_0$ relative to the total size of the population and for smaller $\sigma_X$ and $\sigma_Y$.

The standard errors are reasonable values, and they seem to be fairly consistent under repeated estimation with the same conditions for the generated data.  They increase with increases in $Y_0$ as well as $\sigma_X$ and $\sigma_Y$.

\begin{table}[H]
	
	\caption{$\rr$ Estimates and Std. Errs, SIRSMAX Model,
		Baseline Data, $X_0 = 99950, Y_0 = 50$, 
		$\sigma_X = 10, \sigma_Y = 1$}
	\begin{footnotesize}
		\hskip -1cm
		\begin{tabular}{l|r|r|r|r|r|r|r|r}
			\hline
			Data Set & Auto Est & Auto SE & AutoM Est & AutoM SE & Norm Est & Norm SE & NormM Est & NormM SE\\
			\hline
			Baseline 1 & 1.085556 & 0.000544 & 1.085589 & 0.000546 & 1.085551 & 0.000538 & 1.085568 & 0.000534\\
			\hline
			Baseline 2 & 1.085512 & 0.000531 & 1.085507 & 0.000536 & 1.085553 & 0.000534 & 1.085583 & 0.000536\\
			\hline
			Baseline 3 & 1.085561 & 0.000542 & 1.085528 & 0.000538 & 1.085557 & 0.000539 & 1.085600 & 0.000538\\
			\hline
			Baseline 4 & 1.085540 & 0.000527 & 1.085527 & 0.000537 & 1.085556 & 0.000536 & 1.085547 & 0.000538\\
			\hline
			Baseline 5 & 1.085546 & 0.000531 & 1.085561 & 0.000531 & 1.085545 & 0.000535 & 1.085510 & 0.000535\\
			\hline
		\end{tabular}
	\end{footnotesize}
\end{table}

\begin{table}[H]
	
	\caption{$\rr$ Estimates and Std. Errs, SIRSMAX Model,
		Different Starting Populations, 
		$\sigma_X = 10, \sigma_Y = 1$}
	\begin{footnotesize}
		\hskip -1.7cm
		\begin{tabular}{l|r|r|r|r|r|r|r|r}
			\hline
			Data Set & Auto Est & Auto SE & AutoM Est & AutoM SE & Norm Est & Norm SE & NormM Est & NormM SE\\
			\hline
			$X_0 = 99999, Y_0 = 1$ & 1.001996 & 0.000024 & 1.002034 & 0.000029 & 1.002019 & 0.000026 & 1.002030 & 0.000021\\
			\hline
			$X_0 = 99990, Y_0 = 10$ & 1.019557 & 0.000186 & 1.019528 & 0.000183 & 1.019498 & 0.000179 & 1.019483 & 0.000175\\
			\hline
			$X_0 = 99900, Y_0 = 100$ & 1.148891 & 0.000569 & 1.148956 & 0.000563 & 1.148938 & 0.000580 & 1.148941 & 0.000581\\
			\hline
			$X_0 = 99000, Y_0 = 1000$ & 1.201700 & 0.000907 & 1.201844 & 0.000881 & 1.201772 & 0.000939 & 1.201768 & 0.000898\\
			\hline
		\end{tabular}
	\end{footnotesize}
\end{table}

\begin{table}[H]
	
	\caption{$\rr$ Estimates and Std. Errs, SIRSMAX Model,
		Different Variances, 
		$X_0 = 99000, Y_0 = 1000$}
	\begin{footnotesize}
		\hskip -1.7cm
		\begin{tabular}{l|r|r|r|r|r|r|r|r}
			\hline
			Data Set & Auto Est & Auto SE & AutoM Est & AutoM SE & Norm Est & Norm SE & NormM Est & NormM SE\\
			\hline
			$\sigma_X = 10, \sigma_Y = 1$ & 1.201804 & 0.000884 & 1.201778 & 0.000936 & 1.201741 & 0.000950 & 1.201767 & 0.000849\\
			\hline
			$\sigma_X = 100, \sigma_Y = 10$ & 1.202440 & 0.001007 & 1.202091 & 0.000898 & 1.201573 & 0.000962 & 1.202069 & 0.000931\\
			\hline
			$\sigma_X = 500, \sigma_Y = 50$ & 1.205047 & 0.001527 & 1.203529 & 0.001342 & 1.200705 & 0.001254 & 1.203260 & 0.001214\\
			\hline
			$\sigma_X = 1000, \sigma_Y = 100$ & 1.208288 & 0.002605 & 1.205228 & 0.002166 & 1.199591 & 0.001887 & 1.204841 & 0.001929\\
			\hline
			$\sigma_X = 2000, \sigma_Y = 200$ & 1.214838 & 0.004880 & 1.208625 & 0.004127 & 1.197394 & 0.003492 & 1.207986 & 0.003637\\
			\hline
			$\sigma_X = 5000, \sigma_Y = 500$ & 1.234608 & 0.011933 & 1.218729 & 0.009843 & 1.190987 & 0.008460 & 1.218687 & 0.009868\\
			\hline
			$\sigma_X = 7500, \sigma_Y = 750$ & 1.251420 & 0.017892 & 1.226979 & 0.014483 & 1.185867 & 0.012567 & 1.229087 & 0.016029\\
			\hline
			$\sigma_X = 10000, \sigma_Y = 1000$ & 1.268381 & 0.023922 & 1.235083 & 0.019214 & 1.180831 & 0.016460 & 1.241403 & 0.023960\\
			\hline
			$\sigma_X = 15000, \sigma_Y = 1500$ & 1.303020 & 0.035872 & 1.251167 & 0.028997 & 1.171122 & 0.024414 & 1.273194 & 0.037158\\
			\hline
		\end{tabular}
	\end{footnotesize}
\end{table}

\begin{table}[H]
	
	\caption{$\rr$ Estimates and Std. Errs, SIRSMAX Model,
		Other Models, $X_0 = 99950, Y_0 = 50$, 
		$\sigma_X = 10, \sigma_Y = 1$}
	\begin{footnotesize}
		\hskip -1cm
		\begin{tabular}{l|r|r|r|r|r|r|r|r}
			\hline
			Data Set & Auto Est & Auto SE & AutoM Est & AutoM SE & Norm Est & Norm SE & NormM Est & NormM SE\\
			\hline
			0th & 1.000515 & 0.000022 & 1.000486 & 0.000000 & 1.000466 & 0.000015 & 1.000462 & 0.000003\\
			\hline
			1st & 1.083710 & 0.000021 & 1.083642 & 0.000032 & 1.083606 & 0.000022 & 1.083580 & 0.000018\\
			\hline
			2nd & 1.082579 & 0.000320 & 1.082543 & 0.000312 & 1.082600 & 0.000314 & 1.082560 & 0.000310\\
			\hline
			3rd & 1.089734 & 0.000559 & 1.089601 & 0.000550 & 1.089649 & 0.000554 & 1.089634 & 0.000553\\
			\hline
			4th & 1.079535 & 0.000524 & 1.079535 & 0.000524 & 1.079467 & 0.000516 & 1.079486 & 0.000511\\
			\hline
			Linear SIR & 1.112544 & 0.001242 & 1.112644 & 0.001239 & 1.112546 & 0.001238 & 1.112544 & 0.001236\\
			\hline
		\end{tabular}
	\end{footnotesize}
\end{table}

\subsubsection{SIR Incidence-Prevalence Ratio}

\paragraph{Estimation}

We use basic arithmetic functions in R to calculate the incidence to prevalence ratio for each observation in time and to derive the $\rr$ estimate. 

I don't currently have a standard error calculation. One thing we could do is use the standard deviation of the $\frac{IPR(t)}{D}$ values for across all $t$, since we take the mean of them as the estimate for $\rr$.  

\paragraph{Comments}

The estimates for $\rr$ seem to be very imprecise for this model. Looking at the baseline data, the $\rr$ estimates vary fairly widely given that the 5 data sets were generated under the same conditions. We also see that the estimates change massively as we change $Y_0$ or $\sigma_X$ and $\sigma_Y$. As we see below, the estimate for $\rr$ drops from around 45 for $Y_0 = 1$ to 0.9 for $Y_0 = 1000$. Similarly, the estimate for $\rr$ grows from around 0.9 for ($\sigma_X$, $\sigma_Y$) = (10, 1) to 200 for ($\sigma_X$, $\sigma_Y$) = (15000, 1500). In general, we see that the estimates for $\rr$ decrease as $Y_0$ increases relative to the total population and that they increase as we increase 
$\sigma_X$ and $\sigma_Y$.

\begin{table}[H]
	
	\caption{$\rr$ Estimates and Std. Errs, SIRIPR Model,
		Baseline Data, $X_0 = 99950, Y_0 = 50$, 
		$\sigma_X = 10, \sigma_Y = 1$}
	\begin{footnotesize}
		\hskip -1cm
		\begin{tabular}{l|r|r|r|r|r|r|r|r}
			\hline
			Data Set & Auto Est & Auto SE & AutoM Est & AutoM SE & Norm Est & Norm SE & NormM Est & NormM SE\\
			\hline
			Baseline 1 & 1.568114 & NA & 1.171336 & NA & 1.332469 & NA & 1.062616 & NA\\
			\hline
			Baseline 2 & 1.273631 & NA & 1.287347 & NA & 1.287487 & NA & 1.324252 & NA\\
			\hline
			Baseline 3 & 1.302008 & NA & 1.352183 & NA & 1.528039 & NA & 1.059422 & NA\\
			\hline
			Baseline 4 & 1.181609 & NA & 1.001200 & NA & 1.624614 & NA & 1.128520 & NA\\
			\hline
			Baseline 5 & 1.168529 & NA & 1.278128 & NA & 1.289007 & NA & 1.207690 & NA\\
			\hline
		\end{tabular}
	\end{footnotesize}
\end{table}

\begin{table}[H]
	
	\caption{$\rr$ Estimates and Std. Errs, SIRIPR Model,
		Different Starting Populations, 
		$\sigma_X = 10, \sigma_Y = 1$}
	\begin{footnotesize}
		\hskip -1.7cm
		\begin{tabular}{l|r|r|r|r|r|r|r|r}
			\hline
			Data Set & Auto Est & Auto SE & AutoM Est & AutoM SE & Norm Est & Norm SE & NormM Est & NormM SE\\
			\hline
			$X_0 = 99999, Y_0 = 1$ & 45.345556 & NA & 2.018195 & NA & 62.416323 & NA & 1.301719 & NA\\
			\hline
			$X_0 = 99990, Y_0 = 10$ & 2.756187 & NA & 1.161055 & NA & 3.991245 & NA & 1.288459 & NA\\
			\hline
			$X_0 = 99900, Y_0 = 100$ & 1.250277 & NA & 1.199468 & NA & 1.237907 & NA & 1.291419 & NA\\
			\hline
			$X_0 = 99000, Y_0 = 1000$ & 0.834190 & NA & 0.969512 & NA & 0.900419 & NA & 0.936967 & NA\\
			\hline
		\end{tabular}
	\end{footnotesize}
\end{table}

\begin{table}[H]
	
	\caption{$\rr$ Estimates and Std. Errs, SIRIPR Model,
		Different Variances, 
		$X_0 = 99000, Y_0 = 1000$}
	\begin{footnotesize}
		\hskip -1.7cm
		\begin{tabular}{l|r|r|r|r|r|r|r|r}
			\hline
			Data Set & Auto Est & Auto SE & AutoM Est & AutoM SE & Norm Est & Norm SE & NormM Est & NormM SE\\
			\hline
			$\sigma_X = 10, \sigma_Y = 1$ & 0.985846 & NA & 0.986994 & NA & 0.983782 & NA & 0.985415 & NA \\
			\hline
			$\sigma_X = 100, \sigma_Y = 10$ & 1.237596 & NA & 1.209899 & NA & 1.338217 & NA & 1.323149 & NA\\
			\hline
			$\sigma_X = 500, \sigma_Y = 50$ & 3.544004 & NA & 3.412129 & NA & 4.189569 & NA & 4.080100 & NA\\
			\hline
			$\sigma_X = 1000, \sigma_Y = 100$ & 6.610460 & NA & 6.295422 & NA & 7.893167 & NA & 7.650749 & NA\\
			\hline
			$\sigma_X = 2000, \sigma_Y = 200$ & 13.301341 & NA & 12.235214 & NA & 15.500513 & NA & 15.026026 & NA\\
			\hline
			$\sigma_X = 5000, \sigma_Y = 500$ & 62.640210 & NA & 57.764547 & NA & 49.217485 & NA & 104.547880 & NA\\
			\hline
			$\sigma_X = 7500, \sigma_Y = 750$ & 71.225178 & NA & 105.181361 & NA & 320.428071 & NA & 107.391030 & NA\\
			\hline
			$\sigma_X = 10000, \sigma_Y = 1000$ & 116.249187 & NA & 161.948194 & NA & 126.671661 & NA & 121.517490 & NA \\
			\hline
			$\sigma_X = 15000, \sigma_Y = 1500$ & 235.468174 & NA & 148.873668 & NA & 220.585394 & NA & 308.061708 & NA\\
			\hline
		\end{tabular}
	\end{footnotesize}
\end{table}

\begin{table}[H]
	
	\caption{$\rr$ Estimates and Std. Errs, SIRIPR Model,
		Other Models, $X_0 = 99950, Y_0 = 50$, 
		$\sigma_X = 10, \sigma_Y = 1$}
	\begin{footnotesize}
		\hskip -1cm
		\begin{tabular}{l|r|r|r|r|r|r|r|r}
			\hline
			Data Set & Auto Est & Auto SE & AutoM Est & AutoM SE & Norm Est & Norm SE & NormM Est & NormM SE\\
			\hline
			0th & 1.706806 & NA & 0.009605 & NA & 2.134967 & NA & 0.025424 & NA\\
			\hline
			1st & 3.051573 & NA & 4.744311 & NA & 4.477775 & NA & 4.457355 & NA\\
			\hline
			2nd & 1.425110 & NA & 1.181736 & NA & 1.688859 & NA & 1.088296 & NA\\
			\hline
			3rd & 1.280001 & NA & 1.497495 & NA & 1.569176 & NA & 1.232182 & NA\\
			\hline
			4th & 1.065715 & NA & 1.284711 & NA & 1.373762 & NA & 1.148073 & NA\\
			\hline
			Linear SIR & 1.537546 & NA & 1.212776 & NA & 1.611864 & NA & 1.161722 & NA\\
			\hline
		\end{tabular}
	\end{footnotesize}
\end{table}

\subsubsection{SIR Smoothed Incidence-Prevalence Ratio}

\paragraph{Estimation}

This estimation method is similar to the incidence-prevalence ratio estimator above, except that we first smooth the data for the number of susceptible and infected using a smoothing spline. We use basic arithmetic functions in R to calculate the incidence to prevalence ratio for each observation in time and to derive the $\rr$ estimate.

We estimate the standard error using the jackknife method. For every observation, we fit a smoothed spline to X(t) and Y(t) after taking out that specific observation, then take the standard deviation of $t$ the $\rr$ estimates. 

\paragraph{Comments}

Smoothing the data actually makes the $\rr$ estimates much more precise. All of them are now within $\pm$ 0.5 of the true $\rr$ for the data generated from the classic SIR model. We still see the same pattern as in the non-smoothed incidence-prevalence ratio estimates, that the estimates for $\rr$ decrease as $Y_0$ increases relative to the total population and that they increase as we increase $\sigma_X$ and $\sigma_Y$. For the data generated from other models, we see that there are some negative $\rr$ estimates for the 0th and 1st order linear model data. (This does not happen to be a problem with the non-smoothed IPR estimates.) This might be the results of having some negative incidences in the data affecting the results of the smoothing spline. 

The standard errors for this model are large compared to the SIR models that are soled using optim() (such as least squares). We also see that the standard error increases dramatically as $\sigma_X$ and $\sigma_Y$ increase. 

\begin{table}[H]
	
	\caption{$\rr$ Estimates and Std. Errs, SIRSIPR Model,
		Baseline Data, $X_0 = 99950, Y_0 = 50$, 
		$\sigma_X = 10, \sigma_Y = 1$}
	\begin{footnotesize}
		\hskip -1cm
		\begin{tabular}{l|r|r|r|r|r|r|r|r}
			\hline
			Data Set & Auto Est & Auto SE & AutoM Est & AutoM SE & Norm Est & Norm SE & NormM Est & NormM SE\\
			\hline
			Baseline 1 & 1.168689 & 0.211967 & 1.433300 & 0.275675 & 1.247849 & 0.225538 & 1.403578 & 0.253343\\
			\hline
			Baseline 2 & 1.243889 & 0.223453 & 1.169045 & 0.207609 & 1.165859 & 0.213966 & 1.539909 & 0.280209\\
			\hline
			Baseline 3 & 1.351741 & 0.236626 & 1.236297 & 0.210356 & 1.188403 & 0.218132 & 1.214513 & 0.219824\\
			\hline
			Baseline 4 & 1.222771 & 0.209546 & 1.507230 & 0.279421 & 1.260300 & 0.230086 & 1.290742 & 0.231896\\
			\hline
			Baseline 5 & 1.106582 & 0.190483 & 1.261039 & 0.229758 & 1.248270 & 0.227508 & 1.327386 & 0.239792\\
			\hline
		\end{tabular}
	\end{footnotesize}
\end{table}

\begin{table}[H]
	
	\caption{$\rr$ Estimates and Std. Errs, SIRSIPR Model,
		Different Starting Populations, 
		$\sigma_X = 10, \sigma_Y = 1$}
	\begin{footnotesize}
		\hskip -1.7cm
		\begin{tabular}{l|r|r|r|r|r|r|r|r}
			\hline
			Data Set & Auto Est & Auto SE & AutoM Est & AutoM SE & Norm Est & Norm SE & NormM Est & NormM SE\\
			\hline
			$X_0 = 99999, Y_0 = 1$ & 1.125861 & 2.218581 & 1.396896 & 0.546108 & 1.414820 & 1.323435 & 1.265761 & 0.231673\\
			\hline
			$X_0 = 99990, Y_0 = 10$ & 1.316003 & 0.274009 & 1.446887 & 0.251195 & 1.224239 & 0.233581 & 1.430876 & 0.234044\\
			\hline
			$X_0 = 99900, Y_0 = 100$ & 1.456975 & 0.246964 & 1.203267 & 0.208514 & 1.169186 & 0.197852 & 1.388754 & 0.233590\\
			\hline
			$X_0 = 99000, Y_0 = 1000$ & 1.061364 & 0.014436 & 0.913393 & 0.012300 & 1.027593 & 0.013776 & 1.258616 & 0.016998\\
			\hline
		\end{tabular}
	\end{footnotesize}
\end{table}

\begin{table}[H]
	
	\caption{$\rr$ Estimates and Std. Errs, SIRSIPR Model,
		Different Variances, 
		$X_0 = 99000, Y_0 = 1000$}
	\begin{footnotesize}
		\hskip -1.7cm
		\begin{tabular}{l|r|r|r|r|r|r|r|r}
			\hline
			Data Set & Auto Est & Auto SE & AutoM Est & AutoM SE & Norm Est & Norm SE & NormM Est & NormM SE\\
			\hline
			$\sigma_X = 10, \sigma_Y = 1$ & 1.077390 & 0.014486 & 1.076946 & 0.014585 & 1.077445 & 0.014524 & 1.077259 & 0.014413\\
			\hline
			$\sigma_X = 100, \sigma_Y = 10$ & 1.078355 & 0.015334 & 1.073914 & 0.016113 & 1.078901 & 0.015210 & 1.077047 & 0.014195\\
			\hline
			$\sigma_X = 500, \sigma_Y = 50$ & 1.082754 & 0.030646 & 1.060547 & 0.031371 & 1.085358 & 0.024514 & 1.076160 & 0.022713\\
			\hline
			$\sigma_X = 1000, \sigma_Y = 100$ & 1.088511 & 0.055954 & 1.044073 & 0.055612 & 1.093402 & 0.041119 & 1.075184 & 0.041831\\
			\hline
			$\sigma_X = 2000, \sigma_Y = 200$ & 1.100914 & 0.107709 & 1.011872 & 0.106957 & 1.109397 & 0.076725 & 1.073676 & 0.086113\\
			\hline
			$\sigma_X = 5000, \sigma_Y = 500$ & 1.146051 & 0.254897 & 0.920643 & 0.268296 & 1.156686 & 0.182392 & 1.072789 & 0.244303\\
			\hline
			$\sigma_X = 7500, \sigma_Y = 750$ & 1.194432 & 0.367353 & 0.849913 & 0.409537 & 1.195360 & 0.266558 & 1.076371 & 0.408601\\
			\hline
			$\sigma_X = 10000, \sigma_Y = 1000$ & 1.255238 & 0.471487 & 0.783195 & 0.557464 & 1.233433 & 0.347153 & 1.084101 & 0.611590\\
			\hline
			$\sigma_X = 15000, \sigma_Y = 1500$ & 1.431325 & 0.658287 & 0.659451 & 0.877489 & 1.307994 & 0.498443 & 1.113121 & 1.186676\\
			\hline
		\end{tabular}
	\end{footnotesize}
\end{table}

\begin{table}[H]
	
	\caption{$\rr$ Estimates and Std. Errs, SIRSIPR Model,
		Other Models, $X_0 = 99950, Y_0 = 50$, 
		$\sigma_X = 10, \sigma_Y = 1$}
	\begin{footnotesize}
		\hskip -1cm
		\begin{tabular}{l|r|r|r|r|r|r|r|r}
			\hline
			Data Set & Auto Est & Auto SE & AutoM Est & AutoM SE & Norm Est & Norm SE & NormM Est & NormM SE\\
			\hline
			0th & 0.016596 & 1.57720e-02 & 0.003117 & 0.000529 & -0.006329 & 0.010236 & 0.005741 & 0.002001\\
			\hline
			1st & -7.847035 & 8.15440e+04 & 2.832331 & 163.692234 & 2.274969 & 185.334642 & 2.106098 & 184.676745\\
			\hline
			2nd & 1.094937 & 2.10785e-01 & 0.956905 & 0.138703 & 0.869594 & 0.171361 & 1.179674 & 0.167192\\
			\hline
			3rd & 1.238637 & 2.89984e-01 & 1.446063 & 0.352928 & 1.237537 & 0.285715 & 1.230995 & 0.282838\\
			\hline
			4th & 1.103846 & 1.73547e-01 & 1.131708 & 0.177927 & 1.133062 & 0.188710 & 1.349997 & 0.222899\\
			\hline
			Linear SIR & 1.396246 & 2.38564e-01 & 1.269573 & 0.223832 & 1.305677 & 0.217041 & 1.333689 & 0.222366\\
			\hline
		\end{tabular}
	\end{footnotesize}
\end{table}

\subsubsection{SIR Log Linear}

\paragraph{Estimation}

We use lm() in R to fit a linear regression model to $log(\frac{X(t)}{X(0)})$ on $Z(t)$. We use the coefficient estimate and standard error from lm() as our estimate of and standard error of $\rr$.

\paragraph{Comments}

The $\rr$ estimates here are very accurate and precise. The estimates from the baseline and changing $Y_0$ data sets are all very close to the true $\rr$ value. Increasing $\sigma_X$ and $\sigma_Y$ does decrease the model accuracy. It is not quite as good for data generated from other models, but it gives reasonable results for the 0th and 1st order linear models unlike some of the other models.

The standard errors for the model are relatively small as well, and there are no unusually large values. They tend to decrease as $Y_0$ increases and they tend to increase as $\sigma_X$ and $\sigma_Y$ increase.

\begin{table}[H]
	
	\caption{$\rr$ Estimates and Std. Errs, HARKO Model, 
		Baseline Data, $X_0 = 99950, Y_0 = 50$, 
		$\sigma_X = 10, \sigma_Y = 1$}
	\begin{footnotesize}
		\hskip -1cm
		\begin{tabular}{l|r|r|r|r|r|r|r|r}
			\hline
			Data Set & Auto Est & Auto SE & AutoM Est & AutoM SE & Norm Est & Norm SE & NormM Est & NormM SE\\
			\hline
			Baseline1 & 1.199905 & 7.6e-05 & 1.199973 & 7.2e-05 & 1.200018 & 5.4e-05 & 1.199920 & 4.1e-05\\
			\hline
			Baseline2 & 1.200098 & 7.9e-05 & 1.200053 & 7.3e-05 & 1.200032 & 5.5e-05 & 1.200006 & 4.4e-05\\
			\hline
			Baseline3 & 1.200107 & 7.8e-05 & 1.199705 & 6.7e-05 & 1.200028 & 5.3e-05 & 1.199869 & 4.1e-05\\
			\hline
			Baseline4 & 1.200077 & 7.0e-05 & 1.200122 & 6.0e-05 & 1.200015 & 5.7e-05 & 1.199999 & 4.4e-05\\
			\hline
			Baseline5 & 1.199987 & 8.5e-05 & 1.200006 & 6.5e-05 & 1.199914 & 5.7e-05 & 1.200139 & 4.0e-05\\
			\hline
		\end{tabular}
	\end{footnotesize}
\end{table}

\begin{table}[H]
	
	\caption{$\rr$ Estimates and Std. Errs, HARKO Model,
		Different Starting Populations, 
		$\sigma_X = 10, \sigma_Y = 1$}
	\begin{footnotesize}
		\hskip -1.7cm
		\begin{tabular}{l|r|r|r|r|r|r|r|r}
			\hline
			Data Set & Auto Est & Auto SE & AutoM Est & AutoM SE & Norm Est & Norm SE & NormM Est & NormM SE\\
			\hline
			$X_0 = 99999, Y_0 = 1$ & 1.183163 & 0.002609 & 1.196363 & 0.001372 & 1.188140 & 0.002488 & 1.192792 & 0.000554\\
			\hline
			$X_0 = 99990, Y_0 = 10$ & 1.200267 & 0.000295 & 1.200320 & 0.000269 & 1.200069 & 0.000259 & 1.200435 & 0.000146\\
			\hline
			$X_0 = 99900, Y_0 = 100$ & 1.200009 & 0.000044 & 1.200014 & 0.000043 & 1.199928 & 0.000032 & 1.200010 & 0.000024\\
			\hline
			$X_0 = 99000, Y_0 = 1000$ & 1.199986 & 0.000013 & 1.199981 & 0.000013 & 1.200009 & 0.000010 & 1.200006 & 0.000010\\
			\hline
		\end{tabular}
	\end{footnotesize}
\end{table}

\begin{table}[H]
	
	\caption{$\rr$ Estimates and Std. Errs, HARKO Model, 
		Different Variances, $X_0 = 99000$, $Y_0 = 1000$}
	\begin{footnotesize}
		\hskip -1.7cm
		\begin{tabular}{l|r|r|r|r|r|r|r|r}
			\hline
			Error & Auto Est & Auto SE & AutoM Est & AutoM SE & Norm Est & Norm SE & NormM Est & NormM SE\\
			\hline
			$\sigma_X = 10, \sigma_Y = 1$ & 1.200000 & 0.000014 & 1.200036 & 0.000013 & 1.199988 & 0.000010 & 1.199994 & 0.000010\\
			\hline
			$\sigma_X = 100, \sigma_Y = 10$ & 1.200006 & 0.000142 & 1.200377 & 0.000133 & 1.199883 & 0.000102 & 1.199945 & 0.000097\\
			\hline
			$\sigma_X = 500, \sigma_Y = 50$ & 1.200213 & 0.000712 & 1.202158 & 0.000666 & 1.199521 & 0.000506 & 1.199785 & 0.000482\\
			\hline
			$\sigma_X = 1000, \sigma_Y = 100$ & 1.200917 & 0.001424 & 1.204990 & 0.001330 & 1.199297 & 0.001008 & 1.199719 & 0.000959\\
			\hline
			$\sigma_X = 2000, \sigma_Y = 200$ & 1.203936 & 0.002827 & 1.212506 & 0.002627 & 1.199576 & 0.001988 & 1.200005 & 0.001892\\
			\hline
			$\sigma_X = 5000, \sigma_Y = 500$ & 1.225212 & 0.006669 & 1.244598 & 0.006057 & 1.205086 & 0.004659 & 1.203631 & 0.004414\\
			\hline
			$\sigma_X = 7500, \sigma_Y = 750$ & 1.254099 & 0.009665 & 1.274883 & 0.008428 & 1.213266 & 0.006584 & 1.208945 & 0.006158\\
			\hline
			$\sigma_X = 10000, \sigma_Y = 1000$ & 1.292933 & 0.013084 & 1.306145 & 0.010690 & 1.223726 & 0.008360 & 1.215920 & 0.007672\\
			\hline
			$\sigma_X = 15000, \sigma_Y = 1500$ & 1.439976 & 0.028186 & 1.381843 & 0.016227 & 1.252207 & 0.012016 & 1.235788 & 0.010554\\
			\hline
		\end{tabular}
	\end{footnotesize}
\end{table}

\begin{table}[H]
	
	\caption{$\rr$ Estimates and Std. Errs, HARKO Model,
		Other Models, $X_0 = 99950, Y_0 = 50$, 
		$\sigma_X = 10, \sigma_Y = 1$}
	\begin{footnotesize}
		\hskip -1cm
		\begin{tabular}{l|r|r|r|r|r|r|r|r}
			\hline
			Data Set & Auto Est & Auto SE & AutoM Est & AutoM SE & Norm Est & Norm SE & NormM Est & NormM SE\\
			\hline
			0th & 1.000194 & 0.000021  & 1.000282 & 0.000013 & 1.000185 & 0.000015 & 1.000318 & 0.000018\\
			\hline
			1st & 1.040689 & 0.000468 & 1.040788 & 0.000469 & 1.040680 & 0.000468 & 1.040532 & 0.000467\\
			\hline
			2nd & 1.041906 & 0.000476 & 1.041744 & 0.000475 & 1.041800 & 0.000475 & 1.041751 & 0.000475\\
			\hline
			3rd & 1.046688 & 0.000504 & 1.046444 & 0.000503 & 1.046546 & 0.000504 & 1.046513 & 0.000503\\
			\hline
			4th & 1.041571 & 0.000473 & 1.041571 & 0.000473 & 1.041466 & 0.000473 & 1.041470 & 0.000473\\
			\hline
			Linear SIR & 1.061074 & 0.000585 & 1.061134 & 0.000586 & 1.061092 & 0.000585 & 1.061038 & 0.000585\\
			\hline
		\end{tabular}
	\end{footnotesize}
\end{table}

\subsubsection{SIR Markov Chain}

\paragraph{Estimation}

I'm not sure what I did was correct in this case. Based on what I understood from the paper, the method I came up with for estimating $\rr$ here was using lm() in R. My logic in algebra:

$$X_{t+1} \sim Binom(X_{t}, \alpha^Y)$$ $$ X_{t+1} = X_{t} * \alpha^Y$$ $$X _{t+1} / X_{t} = \alpha^Y$$ $$log(X _{t+1}) - log(X_{t}) = Y * log(\alpha)$$

We would regress $log(X _{t+1}) - log(X_{t})$ on $Y$. However, that regression function returned unreasonable estimates for $\rr$. I then attempted scale $Y$ by the number of contacts per day of each member of the susceptible compartment such that $Y^*$ would be the expected number of infectious people contacted for the day (i.e., $log(X _{t+1}) - log(X_{t}) = Y^* log(\alpha)$ where $Y^* = Y/N * contacts$) The results below were on the assumption that each susceptible person would contact one other person per time step (i.e., $Y^* = Y/N$).

\begin{table}[H]
	
	\caption{$\rr$ Estimates and Std. Errs, SIRMC Model,
		Baseline Data, $X_0 = 99950, Y_0 = 50$, 
		$\sigma_X = 10, \sigma_Y = 1$}
	\begin{footnotesize}
		\hskip -1cm
		\begin{tabular}{l|r|r|r|r|r|r|r|r}
			\hline
			Data Set & Auto Est & Auto SE & AutoM Est & AutoM SE & Norm Est & Norm SE & NormM Est & NormM SE\\
			\hline
			Baseline 1 & 2.840123 & 0.346977 & 2.832794 & 0.298082 & 2.841003 & 0.427146 & 2.842369 & 0.361052\\
			\hline
			Baseline 2 & 2.844091 & 0.344388 & 2.840638 & 0.305295 & 2.841250 & 0.443461 & 2.838177 & 0.336637\\
			\hline
			Baseline 3 & 2.841266 & 0.324645 & 2.845812 & 0.316198 & 2.836290 & 0.398227 & 2.837974 & 0.365954\\
			\hline
			Baseline 4 & 2.843637 & 0.362766 & 2.840456 & 0.300201 & 2.843353 & 0.462407 & 2.836546 & 0.364677\\
			\hline
			Baseline 5 & 2.839390 & 0.335435 & 2.841471 & 0.309445 & 2.839357 & 0.445301 & 2.840635 & 0.376539\\
			\hline
		\end{tabular}
	\end{footnotesize}
\end{table}

\begin{table}[H]
	
	\caption{$\rr$ Estimates and Std. Errs, SIRMC Model,
		Different Starting Populations, 
		$\sigma_X = 10, \sigma_Y = 1$}
	\begin{footnotesize}
		\hskip -1.7cm
		\begin{tabular}{l|r|r|r|r|r|r|r|r}
			\hline
			Data Set & Auto Est & Auto SE & AutoM Est & AutoM SE & Norm Est & Norm SE & NormM Est & NormM SE\\
			\hline
			$X_0 = 99999, Y_0 = 1$ & 2.837554 & 11.144048 & 2.923039 & 2.862449 & 2.735179 & 12.672799 & 2.706761 & 1.871524\\
			\hline
			$X_0 = 99990, Y_0 = 10$ & 2.846620 & 1.220266 & 2.811991 & 0.715527 & 2.844415 & 1.710048 & 2.819770 & 0.863066\\
			\hline
			$X_0 = 99900, Y_0 = 100$ & 2.843606 & 0.232628 & 2.841555 & 0.218771 & 2.839663 & 0.279605 & 2.839069 & 0.256531\\
			\hline
			$X_0 = 99000, Y_0 = 1000$ & 2.843031 & 0.115210 & 2.843140 & 0.112716 & 2.843451 & 0.141831 & 2.843141 & 0.156539\\
			\hline
		\end{tabular}
	\end{footnotesize}
\end{table}

\begin{table}[H]
	
	\caption{$\rr$ Estimates and Std. Errs, SIRMC Model,
		Different Variances, 
		$X_0 = 99000, Y_0 = 1000$}
	\begin{footnotesize}
		\hskip -1.7cm
		\begin{tabular}{l|r|r|r|r|r|r|r|r}
			\hline
			Data Set & Auto Est & Auto SE & AutoM Est & AutoM SE & Norm Est & Norm SE & NormM Est & NormM SE\\
			\hline
			$\sigma_X = 10, \sigma_Y = 1$ & 2.843163 & 0.121074 & 2.843082 & 0.117326 & 2.843563 & 0.163321 & 2.843234 & 0.150979\\
			\hline
			$\sigma_X = 100, \sigma_Y = 10$ & 2.841006 & 1.204736 & 2.840820 & 1.166121 & 2.845950 & 1.640665 & 2.841955 & 1.506458\\
			\hline
			$\sigma_X = 500, \sigma_Y = 50$ & 2.822320 & 5.786070 & 2.834664 & 5.718502 & 2.867771 & 8.562812 & 2.831940 & 7.395682\\
			\hline
			$\sigma_X = 1000, \sigma_Y = 100$ & 2.780071 & 10.584335 & 2.835372 & 11.341172 & 2.922494 & 19.076712 & 2.809761 & 14.170863\\
			\hline
			$\sigma_X = 2000, \sigma_Y = 200$ & 2.649423 & 16.091219 & 2.861883 & 23.385320 & 3.148266 & 59.622533 & 2.736588 & 24.502271\\
			\hline
			$\sigma_X = 5000, \sigma_Y = 500$ & 2.167158 & 14.446035 & 3.104554 & 87.360354 & NaN & 385.053384 & 2.384396 & 29.821372\\
			\hline
			$\sigma_X = 7500, \sigma_Y = 750$ & 1.860269 & 11.045568 & 3.451396 & 242.494249 & NaN & 26.530063 & 2.075721 & 23.380306\\
			\hline
			$\sigma_X = 10000, \sigma_Y = 1000$ & 1.661519 & 9.373613 & 3.871861 & 694.256270 & NaN & 8.570973 & 1.821502 & 17.932802\\
			\hline
			$\sigma_X = 15000, \sigma_Y = 1500$ & 1.494903 & 10.226678 & 4.113933 & 1491.975630 & NaN & 2.509313 & 1.485749 & 12.308392\\
			\hline
		\end{tabular}
	\end{footnotesize}
\end{table}

\begin{table}[H]
	
	\caption{$\rr$ Estimates and Std. Errs, SIRMC Model,
		Other Models, $X_0 = 99950, Y_0 = 50$, 
		$\sigma_X = 10, \sigma_Y = 1$}
	\begin{footnotesize}
		\hskip -1cm
		\begin{tabular}{l|r|r|r|r|r|r|r|r}
			\hline
			Data Set & Auto Est & Auto SE & AutoM Est & AutoM SE & Norm Est & Norm SE & NormM Est & NormM SE\\
			\hline
			0th & NaN & 6.511016e+05 & 7.976281 & 2207.762640 & NaN & 3.490885e+05 & 7.207505 & 1149.944668\\
			\hline
			1st & 3.436204 & 1.350819e+00 & 3.433055 & 1.341851 & 3.437569 & 1.599346e+00 & 3.444295 & 1.441849\\
			\hline
			2nd & 2.934223 & 6.682100e-01 & 2.939971 & 0.443031 & 2.938814 & 7.756630e-01 & 2.939860 & 0.504643\\
			\hline
			3rd & 2.859786 & 3.352930e-01 & 2.867682 & 0.292399 & 2.864071 & 4.244820e-01 & 2.865659 & 0.388652\\
			\hline
			4th & 2.907566 & 3.910230e-01 & 2.907566 & 0.391023 & 2.912020 & 5.038200e-01 & 2.912098 & 0.438172\\
			\hline
			Linear SIR & 2.762911 & 2.063760e-01 & 2.762471 & 0.193776 & 2.762182 & 2.720380e-01 & 2.764241 & 0.229030\\
			\hline
		\end{tabular}
	\end{footnotesize}
\end{table}

\subsubsection{SIR Sequential Bayes}

\paragraph{Estimation}

For the results below, I assume that the prior distribution is distributed as Gamma(1,1). Thus, multiplying by the Poisson likelihood will give us a posterior that is also gamma distributed. The distribution is updated for every day in the data. The final estimate and standard error for $\rr$ are the mean and standard deviation of the final posterior distribution, respectively. In this case, these are the expected value and standard deviation of the final gamma distribution. 

\paragraph{Comments}

The $\rr$ estimates and standard errors are dependent on the choice of prior distribution and parameters. The Gamma(1,1) was chosen only for mathematical simplicity, and other choices could give us vastly different results. 

The $\rr$ estimates here are all fairly reasonable, though they are not as close to the true $\rr$ as some of the other SIR models. Also, in general, the model seems to underestimate $\rr$. There is a spread in the estimates for the baseline data of around 0.2. The estimates appear to increase as $\sigma_X$ and $\sigma_Y$ increase. On the other hand, there does not seem to be a consistent effect with changing $Y_0$

The standard errors are larger than some of the other SIR models, especially those that use lm() or optim() for estimation. There does not seem to be any noticeable patterns with changing the variance of the errors or the starting compartment populations.

\begin{table}[H]
	
	\caption{$\rr$ Estimates and Std. Errs, SEQBAYES Model, 
		Baseline Data, $X_0 = 99950, Y_0 = 50$, 
		$\sigma_X = 10, \sigma_Y = 1$}
	\begin{footnotesize}
		\hskip -1cm
		\begin{tabular}{l|r|r|r|r|r|r|r|r}
			\hline
			Data Set & Auto Est & Auto SE & AutoM Est & AutoM SE & Norm Est & Norm SE & NormM Est & NormM SE\\
			\hline
			Baseline1 & 1.201518 & 0.057374 & 1.058315 & 0.053847 & 1.239136 & 0.058266 & 1.237283 & 0.058222\\
			\hline
			Baseline2 & 1.246696 & 0.058443 & 1.079252 & 0.054377 & 1.124592 & 0.055507 & 1.097017 & 0.054823\\
			\hline
			Baseline3 & 1.039525 & 0.053367 & 1.229901 & 0.058048 & 1.253978 & 0.058614 & 1.082712 & 0.054464\\
			\hline
			Baseline4 & 1.118929 & 0.055368 & 1.137825 & 0.055833 & 1.215893 & 0.057717 & 1.063300 & 0.053974\\
			\hline
			Baseline5 & 1.086093 & 0.054549 & 1.243553 & 0.058369 & 1.050861 & 0.053657 & 1.068459 & 0.054104\\
			\hline
		\end{tabular}
	\end{footnotesize}
\end{table}

\begin{table}[H]
	
	\caption{$\rr$ Estimates and Std. Errs, SEQBAYES Model,
		Different Starting Populations, 
		$\sigma_X = 10, \sigma_Y = 1$}
	\begin{footnotesize}
		\hskip -1.7cm
		\begin{tabular}{l|r|r|r|r|r|r|r|r}
			\hline
			Data Set & Auto Est & Auto SE & AutoM Est & AutoM SE & Norm Est & Norm SE & NormM Est & NormM SE\\
			\hline
			$X_0 = 99999, Y_0 = 1$ & 0.929118 & 0.050453 & 1.000000 & 0.052342 & 0.842065 & 0.048032 & 1.216860 & 0.057740\\
			\hline
			$X_0 = 99990, Y_0 = 10$ & 1.038604 & 0.053343 & 1.167800 & 0.056564 & 0.877679 & 0.049037 & 1.250160 & 0.058524\\
			\hline
			$X_0 = 99900, Y_0 = 100$ & 1.253176 & 0.058595 & 1.069024 & 0.054119 & 1.165760 & 0.056514 & 1.077389 & 0.054330\\
			\hline
			$X_0 = 99000, Y_0 = 1000$ & 0.975516 & 0.051698 & 0.970667 & 0.051569 & 0.947781 & 0.050957 & 0.969026 & 0.051525\\
			\hline
		\end{tabular}
	\end{footnotesize}
\end{table}

\begin{table}[H]
	
	\caption{$\rr$ Estimates and Std. Errs, SEQBAYES Model, 
		Different Variances, $X_0 = 99000$, $Y_0 = 1000$}
	\begin{footnotesize}
		\hskip -1.7cm
		\begin{tabular}{l|r|r|r|r|r|r|r|r}
			\hline
			Error & Auto Est & Auto SE & AutoM Est & AutoM SE & Norm Est & Norm SE & NormM Est & NormM SE\\
			\hline
			$\sigma_X = 10, \sigma_Y = 1$ & 0.984301 & 0.051930 & 0.990291 & 0.052088 & 0.967635 & 0.051488 & 0.968098 & 0.051501\\
			\hline
			$\sigma_X = 100, \sigma_Y = 10$ & 1.014390 & 0.052718 & 1.270396 & 0.058996 & 1.000000 & 0.052342 & 1.000000 & 0.052342\\
			\hline
			$\sigma_X = 500, \sigma_Y = 50$ & 1.033386 & 0.053209 & 1.322754 & 0.060200 & 1.000000 & 0.052342 & 1.000000 & 0.052342\\
			\hline
			$\sigma_X = 1000, \sigma_Y = 100$ & 1.037446 & 0.053313 & 1.347989 & 0.060771 & 1.000000 & 0.052342 & 1.000000 & 0.052342\\
			\hline
			$\sigma_X = 2000, \sigma_Y = 200$ & 1.039747 & 0.053372 & 1.373882 & 0.061352 & 1.000000 & 0.052342 & 1.000000 & 0.052342\\
			\hline
			$\sigma_X = 5000, \sigma_Y = 500$ & 1.041231 & 0.053411 & 1.408605 & 0.062122 & 1.000000 & 0.052342 & 1.000000 & 0.052342\\
			\hline
			$\sigma_X = 7500, \sigma_Y = 750$ & 1.041573 & 0.053419 & 1.424062 & 0.062462 & 1.000000 & 0.052342 & 1.000000 & 0.052342\\
			\hline
			$\sigma_X = 10000, \sigma_Y = 1000$ & 1.041745 & 0.053424 & 1.435049 & 0.062703 & 1.000000 & 0.052342 & 1.000000 & 0.052342\\
			\hline
			$\sigma_X = 15000, \sigma_Y = 1500$ & 1.041919 & 0.053428 & 1.450554 & 0.063041 & 1.000000 & 0.052342 & 1.000000 & 0.052342\\
			\hline
		\end{tabular}
	\end{footnotesize}
\end{table}

\begin{table}[H]
	
	\caption{$\rr$ Estimates and Std. Errs, SEQBAYES Model,
		Other Models, $X_0 = 99950, Y_0 = 50$, 
		$\sigma_X = 10, \sigma_Y = 1$}
	\begin{footnotesize}
		\hskip -1cm
		\begin{tabular}{l|r|r|r|r|r|r|r|r}
			\hline
			Data Set & Auto Est & Auto SE & AutoM Est & AutoM SE & Norm Est & Norm SE & NormM Est & NormM SE\\
			\hline
			0th & 0.751296 & 0.045369 & 1.177502 & 0.056798 & 1.000000 & 0.052342 & 1.234447 & 0.058155\\
			\hline
			1st & 0.997717 & 0.052283 & 0.999939 & 0.052341 & 1.251535 & 0.058556 & 0.895793 & 0.049540\\
			\hline
			2nd & 0.919672 & 0.050196 & 0.946849 & 0.050932 & 0.899505 & 0.049643 & 0.935652 & 0.050630\\
			\hline
			3rd & 1.368874 & 0.061240 & 1.343871 & 0.060678 & 1.359729 & 0.061035 & 1.365769 & 0.061171\\
			\hline
			4th & 1.357390 & 0.060983 & 1.357390 & 0.060983 & 1.346107 & 0.060729 & 1.345567 & 0.060716\\
			\hline
			Linear SIR & 1.123600 & 0.055483 & 1.156827 & 0.056297 & 1.121743 & 0.055437 & 1.194507 & 0.057207\\
			\hline
		\end{tabular}
	\end{footnotesize}
\end{table}

\subsubsection{SEIR Least Squares}

\paragraph{Estimation}

The parameters of the SEIR model ($\beta$, $\gamma$, and $\alpha$) were estimated using optim() in R to minimize the sum of squared errors using Nelder-Mead. The number of iterations was limited to 50 so that the estimate for $\alpha$ did not reach infinity and break the function. The covariance matrix of the estimates of $\alpha$, $\beta$, an $\gamma$ were calculated using sensitivity analysis. The standard error for $\rr$ was then calculated by resampling $\beta$ and $\gamma$ from a multivariate normal distribution with the aforementioned covariance matrix from sensitivity analysis, then taking the standard deviation of the new $\rr$ estimates derived from the set of resampled parameters.

\paragraph{Comments}

In general, we might have issues with fitting the SEIR model to data generated from the classic SIR model, since $\alpha$ will go off to infinity with too many iterations of unconstrained optimization. 

The estimates from the SEIR model are relatively consistent for the baseline data, but they are all slightly larger than the true $\rr$ value. The estimates tend to become more accurate as $Y_0$ increases relative to the total population. However, we can see that increasing the variance of the added errors to the simulated data dramatically decreases the accuracy and precision of the $\rr$ estimates. This includes estimates that are negative, estimates that are orders of magnitude larger than they should be, and estimates that are NA.

The standard errors are an order of magnitude larger than for the SIR model. They are fairly consistent between the multiple baseline set. The standard errors decrease dramatically as $Y_0$ increases. Also, as we see from the estimates above, the standard deviations fluctuate by several orders of magnitude as we change the variance of the added errors. 

\begin{table}[H]
	
	\caption{$\rr$ Estimates and Std. Errs, SEIR Model, 
		Baseline Data, $X_0 = 99950, Y_0 = 50$, 
		$\sigma_X = 10, \sigma_Y = 1$}
	\begin{footnotesize}
		\hskip -1cm
		\begin{tabular}{l|r|r|r|r|r|r|r|r}
			\hline
			Data Set & Auto Est & Auto SE & AutoM Est & AutoM SE & Norm Est & Norm SE & NormM Est & NormM SE\\
			\hline
			Baseline1 & 1.302870 & 0.005985 & 1.304978 & 0.004832 & 1.295983 & 0.007049 & 1.289180 & 0.006367\\
			\hline
			Baseline2 & 1.291782 & 0.005861 & 1.303073 & 0.004532 & 1.293540 & 0.008068 & 1.285063 & 0.004522\\
			\hline
			Baseline3 & 1.291790 & 0.005322 & 1.305184 & 0.004968 & 1.313588 & 0.006870 & 1.295970 & 0.007001\\
			\hline
			Baseline4 & 1.305709 & 0.005793 & 1.303979 & 0.005127 & 1.307232 & 0.006962 & 1.313454 & 0.006151\\
			\hline
			Baseline5 & 1.302877 & 0.005851 & 1.297431 & 0.004872 & 1.302505 & 0.007885 & 1.297394 & 0.006744\\
			\hline
		\end{tabular}
	\end{footnotesize}
\end{table}

\begin{table}[H]
	
	\caption{$\rr$ Estimates and Std. Errs, SEIR Model,
		Different Starting Populations, 
		$\sigma_X = 10, \sigma_Y = 1$}
	\begin{footnotesize}
		\hskip -1.7cm
		\begin{tabular}{l|r|r|r|r|r|r|r|r}
			\hline
			Data Set & Auto Est & Auto SE & AutoM Est & AutoM SE & Norm Est & Norm SE & NormM Est & NormM SE\\
			\hline
			$X_0 = 99999, Y_0 = 1$ & 1.327236 & 0.062503 & 1.305344 & 0.029386 & 1.319333 & 0.104531 & 1.486194 & 0.042930\\
			\hline
			$X_0 = 99990, Y_0 = 10$ & 1.320088 & 0.014535 & 1.327951 & 0.010927 & 1.323049 & 0.019898 & 1.330877 & 0.012021\\
			\hline
			$X_0 = 99900, Y_0 = 100$ & 1.257056 & 0.001195 & 1.252233 & 0.001181 & 1.269212 & 0.001844 & 1.196636 & 0.000635\\
			\hline
			$X_0 = 99000, Y_0 = 1000$ & 1.198724 & 0.000024 & 1.198618 & 0.000026 & 1.198275 & 0.000036 & 1.198736 & 0.000036\\
			\hline
		\end{tabular}
	\end{footnotesize}
\end{table}

\begin{table}[H]
	
	
	\caption{$\rr$ Estimates and Std. Errs, SEIR Model, 
		Different Variances, $X_0 = 99000$, $Y_0 = 1000$}
	\begin{footnotesize}
		\hskip -1.7cm
		\begin{tabular}{l|r|r|r|r|r|r|r|r}
			\hline
			Error & Auto Est & Auto SE & AutoM Est & AutoM SE & Norm Est & Norm SE & NormM Est & NormM SE\\
			\hline
			$\sigma_X = 10, \sigma_Y = 1$ & 1.198667 & 0.022446 & 1.199697 & 0.000054 & 1.184111 & 0.000033 & 1.215409 & 0.003919\\
			\hline
			$\sigma_X = 100, \sigma_Y = 10$ & 0.738349 & 0.027172 & 0.326262 & 0.148088 & 0.876058 & 0.009180 & 1.261285 & 0.000093\\
			\hline
			$\sigma_X = 500, \sigma_Y = 50$ & -0.003532 & 0.056577 & -0.070816 & 0.504898 & 0.094900 & 0.691400 & 1.326638 & 0.030156\\
			\hline
			$\sigma_X = 1000, \sigma_Y = 100$ & 1.652869 & 0.287473 & -0.261653 & 2.358679 & NA & NA & 0.951134 & 0.007148\\
			\hline
			$\sigma_X = 2000, \sigma_Y = 200$ & -0.000235 & 0.079270 & -0.043704 & 0.553296 & 9.411580 & 0.032052 & 1.142685 & 0.017086\\
			\hline
			$\sigma_X = 5000, \sigma_Y = 500$ & 0.014666 & 0.124467 & 0.790375 & 0.015295 & -0.014291 & 0.124025 & 9.473186 & 33.693741\\
			\hline
			$\sigma_X = 7500, \sigma_Y = 750$ & 2.856424 & 11.133371 & 0.808988 & 0.038110 & -0.007135 & 0.109857 & NA & NA\\
			\hline
			$\sigma_X = 10000, \sigma_Y = 1000$ & 2.755506 & 5.698171 & 2250.629540 & 62419.537578 & 0.016792 & 0.294779 & NA & NA\\
			\hline
			$\sigma_X = 15000, \sigma_Y = 1500$ & NA & NA & 0.845936 & 0.056621 & 0.020971 & 0.117313 & 0.703029 & 0.145427\\
			\hline
		\end{tabular}
	\end{footnotesize}
\end{table}

\begin{table}[H]
	
	\caption{$\rr$ Estimates and Std. Errs, SEIR Model,
		Other Models, $X_0 = 99950, Y_0 = 50$, 
		$\sigma_X = 10, \sigma_Y = 1$}
	\begin{footnotesize}
		\hskip -1cm
		\begin{tabular}{l|r|r|r|r|r|r|r|r}
			\hline
			Data Set & Auto Est & Auto SE & AutoM Est & AutoM SE & Norm Est & Norm SE & NormM Est & NormM SE\\
			\hline
			0th & -1.034177 & 0.143642 & -0.055180 & 0.000244 & 0.055060 & 0.029086 & -0.165649 & 0.000683\\
			\hline
			1st & 1.040105 & 0.004954 & 1.190938 & 1.173853 & 1.043084 & 0.004767 & 1.040387 & 0.008804\\
			\hline
			2nd & 1.134694 & 0.000507 & 1.150572 & 0.000496 & 1.132432 & 0.000509 & 1.143565 & 0.000470\\
			\hline
			3rd & 1.303457 & 0.004479 & 1.288333 & 0.004306 & 1.289472 & 0.005720 & 1.287972 & 0.004548\\
			\hline
			4th & 1.264807 & 0.006362 & 1.253277 & 0.004502 & 1.271573 & 0.006568 & 1.247322 & 0.005298\\
			\hline
			Linear SIR & 1.495457 & 0.006932 & 1.496117 & 0.006626 & 1.493676 & 0.009262 & 1.488063 & 0.008507\\
			\hline
		\end{tabular}
	\end{footnotesize}
\end{table}

\subsubsection{MSEIR Least Squares}

\paragraph{Estimation}

The parameters of the MSEIR model were estimated by finding the values that minimizes the sum of squared errors using Nelder-Mead through the constrOptim() in R. This function is the same as optim(), except that it allows us to constrain the search space for the parameters by specifying a constraint matrix. In this case, all of the parameters were constrained to be positive, as they should be by definition. The maximum number of iterations for the optimization was chosen to be 50 in order to limit runtime. 

The standard error was calculated using the delta method. We find the standard deviation of all of the parameters using the Hessian matrix, then use that as the covariance matrix in our delta method calculations.

\paragraph{Comments}

The estimates of $\rr$ are very precise under the same conditions for the classic SIR model. The change in $\sigma_X$ and $\sigma_Y$ do not change the estimates much. They do differ somewhat largely as $Y_0$ changes. The estimates from the data generated from other models are actually consistent and reasonable as well.

The standard errors are large relative to the SEIR or SIR models solved by least squares. There are also NAs for some of the standard error estimates because of singular Hessian matrices.

One thing to note is that the estimates and standard errors may also be highly dependent on the starting parameter estimates given to the optimization function, as there are 5 parameters for the MSEIR model to optimize over.

\begin{table}[H]
	
	\caption{$\rr$ Estimates and Std. Errs, COMP Model, 
		Baseline Data, $X_0 = 99950, Y_0 = 50$, 
		$\sigma_X = 10, \sigma_Y = 1$}
	\begin{footnotesize}
		\hskip -1cm
		\begin{tabular}{l|r|r|r|r|r|r|r|r}
			\hline
			Data Set & Auto Est & Auto SE & AutoM Est & AutoM SE & Norm Est & Norm SE & NormM Est & NormM SE\\
			\hline
			Baseline1 & 1.412982 & 0.010767 & 1.412982 & 0.009669 & 1.412982 & 0.010947 & 1.412982 & 0.010842\\
			\hline
			Baseline2 & 1.412982 & 0.010613 & 1.412982 & 0.010721 & 1.412982 & 0.011393 & 1.412982 & 0.010027\\
			\hline
			Baseline3 & 1.412982 & 0.010823 & 1.412982 & 0.009758 & 1.412982 & 0.010201 & 1.412982 & 0.009806\\
			\hline
			Baseline4 & 1.412982 & 0.011042 & 1.412982 & 0.009580 & 1.412982 & 0.010452 & 1.412982 & 0.011354\\
			\hline
			Baseline5 & 1.412982 & 0.011579 & 1.412982 & 0.009644 & 1.412982 & 0.010383 & 1.412982 & 0.009949\\
			\hline
		\end{tabular}
	\end{footnotesize}
\end{table}

\begin{table}[H]
	
	\caption{$\rr$ Estimates and Std. Errs, COMP Model,
		Different Starting Populations, 
		$\sigma_X = 10, \sigma_Y = 1$}
	\begin{footnotesize}
		\hskip -1.7cm
		\begin{tabular}{l|r|r|r|r|r|r|r|r}
			\hline
			Data Set & Auto Est & Auto SE & AutoM Est & AutoM SE & Norm Est & Norm SE & NormM Est & NormM SE\\
			\hline
			$X_0 = 99999, Y_0 = 1$ & 0.600000 & NA & 0.600000 & NA & 0.600000 & NA & 0.600000 & NA\\
			\hline
			$X_0 = 99990, Y_0 = 10$ & 1.417811 & 0.009877 & 1.428718 & 0.030051 & 1.428718 & 0.025660 & 1.428718 & 0.027867\\
			\hline
			$X_0 = 99900, Y_0 = 100$ & 1.360667 & 0.003552 & 1.360667 & 0.004453 & 1.382555 & 0.003572 & 1.382555 & 0.003212\\
			\hline
			$X_0 = 99000, Y_0 = 1000$ & 1.265725 & 0.002888 & 1.265725 & 0.002807 & 1.265725 & 0.002835 & 1.265725 & 0.003064\\
			\hline
		\end{tabular}
	\end{footnotesize}
\end{table}

\begin{table}[H]
	
	\caption{$\rr$ Estimates and Std. Errs, COMP Model, 
		Different Variances, $X_0 = 99000$, $Y_0 = 1000$}
	\begin{footnotesize}
		\hskip -1.7cm
		\begin{tabular}{l|r|r|r|r|r|r|r|r}
			\hline
			Error & Auto Est & Auto SE & AutoM Est & AutoM SE & Norm Est & Norm SE & NormM Est & NormM SE\\
			\hline
			$\sigma_X = 10, \sigma_Y = 1$ & 1.265725 & 0.002680 & 1.265725 & 0.003044 & 1.265725 & 0.003271 & 1.265725 & 0.002967\\
			\hline
			$\sigma_X = 100, \sigma_Y = 10$ & 1.265725 & 0.003122 & 1.232704 & 0.025432 & 1.265725 & 0.002878 & 1.264345 & 0.003288\\
			\hline
			$\sigma_X = 500, \sigma_Y = 50$ & 1.263026 & 0.006370 & 1.269345 & 0.004386 & 1.258118 & 0.007414 & 1.255344 & 0.006144\\
			\hline
			$\sigma_X = 1000, \sigma_Y = 100$ & 1.234715 & 0.006079 & 1.273213 & 0.000652 & 1.258118 & 0.010512 & 1.260004 & 0.006172\\
			\hline
			$\sigma_X = 2000, \sigma_Y = 200$ & 1.236510 & 0.010850 & 1.246699 & 0.024112 & 1.233704 & 0.011063 & 1.219023 & 0.018803\\
			\hline
			$\sigma_X = 5000, \sigma_Y = 500$ & 1.285391 & 0.002340 & 1.288921 & 0.002671 & 1.222575 & 0.030681 & 1.237872 & 0.017728\\
			\hline
			$\sigma_X = 7500, \sigma_Y = 750$ & 1.267481 & 0.001248 & 1.265853 & 0.001334 & 1.229195 & 0.004376 & 1.208398 & 0.003815\\
			\hline
			$\sigma_X = 10000, \sigma_Y = 1000$ & 1.304435 & 0.001567 & 1.298856 & 0.003194 & 1.246699 & 0.006051 & 1.224308 & 0.001915\\
			\hline
			$\sigma_X = 15000, \sigma_Y = 1500$ & 1.334921 & 0.000388 & 1.319515 & 0.001724 & 1.228979 & 0.002101 & 1.188725 & 0.007633\\
			\hline
		\end{tabular}
	\end{footnotesize}
\end{table}

\begin{table}[H]
	
	\caption{$\rr$ Estimates and Std. Errs, COMP Model,
		Other Models, $X_0 = 99950, Y_0 = 50$, 
		$\sigma_X = 10, \sigma_Y = 1$}
	\begin{footnotesize}
		\hskip -1cm
		\begin{tabular}{l|r|r|r|r|r|r|r|r}
			\hline
			Data Set & Auto Est & Auto SE & AutoM Est & AutoM SE & Norm Est & Norm SE & NormM Est & NormM SE\\
			\hline
			0th & 1.040710 & 0.042623 & 1.038403 & 0.142913 & 1.036463 & 0.053749 & 1.036463 & 0.049341\\
			\hline
			1st & 1.287607 & 0.010336 & 1.287607 & 0.009681 & 1.287607 & 0.009128 & 1.287607 & 0.010393\\
			\hline
			2nd & 1.397864 & 0.000200 & 1.397864 & 0.002296 & 1.397864 & 0.000618 & 1.397864 & 0.001171\\
			\hline
			3rd & 1.364520 & 0.004846 & 1.364520 & 0.004999 & 1.364520 & 0.004918 & 1.364520 & 0.004876\\
			\hline
			4th & 1.397864 & 0.002116 & 1.397864 & 0.002116 & 1.397864 & 0.002426 & 1.397864 & 0.001674\\
			\hline
			Linear SIR & 1.463399 & 0.011059 & 1.463399 & 0.011044 & 1.463399 & 0.010852 & 1.463399 & 0.010955\\
			\hline
		\end{tabular}
	\end{footnotesize}
\end{table}

\subsubsection{Next Generation (SEIR)}

\paragraph{Estimation}

The next generation model below is implemented for the SEIR model. The parameters of the SEIR model ($\beta$, $\gamma$, and $\alpha$) were estimated using constrOptim() in R to minimize the sum of squared errors using Nelder-Mead. The number of iterations was limited to 50 to limit the run time. The constrOptim() function is the same as optim(), except that it allows us to constrain the search space for the parameters by specifying a constraint matrix. In this case, all of the parameters were constrained to be positive, as they should be by definition. We then plug in the parameter estimates to the $F$ and $V$ matrices used to calculate the estimates. The standard errors are calculated by resampling the parameters of the SEIR model and taking the standard deviation of the $\rr$ estimates from the resampled parameters. The standard deviation of the parameters are calculated using the Hessian matrix given by constrOptim() for Nelder-Mead, and resampling is done under the assumption that the parameters are distributed as a multivariate Gaussian.

\paragraph{Comments}

Since this is the SEIR model fit to SIR data, we might have issues with fitting. A lot of the $\rr$ estimates are exactly 1.4. It is not clear why this is or why this is not the case for some of the other data sets. Like we see from the SEIR least squares model, there are some NA estimates, though there are no negative estimates. The estimates do not fluctuate as much as those from the SEIR least squares model either. The $\rr$ estimates also become less accurate for large values of $\sigma_X$ and $\sigma_Y$. Looking at the data generated from other models, the next generation does not seem to work well for the 0th and 1st order data, where the number of people in the S and aI compartment change linearly rather than exponentially.

The standard errors are approximately of the same order of magnitude as those from the SEIR least squares model. Also, like for the SEIR least squares model, they are fairly consistent between the multiple baseline set, the standard errors decrease dramatically as $Y_0$ increases, and the standard deviations fluctuate by several orders of magnitude as we change the variance of the added errors. The most extreme values of standard errors that we see here are not as unreasonably gigantic as for the SEIR least squares model either.

\begin{table}[H]
	
	\caption{$\rr$ Estimates and Std. Errs, NEXTGEN Model, 
		Baseline Data, $X_0 = 99950, Y_0 = 50$, 
		$\sigma_X = 10, \sigma_Y = 1$}
	\begin{footnotesize}
		\hskip -1cm
		\begin{tabular}{l|r|r|r|r|r|r|r|r}
			\hline
			Data Set & Auto Est & Auto SE & AutoM Est & AutoM SE & Norm Est & Norm SE & NormM Est & NormM SE\\
			\hline
			Baseline1 & 1.4 & 0.008096 & 1.4 & 0.005942 & 1.4 & 0.010666 & 1.4 & 0.007859\\
			\hline
			Baseline2 & 1.4 & 0.007525 & 1.4 & 0.006043 & 1.4 & 0.009713 & 1.4 & 0.006573\\
			\hline
			Baseline3 & 1.4 & 0.006891 & 1.4 & 0.006580 & 1.4 & 0.008550 & 1.4 & 0.007550\\
			\hline
			Baseline4 & 1.4 & 0.006989 & 1.4 & 0.005960 & 1.4 & 0.009015 & 1.4 & 0.006935\\
			\hline
			Baseline5 & 1.4 & 0.006809 & 1.4 & 0.006459 & 1.4 & 0.009730 & 1.4 & 0.007778\\
			\hline
		\end{tabular}
	\end{footnotesize}
\end{table}

\begin{table}[H]
	
	\caption{$\rr$ Estimates and Std. Errs, NEXTGEN Model,
		Different Starting Populations, 
		$\sigma_X = 10, \sigma_Y = 1$}
	\begin{footnotesize}
		\hskip -1.7cm
		\begin{tabular}{l|r|r|r|r|r|r|r|r}
			\hline
			Data Set & Auto Est & Auto SE & AutoM Est & AutoM SE & Norm Est & Norm SE & NormM Est & NormM SE\\
			\hline
			$X_0 = 99999, Y_0 = 1$ & 1.400000 & 0.085421 & 1.400000 & 0.041869 & 1.400000 & 0.159127 & 1.437675 & 0.030005\\
			\hline
			$X_0 = 99990, Y_0 = 10$ & 1.400000 & 0.020310 & 1.400000 & 0.012602 & 1.400000 & 0.023171 & 1.400000 & 0.015834\\
			\hline
			$X_0 = 99900, Y_0 = 100$ & 1.400000 & 0.004333 & 1.400000 & 0.004476 & 1.400000 & 0.005505 & 1.400000 & 0.004161\\
			\hline
			$X_0 = 99000, Y_0 = 1000$ & 1.200159 & 0.000029 & 1.198359 & 0.000025 & 1.198359 & 0.000028 & 1.200159 & 0.000031\\
			\hline
		\end{tabular}
	\end{footnotesize}
\end{table}

\begin{table}[H]
	
	\caption{$\rr$ Estimates and Std. Errs, NEXTGEN Model, 
		Different Variances, $X_0 = 99000$, $Y_0 = 1000$}
	\begin{footnotesize}
		\hskip -1.7cm
		\begin{tabular}{l|r|r|r|r|r|r|r|r}
			\hline
			Error & Auto Est & Auto SE & AutoM Est & AutoM SE & Norm Est & Norm SE & NormM Est & NormM SE\\
			\hline
			$\sigma_X = 10, \sigma_Y = 1$ & 1.213314 & 0.000028 & 1.212138 & 0.000030 & 1.213314 & 0.000034 & 1.213314 & 0.000031\\
			\hline
			$\sigma_X = 100, \sigma_Y = 10$ & 1.213314 & 0.000231 & 1.204838 & 0.000240 & 1.213314 & 0.000333 & 1.213314 & 0.000345\\
			\hline
			$\sigma_X = 500, \sigma_Y = 50$ & 1.213314 & 0.001189 & 1.204838 & 0.001234 & 1.179108 & 0.002048 & 1.213314 & 0.001537\\
			\hline
			$\sigma_X = 1000, \sigma_Y = 100$ & 1.198952 & 0.005372 & 1.207589 & 0.005635 & 1.198952 & 0.007195 & 1.198952 & 0.006684\\
			\hline
			$\sigma_X = 2000, \sigma_Y = 200$ & 1.237684 & 0.006507 & 1.249285 & 0.025702 & 1.215487 & 0.007941 & 1.198952 & 0.024341\\
			\hline
			$\sigma_X = 5000, \sigma_Y = 500$ & 1.210888 & 0.039465 & 0.772582 & 0.018292 & 0.002250 & 4.601591 & 1.167461 & 0.382569\\
			\hline
			$\sigma_X = 7500, \sigma_Y = 750$ & 1.191708 & 0.326848 & 0.820021 & 0.011286 & 0.002250 & 0.100353 & NA & NA\\
			\hline
			$\sigma_X = 10000, \sigma_Y = 1000$ & 3.356645 & 0.725374 & 0.836387 & 0.011665 & 0.002250 & 1.020673 & NA & NA\\
			\hline
			$\sigma_X = 15000, \sigma_Y = 1500$ & 2.072075 & 21.625905 & 0.655863 & 0.052597 & 0.002250 & 0.688567 & NA & NA\\
			\hline
		\end{tabular}
	\end{footnotesize}
\end{table}

\begin{table}[H]
	
	\caption{$\rr$ Estimates and Std. Errs, NEXTGEN Model,
		Other Models, $X_0 = 99950, Y_0 = 50$, 
		$\sigma_X = 10, \sigma_Y = 1$}
	\begin{footnotesize}
		\hskip -1cm
		\begin{tabular}{l|r|r|r|r|r|r|r|r}
			\hline
			Data Set & Auto Est & Auto SE & AutoM Est & AutoM SE & Norm Est & Norm SE & NormM Est & NormM SE\\
			\hline
			0th & 0.000095 & 0.009102 & 0.000082 & 0.000173 & 0.055180 & 0.017657 & 0.000023 & 0.000389\\
			\hline
			1st & 1.318348 & 258.153117 & 1.318348 & 5.918045 & 1.318348 & 0.628775 & 1.318348 & 8.692243\\
			\hline
			2nd & 1.400000 & 0.009256 & 1.400000 & 0.005901 & 1.400000 & 0.009429 & 1.400000 & 0.006330\\
			\hline
			3rd & 1.400000 & 0.006097 & 1.400000 & 0.006041 & 1.400000 & 0.007921 & 1.400000 & 0.007604\\
			\hline
			4th & 1.400000 & 0.007004 & 1.400000 & 0.006878 & 1.400000 & 0.009211 & 1.400000 & 0.008413\\
			\hline
			Linear SIR & 1.483791 & 0.007043 & 1.483791 & 0.006561 & 1.483791 & 0.008190 & 1.483791 & 0.007156\\
			\hline
		\end{tabular}
	\end{footnotesize}
\end{table}

\subsection{Exponential Growth}

\subsubsection{Exponential Growth}

\paragraph{Estimation}

We use Poisson regression with glm() in R to fit the incidence on the time step, without an intercept. We then use the coefficient estimate from the regression to find the estimate for $\rr$ by plugging in to the moment generating function. The standard error is calculated using the standard error of the coefficient from glm() and the delta method.

\paragraph{Comments}

It is assumed here that the exponential growth in incidence is seen throughout the entire time span of the data set. This may not be true, but there doesn't seem to be a mathematical way to determine when "exponential growth" starts or stops. We also assume that the generation time distribution is Uniform(10,30). This is somewhat arbitrarily chosen. While we know that the mean time between a primary and secondary case is 20 days for the classic SIR data sets that we generated, we have to choose the distribution around the mean by ourselves.

The estimates here for $\rr$ are consistently too small compared to the true $\rr$. In particular, the estimates are almost all under 1 when the true $\rr$ is above 1, meaning we would predict that there is no epidemic when there actually is. It seems that $\rr$ increases as $Y_0$ increases as well as when $\sigma_X$ and $\sigma_Y$ increase. It is unclear whether this is because the model becomes more accurate or if it just gets larger in general. The model does not break when handling linearly shaped incidence data (although one could argue that this isn't exponential growth, so this model does not apply here in these cases.)

The standard errors are all fairly small, among the order of $10^{-3}$. They seem to decrease as $Y_0$ increases as well as when $\sigma_X$ and $\sigma_Y$ increases.

\begin{table}[H]
	
	\caption{$\rr$ Estimates and Std. Errs, EXPGROWTH Model, 
		Baseline Data, $X_0 = 99950, Y_0 = 50$, 
		$\sigma_X = 10, \sigma_Y = 1$}
	\begin{footnotesize}
		\hskip -1cm
		\begin{tabular}{l|r|r|r|r|r|r|r|r}
			\hline
			Data Set & Auto Est & Auto SE & AutoM Est & AutoM SE & Norm Est & Norm SE & NormM Est & NormM SE\\
			\hline
			Baseline 1 & 0.928417 & 0.001317 & 0.928356 & 0.001319 & 0.928626 & 0.001310 & 0.928228 & 0.001323\\
			\hline
			Baseline 2 & 0.928419 & 0.001317 & 0.928247 & 0.001323 & 0.928594 & 0.001311 & 0.928289 & 0.001321\\
			\hline
			Baseline 3 & 0.928401 & 0.001318 & 0.928163 & 0.001326 & 0.928614 & 0.001310 & 0.928290 & 0.001321\\
			\hline
			Baseline 4 & 0.928395 & 0.001318 & 0.928248 & 0.001323 & 0.928611 & 0.001310 & 0.928310 & 0.001321\\
			\hline
			Baseline 5 & 0.928412 & 0.001317 & 0.928241 & 0.001323 & 0.928651 & 0.001309 & 0.928247 & 0.001323\\
			\hline
		\end{tabular}
	\end{footnotesize}
\end{table}

\begin{table}[H]
	
	\caption{$\rr$ Estimates and Std. Errs, EXPGROWTH Model,
		Different Starting Populations, 
		$\sigma_X = 10, \sigma_Y = 1$}
	\begin{footnotesize}
		\hskip -1.7cm
		\begin{tabular}{l|r|r|r|r|r|r|r|r}
			\hline
			Data Set & Auto Est & Auto SE & AutoM Est & AutoM SE & Norm Est & Norm SE & NormM Est & NormM SE\\
			\hline
			$X_0 = 99999, Y_0 = 1$ & 0.889241 & 0.003555 & 0.854729 & 0.008135 & 0.895513 & 0.003041 & 0.857664 & 0.007601\\
			\hline
			$X_0 = 99990, Y_0 = 10$ & 0.905684 & 0.002355 & 0.902737 & 0.002537 & 0.908169 & 0.002211 & 0.902592 & 0.002546\\
			\hline
			$X_0 = 99900, Y_0 = 100$ & 0.936306 & 0.001074 & 0.936309 & 0.001074 & 0.936398 & 0.001072 & 0.936345 & 0.001073\\
			\hline
			$X_0 = 99000, Y_0 = 1000$ & 0.944202 & 0.000875 & 0.944200 & 0.000875 & 0.944184 & 0.000876 & 0.944210 & 0.000875\\
			\hline
		\end{tabular}
	\end{footnotesize}
\end{table}

\begin{table}[H]
	
	\caption{$\rr$ Estimates and Std. Errs, EXPGROWTH Model, 
		Different Variances, $X_0 = 99000$, $Y_0 = 1000$}
	\begin{footnotesize}
		\hskip -1.7cm
		\begin{tabular}{l|r|r|r|r|r|r|r|r}
			\hline
			Data Set & Auto Est & Auto SE & AutoM Est & AutoM SE & Norm Est & Norm SE & NormM Est & NormM SE\\
			\hline
			$\sigma_X = 10, \sigma_Y = 1$ & 0.944214 & 0.000875 & 0.944234 & 0.000875 & 0.944177 & 0.000876 & 0.944203 & 0.000875\\
			\hline
			$\sigma_X = 100, \sigma_Y = 10$ & 0.948437 & 0.000784 & 0.948047 & 0.000792 & 0.950444 & 0.000744 & 0.949851 & 0.000756\\
			\hline
			$\sigma_X = 500, \sigma_Y = 50$ & 0.968003 & 0.000471 & 0.967327 & 0.000479 & 0.972145 & 0.000422 & 0.970875 & 0.000437\\
			\hline
			$\sigma_X = 1000, \sigma_Y = 100$ & 0.979365 & 0.000350 & 0.978657 & 0.000356 & 0.983895 & 0.000311 & 0.982500 & 0.000322\\
			\hline
			$\sigma_X = 2000, \sigma_Y = 200$ & 0.991428 & 0.000255 & 0.990718 & 0.000260 & 0.996167 & 0.000225 & 0.994717 & 0.000234\\
			\hline
			$\sigma_X = 5000, \sigma_Y = 500$ & 1.007910 & 0.000166 & 1.007211 & 0.000169 & 1.012791 & 0.000146 & 1.011292 & 0.000152\\
			\hline
			$\sigma_X = 7500, \sigma_Y = 750$ & 1.015308 & 0.000136 & 1.014610 & 0.000139 & 1.020223 & 0.000120 & 1.018711 & 0.000125\\
			\hline
			$\sigma_X = 10000, \sigma_Y = 1000$ & 1.020580 & 0.000119 & 1.019883 & 0.000121 & 1.025513 & 0.000104 & 1.023994 & 0.000109\\
			\hline
			$\sigma_X = 15000, \sigma_Y = 1500$ & 1.028035 & 0.000098 & 1.027339 & 0.000100 & 1.032988 & 0.000086 & 1.031463 & 0.000089\\
			\hline
		\end{tabular}
	\end{footnotesize}
\end{table}

\begin{table}[H]
	
	\caption{$\rr$ Estimates and Std. Errs, EXPGROWTH Model,
		Other Models, $X_0 = 99950, Y_0 = 50$, 
		$\sigma_X = 10, \sigma_Y = 1$}
	\begin{footnotesize}
		\hskip -1cm
		\begin{tabular}{l|r|r|r|r|r|r|r|r}
			\hline
			Data Set & Auto Est & Auto SE & AutoM Est & AutoM SE & Norm Est & Norm SE & NormM Est & NormM SE\\
			\hline
			0th & 0.888590 & 0.003613 & 0.755287 & 0.049788 & 0.894107 & 0.003150 & 0.784131 & 0.032919\\
			\hline
			1st & 0.919899 & 0.001639 & 0.919904 & 0.001639 & 0.920147 & 0.001629 & 0.919691 & 0.001648\\
			\hline
			2nd & 0.927250 & 0.001357 & 0.926930 & 0.001368 & 0.927352 & 0.001354 & 0.926933 & 0.001368\\
			\hline
			3rd & 0.929326 & 0.001287 & 0.929046 & 0.001296 & 0.929436 & 0.001283 & 0.929085 & 0.001295\\
			\hline
			4th & 0.927252 & 0.001357 & 0.927252 & 0.001357 & 0.927408 & 0.001352 & 0.927035 & 0.001365\\
			\hline
			Linear SIR & 0.935018 & 0.001111 & 0.934939 & 0.001113 & 0.935133 & 0.001107 & 0.934904 & 0.001114\\
			\hline
		\end{tabular}
	\end{footnotesize}
\end{table}

\subsubsection{Maximum Likelihood}

\paragraph{Estimation}

We assume that the generation time distribution is Uniform(10,30). This is somewhat arbitrarily chosen. While we know that the mean time between a primary and secondary case is 20 days for the classic SIR data sets that we generated, we have to choose the distribution around the mean by ourselves. We also round to the nearest integer because our data contains non-integer counts of cases.

Note that the mathematical calculations are the same here as for the branching process.

We use basic algebra in R to find the estimates for $\rr$, and we use the Fisher information to calculate the standard error for the MLE.

\paragraph{Comments}

All of the $\rr$ estimates are less than 1, while the true $\rr$ is greater than 1. Changing $Y_0$ does not appear to change much, but increasing $\sigma_X$ and $\sigma_Y$ seems to increase the estimate of $\rr$ towards the true value. It is not clear whether the estimate becomes more accurate or simply increases.

The standard errors are really small, around the same order of magnitude as the optim() models. They decrease as $Y_0$ increases and increase as $\sigma_X$ and $\sigma_Y$ increase. We have some negative standard error estimates for the 0th order model data because the sum of the incidences is actually negative due to our added random error. This would probably not happen with actual data.

\begin{table}[H]
	
	\caption{$\rr$ Estimates and Std. Errs, MLE Model, 
		Baseline Data, $X_0 = 99950, Y_0 = 50$, 
		$\sigma_X = 10, \sigma_Y = 1$}
	\begin{footnotesize}
		\hskip -1cm
		\begin{tabular}{l|r|r|r|r|r|r|r|r}
			\hline
			Data Set & Auto Est & Auto SE & AutoM Est & AutoM SE & Norm Est & Norm SE & NormM Est & NormM SE\\
			\hline
			Baseline1 & 0.899074 & 9.7e-05 & 0.902153 & 9.7e-05 & 0.899199 & 9.7e-05 & 0.898925 & 9.7e-05\\
			\hline
			Baseline2 & 0.898625 & 9.7e-05 & 0.899906 & 9.7e-05 & 0.899591 & 9.7e-05 & 0.900620 & 9.7e-05\\
			\hline
			Baseline3 & 0.898641 & 9.7e-05 & 0.896045 & 9.7e-05 & 0.901772 & 9.7e-05 & 0.900604 & 9.7e-05\\
			\hline
			Baseline4 & 0.898617 & 9.7e-05 & 0.899951 & 9.7e-05 & 0.898398 & 9.7e-05 & 0.901681 & 9.7e-05\\
			\hline
			Baseline5 & 0.901165 & 9.7e-05 & 0.899822 & 9.7e-05 & 0.900534 & 9.7e-05 & 0.900186 & 9.7e-05\\
			\hline
		\end{tabular}
	\end{footnotesize}
\end{table}

\begin{table}[H]
	
	\caption{$\rr$ Estimates and Std. Errs, MLE Model,
		Different Starting Populations, 
		$\sigma_X = 10, \sigma_Y = 1$}
	\begin{footnotesize}
		\hskip -1.7cm
		\begin{tabular}{l|r|r|r|r|r|r|r|r}
			\hline
			Data Set & Auto Est & Auto SE & AutoM Est & AutoM SE & Norm Est & Norm SE & NormM Est & NormM SE\\
			\hline
			$X_0 = 99999, Y_0 = 1$ & 0.912953 & 0.003775 & 0.861994 & 0.003368 & 0.877184 & 0.003582 & 0.995336 & 0.004095\\
			\hline
			$X_0 = 99990, Y_0 = 10$ & 0.903707 & 0.000390 & 0.926002 & 0.000401 & 0.909539 & 0.000395 & 0.923559 & 0.000402\\
			\hline
			$X_0 = 99900, Y_0 = 100$ & 0.890136 & 0.000060 & 0.891312 & 0.000060 & 0.891251 & 0.000060 & 0.891413 & 0.000060\\
			\hline
			$X_0 = 99000, Y_0 = 1000$ & 0.863266 & 0.000024 & 0.863334 & 0.000024 & 0.862902 & 0.000024 & 0.863491 & 0.000024\\
			\hline
		\end{tabular}
	\end{footnotesize}
\end{table}


\begin{table}[H]
	
	\caption{$\rr$ Estimates and Std. Errs, MLE Model, 
		Different Variances, $X_0 = 99000$, $Y_0 = 1000$}
	\begin{footnotesize}
		\hskip -1.7cm
		\begin{tabular}{l|r|r|r|r|r|r|r|r}
			\hline
			Error & Auto Est & Auto SE & AutoM Est & AutoM SE & Norm Est & Norm SE & NormM Est & NormM SE\\
			\hline
			$\sigma_X = 10, \sigma_Y = 1$ & 0.863286 & 2.4e-05 & 0.864054 & 0.000024 & 0.862686 & 2.4e-05 & 0.863301 & 2.4e-05\\
			\hline
			$\sigma_X = 100, \sigma_Y = 10$ & 0.865256 & 2.4e-05 & 0.872950 & 0.000025 & 0.859255 & 2.4e-05 & 0.865412 & 2.4e-05\\
			\hline
			$\sigma_X = 500, \sigma_Y = 50$ & 0.873904 & 2.4e-05 & 0.912804 & 0.000026 & 0.844000 & 2.4e-05 & 0.874831 & 2.5e-05\\
			\hline
			$\sigma_X = 1000, \sigma_Y = 100$ & 0.884476 & 2.4e-05 & 0.963357 & 0.000027 & 0.824913 & 2.3e-05 & 0.886693 & 2.5e-05\\
			\hline
			$\sigma_X = 2000, \sigma_Y = 200$ & 0.904863 & 2.4e-05 & 1.066995 & 0.000031 & 0.786679 & 2.2e-05 & 0.910719 & 2.6e-05\\
			\hline
			$\sigma_X = 5000, \sigma_Y = 500$ & 0.960573 & 2.4e-05 & 1.399764 & 0.000043 & 0.671494 & 1.9e-05 & 0.985289 & 2.9e-05\\
			\hline
			$\sigma_X = 7500, \sigma_Y = 750$ & 1.001610 & 2.4e-05 & 1.705105 & 0.000054 & 0.574948 & 1.6e-05 & 1.050455 & 3.1e-05\\
			\hline
			$\sigma_X = 10000, \sigma_Y = 1000$ & 1.038534 & 2.3e-05 & 2.039858 & 0.000068 & 0.477891 & 1.4e-05 & 1.118568 & 3.4e-05\\
			\hline
			$\sigma_X = 15000, \sigma_Y = 1500$ & 1.102292 & 2.2e-05 & 2.816412 & 0.000104 & 0.282224 & 8.0e-06 & 1.264474 & 4.1e-05\\
			\hline
		\end{tabular}
	\end{footnotesize}
\end{table}

\begin{table}[H]
	
	\caption{$\rr$ Estimates and Std. Errs, MLE Model,
		Other Models, $X_0 = 99950, Y_0 = 50$, 
		$\sigma_X = 10, \sigma_Y = 1$}
	\begin{footnotesize}
		\hskip -1cm
		\begin{tabular}{l|r|r|r|r|r|r|r|r}
			\hline
			Data Set & Auto Est & Auto SE & AutoM Est & AutoM SE & Norm Est & Norm SE & NormM Est & NormM SE\\
			\hline
			0th & 3.167663 & -0.894136 & 8.459310 & 11.375438 & 3.145217 & -0.764813 & 6.433011 & 3.054282\\
			\hline
			1st & 0.872597 & 0.000099 & 0.875151 & 0.000100 & 0.873004 & 0.000099 & 0.870359 & 0.000099\\
			\hline
			2nd & 0.894306 & 0.000101 & 0.892614 & 0.000101 & 0.892698 & 0.000101 & 0.892652 & 0.000101\\
			\hline
			3rd & 0.902721 & 0.000093 & 0.900171 & 0.000093 & 0.901044 & 0.000093 & 0.900592 & 0.000093\\
			\hline
			4th & 0.901112 & 0.000104 & 0.901112 & 0.000104 & 0.900495 & 0.000104 & 0.900734 & 0.000104\\
			\hline
			Linear SIR & 0.915222 & 0.000075 & 0.915111 & 0.000074 & 0.915750 & 0.000075 & 0.914991 & 0.000075\\
			\hline
		\end{tabular}
	\end{footnotesize}
\end{table}

\subsubsection{Initial Growth}

\paragraph{Estimation}

We use Poisson regression with glm() in R to fit the incidence on the time step, without an intercept. We then use the coefficient estimate from the regression to find the estimate for $\rr$ by plugging in to the given equation. The duration time is assumed to be $\frac{1}{\gamma}$ from the true data. We use the delta method to solve for the standard errors, using the standard error given from the regression from glm().

\paragraph{Comments}

This model assumes that the incidence is in a period of exponential growth throughout the entire time period. I have fitted the model on the entire data sets.

The estimates here are actually all pretty close to the true $\rr$, and they are extremely consistent for the baseline data sets. They also seem to increase as $Y_0$ increases and as $\sigma_X$ and $\sigma_Y$ increase. This seems to occur regardless of accuracy. The model also handles the 0th and 1st order data sets well, giving us somewhat reasonable estimates (although one could argue that this isn't exponential growth, so this model does not apply here in these cases.)

The standard error estimates are all reasonable. They seem to decrease slightly as $Y_0$ increases and as $\sigma_X$ and $\sigma_Y$ increase.

\begin{table}[H]
	
	\caption{$\rr$ Estimates and Std. Errs, INITGROWTH Model, 
		Baseline Data, $X_0 = 99950, Y_0 = 50$, 
		$\sigma_X = 10, \sigma_Y = 1$}
	\begin{footnotesize}
		\hskip -1cm
		\begin{tabular}{l|r|r|r|r|r|r|r|r}
			\hline
			Data Set & Auto Est & Auto SE & AutoM Est & AutoM SE & Norm Est & Norm SE & NormM Est & NormM SE\\
			\hline
			Baseline 1 & 1.248124 & 0.016595 & 1.247917 & 0.016623 & 1.248829 & 0.016501 & 1.247484 & 0.016682\\
			\hline
			Baseline 2 & 1.248130 & 0.016594 & 1.247549 & 0.016673 & 1.248722 & 0.016515 & 1.247691 & 0.016653\\
			\hline
			Baseline 3 & 1.248067 & 0.016603 & 1.247266 & 0.016711 & 1.248788 & 0.016506 & 1.247692 & 0.016654\\
			\hline
			Baseline 4 & 1.248048 & 0.016605 & 1.247550 & 0.016672 & 1.248777 & 0.016508 & 1.247761 & 0.016644\\
			\hline
			Baseline 5 & 1.248106 & 0.016597 & 1.247529 & 0.016675 & 1.248912 & 0.016490 & 1.247547 & 0.016673\\
			\hline
		\end{tabular}
	\end{footnotesize}
\end{table}

\begin{table}[H]
	
	\caption{$\rr$ Estimates and Std. Errs, INITGROWTH Model,
		Different Starting Populations, 
		$\sigma_X = 10, \sigma_Y = 1$}
	\begin{footnotesize}
		\hskip -1.7cm
		\begin{tabular}{l|r|r|r|r|r|r|r|r}
			\hline
			Data Set & Auto Est & Auto SE & AutoM Est & AutoM SE & Norm Est & Norm SE & NormM Est & NormM SE\\
			\hline
			$X_0 = 99999, Y_0 = 1$ & 1.113607 & 0.047776 & 0.991326 & 0.116074 & 1.135438 & 0.040440 & 1.001869 & 0.107891\\
			\hline
			$X_0 = 99990, Y_0 = 10$ & 1.170601 & 0.030784 & 1.160443 & 0.033326 & 1.179144 & 0.028787 & 1.159943 & 0.033456\\
			\hline
			$X_0 = 99900, Y_0 = 100$ & 1.274694 & 0.013368 & 1.274705 & 0.013367 & 1.275003 & 0.013334 & 1.274823 & 0.013354\\
			\hline
			$X_0 = 99000, Y_0 = 1000$ & 1.301122 & 0.010759 & 1.301114 & 0.010760 & 1.301059 & 0.010765 & 1.301148 & 0.010757\\
			\hline
		\end{tabular}
	\end{footnotesize}
\end{table}

\begin{table}[H]
	
	\caption{$\rr$ Estimates and Std. Errs, INITGROWTH Model, 
		Different Variances, $X_0 = 99000$, $Y_0 = 1000$}
	\begin{footnotesize}
		\hskip -1.7cm
		\begin{tabular}{l|r|r|r|r|r|r|r|r}
			\hline
			Data Set & Auto Est & Auto SE & AutoM Est & AutoM SE & Norm Est & Norm SE & NormM Est & NormM SE\\
			\hline
			$\sigma_X = 10, \sigma_Y = 1$ & 1.301161 & 0.010756 & 1.301227 & 0.010750 & 1.301037 & 0.010767 & 1.301125 & 0.010759\\
			\hline
			$\sigma_X = 100, \sigma_Y = 10$ & 1.315228 & 0.009575 & 1.313931 & 0.009678 & 1.321895 & 0.009059 & 1.319928 & 0.009209\\
			\hline
			$\sigma_X = 500, \sigma_Y = 50$ & 1.379802 & 0.005578 & 1.377586 & 0.005683 & 1.393349 & 0.004975 & 1.389199 & 0.005152\\
			\hline
			$\sigma_X = 1000, \sigma_Y = 100$ & 1.416861 & 0.004075 & 1.414559 & 0.004155 & 1.431547 & 0.003595 & 1.427030 & 0.003736\\
			\hline
			$\sigma_X = 2000, \sigma_Y = 200$ & 1.455861 & 0.002919 & 1.453577 & 0.002977 & 1.471091 & 0.002561 & 1.466436 & 0.002665\\
			\hline
			$\sigma_X = 5000, \sigma_Y = 500$ & 1.508597 & 0.001851 & 1.506373 & 0.001888 & 1.524094 & 0.001618 & 1.519341 & 0.001686\\
			\hline
			$\sigma_X = 7500, \sigma_Y = 750$ & 1.532063 & 0.001510 & 1.529856 & 0.001539 & 1.547585 & 0.001318 & 1.542817 & 0.001374\\
			\hline
			$\sigma_X = 10000, \sigma_Y = 1000$ & 1.548711 & 0.001305 & 1.546515 & 0.001331 & 1.564232 & 0.001140 & 1.559461 & 0.001188\\
			\hline
			$\sigma_X = 15000, \sigma_Y = 1500$ & 1.572148 & 0.001063 & 1.569966 & 0.001084 & 1.587649 & 0.000928 & 1.582882 & 0.000968\\
			\hline
		\end{tabular}
	\end{footnotesize}
\end{table}

\begin{table}[H]
	
	\caption{$\rr$ Estimates and Std. Errs, INITGROWTH Model,
		Other Models, $X_0 = 99950, Y_0 = 50$, 
		$\sigma_X = 10, \sigma_Y = 1$}
	\begin{footnotesize}
		\hskip -1cm
		\begin{tabular}{l|r|r|r|r|r|r|r|r}
			\hline
			Data Set & Auto Est & Auto SE & AutoM Est & AutoM SE & Norm Est & Norm SE & NormM Est & NormM SE\\
			\hline
			0th & 1.111335 & 0.048605 & 0.616117 & 0.861217 & 1.130557 & 0.041983 & 0.728769 & 0.536725\\
			\hline
			1st & 1.219243 & 0.020940 & 1.219259 & 0.020937 & 1.220088 & 0.020800 & 1.218537 & 0.021060\\
			\hline
			2nd & 1.244179 & 0.017134 & 1.243097 & 0.017284 & 1.244522 & 0.017086 & 1.243106 & 0.017283\\
			\hline
			3rd & 1.251193 & 0.016187 & 1.250249 & 0.016312 & 1.251565 & 0.016139 & 1.250381 & 0.016295\\
			\hline
			4th & 1.244185 & 0.017133 & 1.244185 & 0.017133 & 1.244711 & 0.017060 & 1.243452 & 0.017235\\
			\hline
			Linear SIR & 1.270365 & 0.013849 & 1.270099 & 0.013879 & 1.270753 & 0.013805 & 1.269983 & 0.013893\\
			\hline
		\end{tabular}
	\end{footnotesize}
\end{table}

\subsubsection{Final Size}

\paragraph{Estimation}

We use basic arithmetic functions in R to calculate the $\rr$ estimates. We use the delta method to calculate the standard error; treating the final size a proportion of the total population, we use the variance of proportions equation($ \sqrt{\frac{p(1-p)}{N}}$) to calculate the variance of the final size.

\paragraph{Comments}

The model depends on our estimate for the mean infectious period. In this case, we use the true infectious period from the classic SIR data ($\frac{1}{\gamma}$). The estimates and standard errors for the baseline data sets are almost exactly the same between the different ones. The estimates tend to become less accurate as $\sigma_X$ and $\sigma_Y$ increase. They increase as $Y_0$ increases as well. However, this may be due to the fact that the model does not take starting compartment sizes in to account; thus, it does not differentiate between the change in final size caused by changing $Y_0$ when the data are generated from the classic SIR model. Also, the model does not give crazy results for data that are not generated from the classic SIR model, as we might expect since all it uses for estimation is the final number of people who were infected.

The standard errors are all relatively small in magnitude given the way I calculated them, and they increase as $Y_0$ and as $\sigma_X$ and $\sigma_Y$ increase.

\begin{table}[H]
	
	\caption{$\rr$ Estimates and Std. Errs, FINALSIZE Model, 
		Baseline Data, $X_0 = 99950, Y_0 = 50$, 
		$\sigma_X = 10, \sigma_Y = 1$}
	\begin{footnotesize}
		\hskip -1cm
		\begin{tabular}{l|r|r|r|r|r|r|r|r}
			\hline
			Data Set & Auto Est & Auto SE & AutoM Est & AutoM SE & Norm Est & Norm SE & NormM Est & NormM SE\\
			\hline
			Baseline1 & 1.044459 & 0.000492 & 1.044644 & 0.000494 & 1.044435 & 0.000492 & 1.044410 & 0.000492\\
			\hline
			Baseline2 & 1.044363 & 0.000492 & 1.044445 & 0.000492 & 1.044433 & 0.000492 & 1.044518 & 0.000493\\
			\hline
			Baseline3 & 1.044434 & 0.000492 & 1.044279 & 0.000491 & 1.044565 & 0.000493 & 1.044521 & 0.000493\\
			\hline
			Baseline4 & 1.044370 & 0.000492 & 1.044446 & 0.000492 & 1.044374 & 0.000492 & 1.044557 & 0.000493\\
			\hline
			Baseline5 & 1.044480 & 0.000493 & 1.044438 & 0.000492 & 1.044475 & 0.000493 & 1.044442 & 0.000492\\
			\hline
		\end{tabular}
	\end{footnotesize}
\end{table}

\begin{table}[H]
	
	\caption{$\rr$ Estimates and Std. Errs, FINALSIZE Model,
		Different Starting Populations, 
		$\sigma_X = 10, \sigma_Y = 1$}
	\begin{footnotesize}
		\hskip -1.7cm
		\begin{tabular}{l|r|r|r|r|r|r|r|r}
			\hline
			Data Set & Auto Est & Auto SE & AutoM Est & AutoM SE & Norm Est & Norm SE & NormM Est & NormM SE\\
			\hline
			$X_0 = 99999, Y_0 = 1$ & 1.001111 & 0.000075 & 1.001110 & 0.000075 & 1.001081 & 0.000074 & 1.001217 & 0.000078\\
			\hline
			$X_0 = 99990, Y_0 = 10$ & 1.010679 & 0.000234 & 1.010895 & 0.000236 & 1.010676 & 0.000234 & 1.010824 & 0.000235\\
			\hline
			$X_0 = 99900, Y_0 = 100$ & 1.073689 & 0.000652 & 1.073782 & 0.000652 & 1.073845 & 0.000653 & 1.073877 & 0.000653\\
			\hline
			$X_0 = 99000, Y_0 = 1000$ & 1.203036 & 0.001215 & 1.203028 & 0.001215 & 1.202929 & 0.001214 & 1.203088 & 0.001215\\
			\hline
		\end{tabular}
	\end{footnotesize}
\end{table}

\begin{table}[H]
	
	\caption{$\rr$ Estimates and Std. Errs, FINALSIZE Model, 
		Different Variances, $X_0 = 99000$, $Y_0 = 1000$}
	\begin{footnotesize}
		\hskip -1.7cm
		\begin{tabular}{l|r|r|r|r|r|r|r|r}
			\hline
			Error & Auto Est & Auto SE & AutoM Est & AutoM SE & Norm Est & Norm SE & NormM Est & NormM SE\\
			\hline
			$\sigma_X = 10, \sigma_Y = 1$ & 1.203100 & 0.001215 & 1.203221 & 0.001215 & 1.202857 & 0.001214 & 1.203019 & 0.001215\\
			\hline
			$\sigma_X = 100, \sigma_Y = 10$ & 1.204254 & 0.001220 & 1.205468 & 0.001224 & 1.201827 & 0.001210 & 1.203450 & 0.001216\\
			\hline
			$\sigma_X = 500, \sigma_Y = 50$ & 1.209421 & 0.001240 & 1.215599 & 0.001265 & 1.197277 & 0.001191 & 1.205368 & 0.001224\\
			\hline
			$\sigma_X = 1000, \sigma_Y = 100$ & 1.215968 & 0.001267 & 1.228600 & 0.001317 & 1.191656 & 0.001169 & 1.207778 & 0.001234\\
			\hline
			$\sigma_X = 2000, \sigma_Y = 200$ & 1.229359 & 0.001320 & 1.255800 & 0.001425 & 1.180629 & 0.001124 & 1.212639 & 0.001253\\
			\hline
			$\sigma_X = 5000, \sigma_Y = 500$ & 1.272128 & 0.001489 & 1.348709 & 0.001792 & 1.149157 & 0.000994 & 1.227550 & 0.001313\\
			\hline
			$\sigma_X = 7500, \sigma_Y = 750$ & 1.311117 & 0.001643 & 1.442921 & 0.002168 & 1.124616 & 0.000888 & 1.240372 & 0.001364\\
			\hline
			$\sigma_X = 10000, \sigma_Y = 1000$ & 1.353661 & 0.001811 & 1.558903 & 0.002644 & 1.101452 & 0.000785 & 1.253572 & 0.001416\\
			\hline
			$\sigma_X = 15000, \sigma_Y = 1500$ & 1.451957 & 0.002204 & 1.908199 & 0.004213 & 1.058763 & 0.000574 & 1.281188 & 0.001525\\
			\hline
		\end{tabular}
	\end{footnotesize}
\end{table}

\begin{table}[H]
	
	\caption{$\rr$ Estimates and Std. Errs, FINALSIZE Model,
		Other Models, $X_0 = 99950, Y_0 = 50$, 
		$\sigma_X = 10, \sigma_Y = 1$}
	\begin{footnotesize}
		\hskip -1cm
		\begin{tabular}{l|r|r|r|r|r|r|r|r}
			\hline
			Data Set & Auto Est & Auto SE & AutoM Est & AutoM SE & Norm Est & Norm SE & NormM Est & NormM SE\\
			\hline
			0th & 1.000194 & 0.000031 & 1.000282 & 0.000038 & 1.000185 & 0.000030 & 1.000318 & 0.000040\\
			\hline
			1st & 1.040704 & 0.000470 & 1.040802 & 0.000470 & 1.040694 & 0.000469 & 1.040546 & 0.000469\\
			\hline
			2nd & 1.041921 & 0.000477 & 1.041759 & 0.000476 & 1.041815 & 0.000476 & 1.041766 & 0.000476\\
			\hline
			3rd & 1.046705 & 0.000506 & 1.046460 & 0.000504 & 1.046563 & 0.000505 & 1.046529 & 0.000505\\
			\hline
			4th & 1.041585 & 0.000475 & 1.041585 & 0.000475 & 1.041481 & 0.000474 & 1.041485 & 0.000474\\
			\hline
			Linear SIR & 1.061096 & 0.000587 & 1.061156 & 0.000587 & 1.061115 & 0.000587 & 1.061060 & 0.000586\\
			\hline
		\end{tabular}
	\end{footnotesize}
\end{table}

\subsection{Networks}

\subsubsection{Branching Process}

\paragraph{Estimation}

We assume that the generation time distribution is Uniform(10,30). This is somewhat arbitrarily chosen. While we know that the mean time between a primary and secondary case is 20 days for the classic SIR data sets that we generated, we have to choose the distribution around the mean by ourselves. We also round to the nearest integer because our data contains non-integer counts of cases.

Note that the mathematical calculations are the same here as for the MLE.

We use basic algebra in R to find the estimates for $\rr$, and we use the second derivative to calculate the standard error for the branching process.

\paragraph{Comments}

All of the $\rr$ estimates are less than 1, while the true $\rr$ is greater than 1. Changing $Y_0$ does not appear to change much, but increasing $\sigma_X$ and $\sigma_Y$ seems to increase the estimate of $\rr$ towards the true value. It is not clear whether the estimate becomes more accurate or simply increases.

The standard errors are really small, around the same order of magnitude as the optim() models. They decrease as $Y_0$ increases and increase as $\sigma_X$ and $\sigma_Y$ increase. We have some negative standard error estimates for the 0th order model data because the sum of the incidences is actually negative due to our added random error. This would probably not happen with actual data.

\begin{table}[H]
	
	\caption{$\rr$ Estimates and Std. Errs, BRANCH Model, 
		Baseline Data, $X_0 = 99950, Y_0 = 50$, 
		$\sigma_X = 10, \sigma_Y = 1$}
	\begin{footnotesize}
		\hskip -1cm
		\begin{tabular}{l|r|r|r|r|r|r|r|r}
			\hline
			Data Set & Auto Est & Auto SE & AutoM Est & AutoM SE & Norm Est & Norm SE & NormM Est & NormM SE\\
			\hline
			Baseline1 & 0.899074 & 9.7e-05 & 0.902153 & 9.7e-05 & 0.899199 & 9.7e-05 & 0.898925 & 9.7e-05\\
			\hline
			Baseline2 & 0.898625 & 9.7e-05 & 0.899906 & 9.7e-05 & 0.899591 & 9.7e-05 & 0.900620 & 9.7e-05\\
			\hline
			Baseline3 & 0.898641 & 9.7e-05 & 0.896045 & 9.7e-05 & 0.901772 & 9.7e-05 & 0.900604 & 9.7e-05\\
			\hline
			Baseline4 & 0.898617 & 9.7e-05 & 0.899951 & 9.7e-05 & 0.898398 & 9.7e-05 & 0.901681 & 9.7e-05\\
			\hline
			Baseline5 & 0.901165 & 9.7e-05 & 0.899822 & 9.7e-05 & 0.900534 & 9.7e-05 & 0.900186 & 9.7e-05\\
			\hline
		\end{tabular}
	\end{footnotesize}
\end{table}

\begin{table}[H]
	
	\caption{$\rr$ Estimates and Std. Errs, BRANCH Model,
		Different Starting Populations, 
		$\sigma_X = 10, \sigma_Y = 1$}
	\begin{footnotesize}
		\hskip -1.7cm
		\begin{tabular}{l|r|r|r|r|r|r|r|r}
			\hline
			Data Set & Auto Est & Auto SE & AutoM Est & AutoM SE & Norm Est & Norm SE & NormM Est & NormM SE\\
			\hline
			$X_0 = 99999, Y_0 = 1$ & 0.912953 & 0.003775 & 0.861994 & 0.003368 & 0.877184 & 0.003582 & 0.995336 & 0.004095\\
			\hline
			$X_0 = 99990, Y_0 = 10$ & 0.903707 & 0.000390 & 0.926002 & 0.000401 & 0.909539 & 0.000395 & 0.923559 & 0.000402\\
			\hline
			$X_0 = 99900, Y_0 = 100$ & 0.890136 & 0.000060 & 0.891312 & 0.000060 & 0.891251 & 0.000060 & 0.891413 & 0.000060\\
			\hline
			$X_0 = 99000, Y_0 = 1000$ & 0.863266 & 0.000024 & 0.863334 & 0.000024 & 0.862902 & 0.000024 & 0.863491 & 0.000024\\
			\hline
		\end{tabular}
	\end{footnotesize}
\end{table}

\begin{table}[H]
	
	\caption{$\rr$ Estimates and Std. Errs, BRANCH Model, 
		Different Variances, $X_0 = 99000$, $Y_0 = 1000$}
	\begin{footnotesize}
		\hskip -1.7cm
		\begin{tabular}{l|r|r|r|r|r|r|r|r}
			\hline
			Error & Auto Est & Auto SE & AutoM Est & AutoM SE & Norm Est & Norm SE & NormM Est & NormM SE\\
			\hline
			$\sigma_X = 10, \sigma_Y = 1$ & 0.863286 & 2.4e-05 & 0.864054 & 0.000024 & 0.862686 & 2.4e-05 & 0.863301 & 2.4e-05\\
			\hline
			$\sigma_X = 100, \sigma_Y = 10$ & 0.865256 & 2.4e-05 & 0.872950 & 0.000025 & 0.859255 & 2.4e-05 & 0.865412 & 2.4e-05\\
			\hline
			$\sigma_X = 500, \sigma_Y = 50$ & 0.873904 & 2.4e-05 & 0.912804 & 0.000026 & 0.844000 & 2.4e-05 & 0.874831 & 2.5e-05\\
			\hline
			$\sigma_X = 1000, \sigma_Y = 100$ & 0.884476 & 2.4e-05 & 0.963357 & 0.000027 & 0.824913 & 2.3e-05 & 0.886693 & 2.5e-05\\
			\hline
			$\sigma_X = 2000, \sigma_Y = 200$ & 0.904863 & 2.4e-05 & 1.066995 & 0.000031 & 0.786679 & 2.2e-05 & 0.910719 & 2.6e-05\\
			\hline
			$\sigma_X = 5000, \sigma_Y = 500$ & 0.960573 & 2.4e-05 & 1.399764 & 0.000043 & 0.671494 & 1.9e-05 & 0.985289 & 2.9e-05\\
			\hline
			$\sigma_X = 7500, \sigma_Y = 750$ & 1.001610 & 2.4e-05 & 1.705105 & 0.000054 & 0.574948 & 1.6e-05 & 1.050455 & 3.1e-05\\
			\hline
			$\sigma_X = 10000, \sigma_Y = 1000$ & 1.038534 & 2.3e-05 & 2.039858 & 0.000068 & 0.477891 & 1.4e-05 & 1.118568 & 3.4e-05\\
			\hline
			$\sigma_X = 15000, \sigma_Y = 1500$ & 1.102292 & 2.2e-05 & 2.816412 & 0.000104 & 0.282224 & 8.0e-06 & 1.264474 & 4.1e-05\\
			\hline
		\end{tabular}
	\end{footnotesize}
\end{table}

\begin{table}[H]
	
	\caption{$\rr$ Estimates and Std. Errs, BRANCH Model,
		Other Models, $X_0 = 99950, Y_0 = 50$, 
		$\sigma_X = 10, \sigma_Y = 1$}
	\begin{footnotesize}
		\hskip -1cm
		\begin{tabular}{l|r|r|r|r|r|r|r|r}
			\hline
			Data Set & Auto Est & Auto SE & AutoM Est & AutoM SE & Norm Est & Norm SE & NormM Est & NormM SE\\
			\hline
			0th & 3.167663 & -0.894136 & 8.459310 & 11.375438 & 3.145217 & -0.764813 & 6.433011 & 3.054282\\
			\hline
			1st & 0.872597 & 0.000099 & 0.875151 & 0.000100 & 0.873004 & 0.000099 & 0.870359 & 0.000099\\
			\hline
			2nd & 0.894306 & 0.000101 & 0.892614 & 0.000101 & 0.892698 & 0.000101 & 0.892652 & 0.000101\\
			\hline
			3rd & 0.902721 & 0.000093 & 0.900171 & 0.000093 & 0.901044 & 0.000093 & 0.900592 & 0.000093\\
			\hline
			4th & 0.901112 & 0.000104 & 0.901112 & 0.000104 & 0.900495 & 0.000104 & 0.900734 & 0.000104\\
			\hline
			Linear SIR & 0.915222 & 0.000075 & 0.915111 & 0.000074 & 0.915750 & 0.000075 & 0.914991 & 0.000075\\
			\hline
		\end{tabular}
	\end{footnotesize}
\end{table}




\end{document}


%%%%%%%%%%%%%%%%%%%%%%
\Textbf{Title:} 

\textbf{Author:} 

\textbf{Citation:} 

\textbf{Major themes:} 

\textbf{Notes:}
\\
%%%%%%%%%%%%%%%%%%%%%%




\begin{figure}[h]
\begin{center}
\includegraphics[width=4in]{mvhw7_3c.pdf}
\end{center}
\caption{
Biplots of the different continuous variables and their correlations.}\label{fig3c}
\end{figure}

%% two pictures whoa

\begin{figure}[h]
\centering


\begin{figure}[h]
\centering

\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{mt_eda_cont_hists.pdf}
  \caption{Histograms of Arrival Delay and continuous covariates.  Arrival delay seems to have a right skewed distribution.  This may indicate that we will be transforming this variable later on.  After transforming Air Time and Distance by a log transformation, we don't really seem to have many outliers in our covariates.  We seem to have outliers in the CRS Dep. Time and Arrival Time; however, time is cyclical and so these are not, in fact outliers.}
  \label{hists}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{mt_eda_cont_hists.pdf}
  \caption{\textcolor{red}{Placeholder}}
  \label{tabs1}
\end{subfigure}
\caption{}
\end{figure}


\begin{table}
\begin{tabular}{l | a | b | a | b}
\hline
\rowcolor{LightCyan}
\mc{1}{}  & \mc{1}{x} & \mc{1}{y} & \mc{1}{w} & \mc{1}{z} \    \hline
variable 1 & a & b & c & d \    variable 2 & a & b & c & d \\ \hline
\end{tabular}
\end{table}






%%
\begin{figure}
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{resids_full.pdf}
  \caption{}
  \label{residsf}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{diags_full.pdf}
  \caption{ }
  \label{diagsf}
\end{subfigure}
\caption{}
\end{figure}




